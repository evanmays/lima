# Reproduce LIMA

Weekend effort to reproduce Meta's Less is More for Aligment (LIMA)

Hypothesis: Tiny finetuning dataset size only worked well because of significant overlap with the pretraining dataset.

I cheat by using their already published paper author prompts.

Half finished, accepting PRs

- [ ] Stack Exchange (STEM)
- [ ] Stack Exchange (Other)
- [ ] wikiHow
- [ ] Pushshift r/WritingPrompts
- [ ] Natural Instructions
- [ ] Paper Authors (Group A)

## Instructions

Need 128GB ram for downloading/unzipping the dataset

Install `py7zr`

On mac its `brew install py7zr`
